[
    {
        "title": "2001.08361",
        "year": "2020",
        "file_name": "2001.08361.pdf",
        "full_text": "Scaling Laws for Neural Language Models Jared Kaplan   Johns Hopkins University, OpenAI jaredk@jhu.edu Sam McCandlish  OpenAI sam@openai.com Tom Henighan OpenAI henighan@openai.com Tom B. Brown OpenAI tom@openai.com Benjamin Chess OpenAI bchess@openai.com Rewon Child OpenAI rewon@openai.com Scott Gray OpenAI scott@openai.com Alec Radford OpenAI alec@openai.com Jeffrey Wu OpenAI jeffwu@openai.com Dario Amodei OpenAI damodei@openai.com Abstract We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of over tting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a  xed compute budget. Larger models are signi cantly more sample- ef cient, such that optimally compute-ef cient training involves training very large models on a relatively modest amount of data and stopping signi cantly before convergence.  Equal contribution. Contributions: Jared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex- periments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided guidance throughout the project. arXiv:2001.08361v1 [cs.LG] 23 Jan 2020 Contents 1 Introduction 2 2 Background and Methods 6 3 Empirical Results and Basic Power Laws 7 4 Charting the In nite Data Limit and Over tting 10 5 Scaling Laws with Model Size and Training Time 12 6 Optimal Allocation of the Compute Budget 14 7 Related Work 18 8 Discussion 18 Appendices 20 A Summary of Power Laws 20 B Empirical Model of Compute-Ef cient Frontier 20 C Caveats 22 D Supplemental Figures 23 1 Introduction Language provides a natural domain for the study of arti cial intelligence, as the vast majority of reason- ing tasks can be ef ciently expressed and evaluated in language, and the world s text provides a wealth of data for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan- guage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching human-level performance on many speci c tasks [WPN+19], including the composition of coherent multi- paragraph prompted text samples [RWC+19]. One might expect language modeling performance to depend on model architecture, the size of neural models, the computing power used to train them, and the data available for this training process. In this work we will empirically investigate the dependence of language modeling loss on all of these factors, focusing on the Transformer architecture [VSP+17, LSP+18]. The high ceiling and low  oor for performance on language tasks allows us to study trends over more than seven orders of magnitude in scale. Throughout we will observe precise power-law scalings for performance as a function of training time, con- text length, dataset size, model size, and compute budget. 1.1 Summary Our key  ndings for Transformer language models are are as follows: 2Here we display predicted compute when using a suf ciently small batch size. See Figure 13 for comparison to the purely empirical data. 2 Dataset Size tokens Parameters non-embedding Compute PF-days, non-embedding Test Loss Figure 1 Language modeling performance improves smoothly as we increase the model size, datasetset size, and amount of compute2 used for training. For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two. Performance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embed- dings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3) Smooth power laws: Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must  atten out eventually before reaching zero loss. (Section 3) Universality of over tting: Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held  xed while the other increases. The performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4) Universality of training: Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. (Section 5) Transfer improves with test performance: When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss   in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2) Sample ef ciency: Large models are more sample-ef cient than small models, reaching the same level of performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4). Convergence is inef cient: When working within a  xed compute budget C but without any other restric- tions on the model size N or available data D, we attain optimal performance by training very large models and stopping signi cantly short of convergence (see Figure 3). Maximally compute-ef cient training would therefore be far more sample ef cient than one might expect based on training small models to convergence, with data requirements growing very slowly as D  C0.27 with training compute. (Section 6) Optimal batch size: The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million tokens at convergence for the largest models we can train. (Section 5.1) Taken together, these results show that language modeling performance improves smoothly and predictably as we appropriately scale up model size, data, and compute. We expect that larger language models will perform better and be more sample ef cient than current models. 3 Larger models require fewer samples to reach the same performance 10 8 6 4 The optimal model size grows smoothly with the loss target and compute budget Line color indicates number of parameters 107 109 1011 Tokens Processed Compute (PF-days) 10-9 10-6 10-3 100 Test Loss Compute-e cient training stops far short of convergence 103 109 106 103 Params 109 Params 10 8 6 4 Figure 2 We show a series of language model training runs, with models ranging in size from 103 to 109 parameters (excluding embeddings). 100x Batch Size <10x Serial Steps >1,000,000x Model Size Data requirements grow relatively slowly Optimal model size increases very quickly Minimum serial steps increases negligibly Figure 3 As more compute becomes available, we can choose how much to allocate towards training larger models, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in compute. For optimally compute-ef cient training, most of the increase should go towards increased model size. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to increase parallelism through larger batch sizes, with only a very small increase in serial training time required. 1.2 Summary of Scaling Laws The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law when performance is limited by only either the number of non-embedding parameters N, the dataset size D, or the optimally allocated compute budget Cmin (see Figure 1): 1. For models with a limited number of parameters, trained to convergence on suf ciently large datasets: L(N) = (Nc/N) N ;  N  0.076, Nc  8.8   1013 (non-embedding parameters) (1.1) 2. For large models trained with a limited dataset with early stopping: L(D) = (Dc/D) D ;  D  0.095, Dc  5.4   1013 (tokens) (1.2) 3. When training with a limited amount of compute, a suf ciently large dataset, an optimally-sized model, and a suf ciently small batch size (making optimal3 use of compute): L(Cmin) = \u0000Cmin c /Cmin \u0001 min C ;  min C  0.050, Cmin c  3.1   108 (PF-days) (1.3) 3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at  xed batch size, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5). 4 107 108 109 1010 Tokens in Dataset 2.5 3.0 3.5 4.0 4.5 Loss Loss vs Model and Dataset Size Params 708M 302M 85M 3M 25M 393.2K 104 105 Estimated Smin 2.4 2.8 3.2 3.6 4.0 4.4 Loss Loss vs Model Size and Training Steps 106 107 108 Parameters (non-embed) Figure 4 Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model sizes N can be  t with Equation (1.6), which is parameterized in terms of Smin, the number of steps when training at large batch size (details in Section 5.1). These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters (depth, width, number of self-attention heads), with speci c numerical values associated with the Webtext2 training set [RWC+19]. The power laws  N,  D,  min C specify the degree of performance improvement expected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that is smaller by a factor 2 N = 0.95. The precise numerical values of Nc, Cmin c , and Dc depend on the vocabulary size and tokenization and hence do not have a fundamental meaning. The critical batch size, which determines the speed/ef ciency tradeoff for data parallelism ([MKAT18]), also roughly obeys a power law in L: Bcrit (L) = B  L1/ B , B 2   108 tokens,  B  0.21 (1.4) Equation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset size sublinearly according to D  N  N  D  N 0.74. In fact, we  nd that there is a single equation combining (1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of over tting: L(N, D) = \"\u0012Nc N \u0013  N  D + Dc D # D (1.5) with  ts pictured on the left in  gure 4. We conjecture that this functional form may also parameterize the trained log-likelihood for other generative modeling tasks. When training a given model for a  nite number of parameter update steps S in the in nite data limit, after an initial transient period, the learning curves can be accurately  t by (see the right of  gure 4) L(N, S) = \u0012Nc N \u0013 N + \u0012 Sc Smin(S) \u0013 S (1.6) where Sc  2.1   103 and  S  0.76, and Smin(S) is the minimum possible number of optimization steps (parameter updates) estimated using Equation (5.4). When training within a  xed compute budget C, but with no other constraints, Equation (1.6) leads to the prediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size D should grow as N  C min C / N , B  C min C / B, S  C min C / S, D = B   S (1.7) with  min C = 1/ (1/ S + 1/ B + 1/ N) (1.8) which closely matches the empirically optimal results N  C0.73 min , B  C0.24 min , and S  C0.03 min . As the computational budget C increases, it should be spent primarily on larger models, without dramatic increases in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become increasingly sample ef cient. In practice, researchers typically train smaller models for longer than would 5 be maximally compute-ef cient because of hardware constraints. Optimal performance depends on total compute as a power law (see Equation (1.3)). We provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve  ts and their implications for training time, and a breakdown of our results per token. We also make some brief compar- isons to LSTMs and recurrent Transformers [DGV+18]. 1.3 Notation We use the following notation:   L   the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in some cases we report the loss for speci c tokens within the context.   N   the number of model parameters, excluding all vocabulary and positional embeddings   C  6NBS   an estimate of the total non-embedding training compute, where B is the batch size, and S is the number of training steps (ie parameter updates). We quote numerical values in PF-days, where one PF-day = 1015   24   3600 = 8.64   1019  oating point operations.   D   the dataset size in tokens   Bcrit   the critical batch size [MKAT18], de ned and discussed in Section 5.1. Training at the critical batch size provides a roughly optimal compromise between time and compute ef ciency.   Cmin   an estimate of the minimum amount of non-embedding compute to reach a given value of the loss. This is the training compute that would be used if the model were trained at a batch size much less than the critical batch size.   Smin   an estimate of the minimal number of training steps needed to reach a given value of the loss. This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size.    X   power-law exponents for the scaling of the loss as L(X)  1/X X where X can be any of N, D, C, S, B, Cmin. 2 Background and Methods We train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized using byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres- sive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal performance metric. We record the loss on the WebText2 test distribution and on a selection of other text distributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though we also train LSTM models and Universal Transformers [DGV+18] for comparison. 2.1 Parameter and Compute Scaling of Transformers We parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di- mension of the residual stream), d (dimension of the intermediate feed-forward layer), dattn (dimension of the attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input context, with nctx = 1024 except where otherwise noted. We use N to denote the model size, which we de ne as the number of non-embedding parameters N  2dmodelnlayer (2dattn + d ) = 12nlayerd2 model with the standard dattn = d /4 = dmodel (2.1) where we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters in an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include these when discussing the  model size  N; we will see that this produces signi cantly cleaner scaling laws. Evaluating a forward pass of the Transformer involves roughly Cforward  2N + 2nlayernctxdmodel (2.2) add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in matrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1. 6 Operation Parameters FLOPs per Token Embed (nvocab + nctx) dmodel 4dmodel Attention: QKV nlayerdmodel3dattn 2nlayerdmodel3dattn Attention: Mask   2nlayernctxdattn Attention: Project nlayerdattndmodel 2nlayerdattndembd Feedforward nlayer2dmodeld  2nlayer2dmodeld  De-embed   2dmodelnvocab Total (Non-Embedding) N = 2dmodelnlayer (2dattn + d ) Cforward = 2N + 2nlayernctxdattn Table 1 Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted. For contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a relatively small fraction of the total compute. Since we primarily study models where dmodel  nctx/12, we do not include context-dependent terms in our training compute estimate. Accounting for the backwards pass (approximately twice the compute as the forwards pass), we then de ne the estimated non-embedding compute as C  6N  oating point operators per training token. 2.2 Training Procedures Unless otherwise noted, we train models with the Adam optimizer [KB14] for a  xed 2.5   105 steps with a batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than 1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and schedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of learning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate schedule with a 3000 step linear warmup followed by a cosine decay to zero. 2.3 Datasets We train our models on an extended version of the WebText dataset described in [RWC+19]. The original WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62   1010 words (as de ned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields 2.29   1010 tokens. We reserve 6.6   108 of these tokens for use as a test set, and we also test on similarly- prepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection of publicly-available Internet Books. 3 Empirical Results and Basic Power Laws To characterize language model scaling we train a wide variety of models, varying a number of factors including:   Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)   Dataset size (ranging from 22 million to 23 billion tokens)   Shape (including depth, width, attention heads, and feed-forward dimension)   Context length (1024 for most runs, though we also experiment with shorter contexts)   Batch size (219 for most runs, but we also vary it to measure the critical batch size) 7 Feed-Forward Ratio (dff / dmodel) 50M Parameters Aspect Ratio (dmodel / nlayer) Attention Head Dimension (dmodel / nhead) 25M Parameters 10% 8% 6% 4% 2% 0% Loss Increase A wide range of architectures achieve similar performance 22% additional compute compensates for 1% loss increase Figure 5 Performance depends very mildly on model shape when the total number of non-embedding parameters N is held  xed. The loss varies only a few percent over a wide range of shapes. Small differences in parameter counts are compensated for by using the  t to L(N) as a baseline. Aspect ratio in particular can vary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a loss within 3% of the (48, 1600) model used in [RWC+19]. 106 107 108 109 Parameters (with embedding) 2 3 4 5 6 7 Test Loss 0 Layer 1 Layer 2 Layers 3 Layers 6 Layers > 6 Layers 103 104 105 106 107 108 109 Parameters (non-embedding) 2 3 4 5 6 7 Test Loss 1 Layer 2 Layers 3 Layers 6 Layers > 6 Layers Figure 6 Left: When we include embedding parameters, performance appears to depend strongly on the number of layers in addition to the number of parameters. Right: When we exclude embedding parameters, the performance of models with different depths converge to a single trend. Only models with fewer than 2 layers or with extreme depth-to-width ratios deviate signi cantly from the trend. In this section we will display data along with empirically-motivated  ts, deferring theoretical analysis to later sections. 3.1 Approximate Transformer Shape and Hyperparameter Independence Transformer performance depends very weakly on the shape parameters nlayer, nheads, and d when we hold the total non-embedding parameter count N  xed. To establish these results we trained models with  xed size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer, we simultaneously varied dmodel while keeping N  12nlayerd2 model  xed. Similarly, to vary d at  xed model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table 1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5. 3.2 Performance with Non-Embedding Parameter Count N In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape (nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through (207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-  tting (except possibly for the very largest models). As shown in Figure 1, we  nd a steady trend with non-embedding parameter count N, which can be  t to the  rst term of Equation (1.5), so that L(N)   \u0012Nc N \u0013 N (3.1) 8 LSTM plateaus after <100 tokens Transformer improves through the whole context 2M 200M 3M 300M 5 4 3 2 6 Token Index in Context 103 102 101 Transformers asymptotically outperform LSTMs due to improved use of long contexts 3.6 4.2 3.0 2.4 4.8 5.4 105 108 106 107 109 Parameters (non-embedding) Transformers LSTMs 1 Layer 2 Layers 4 Layers Test Loss Per-token Test Loss Parameters: 400K 400K Figure 7 To observe these trends it is crucial to study performance as a function of N; if we instead use the total parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in recent work [LCG+19]. Although these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets is also a power-law in N with nearly identical power, as shown in Figure 8. 3.2.1 Comparing to LSTMs and Universal Transformers In Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter count N. The LSTMs were trained with the same dataset and context length. We see from these  gures that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens. We present power-law relationships between performance and context position Appendix D.5, where increasingly large powers for larger models suggest improved ability to quickly recognize patterns. We also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the cost of additional compute per-parameter. 3.2.2 Generalization Among Data Distributions We have also tested our models on a set of additional text data distributions. The test loss on these datasets as a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2 dataset. We see that the loss on these other data distributions improves smoothly with model size, in direct parallel with the improvement on WebText2. We  nd that generalization depends almost exclusively on the in-distribution validation loss, and does not depend on the duration of training or proximity to convergence. We also observe no dependence on model depth (see Appendix D.8). 3.3 Performance with Dataset Size and Compute We display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute C in Figure 1. For the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on  xed subsets of the WebText2 dataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be  t with simple power-law L(D)   \u0012Dc D \u0013 D (3.2) in the dataset size. The data and  t appear in Figure 1. The total amount of non-embedding compute used during training can be estimated as C = 6NBS, where B is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and backward passes. Thus for a given value of C we can scan over all models with various N to  nd the model 9 104 105 106 107 108 109 Parameters (non-embedding) 3 4 5 6 7 Test Loss WebText2 (Test) Internet Books Books Wikipedia Common Crawl 2.5 3.0 3.5 4.0 4.5 5.0 Test Loss on Training Distribution 2.5 3.0 3.5 4.0 4.5 5.0 Loss on Other Distribution Books during training Wikipedia during training Books at convergence Wikipedia at convergence Figure 8 Left: Generalization performance to other data distributions improves smoothly with model size, with only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener- alization performance depends only on training distribution performance, and not on the phase of training. We compare generalization of converged models (points) to that of a single large model (dashed curves) as it trains. with the best performance on step S = C 6BS . Note that in these results the batch size B remains  xed for all models, which means that these empirical results are not truly optimal. We will account for this in later sections using an adjusted Cmin to produce cleaner trends. The result appears as the heavy black line on the left-hand plot in Figure 1. It can be  t with L(C)   \u0012Cc C \u0013 C (3.3) The  gure also includes images of individual learning curves to clarify when individual models are optimal. We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample ef ciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix. 4 Charting the In nite Data Limit and Over tting In Section 3 we found a number of basic scaling laws for language modeling performance. Here we will study the performance of a model of size N trained on a dataset with D tokens while varying N and D simultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling law of Equation (1.5). This provides guidance on how much data we would need to train models of increasing size while keeping over tting under control. 4.1 Proposed L(N, D) Equation We have chosen the parameterization (1.5) (repeated here for convenience): L(N, D) = \"\u0012Nc N \u0013  N  D + Dc D # D (4.1) using three principles: 1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The parameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling. 2. Fixing D and sending N  , the overall loss should approach L(D). Conversely,  xing N and sending D  the loss must approach L(N). 3. L(N, D) should be analytic at D =  , so that it has a series expansion in 1/D with integer powers. Theoretical support for this principle is signi cantly weaker than for the  rst two. Our choice of L(N, D) satis es the  rst requirement because we can rescale Nc, Dc with changes in the vocabulary. This also implies that the values of Nc, Dc have no fundamental meaning. 10 106 107 108 109 Params (non-embed) 2.5 3.0 3.5 4.0 4.5 Test Loss Data Size Bottleneck Data Size 21M 43M 86M 172M 344M 688M 1.4B 22.0B 10 4 10 3 10 2 10 1 N N/ D/D 0.0 0.1 0.2 0.3 0.4 0.5 L/L(D = ) 1 Overfitting Data Size 21M 43M 86M 172M 344M 688M 1.4B 22.0B Figure 9 The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N according to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller  xed D, performance stops improving as N increases and the model begins to over t. (The reverse is also true, see Figure 4.) Right: The extent of over tting depends predominantly on the ratio N  N  D /D, as predicted in equation (4.3). The line is our  t to that equation. Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we expect that larger models should always perform better than smaller models. But with  xed  nite D, we also do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly, a model with  xed size will be capacity-limited. These considerations motivate our second principle. Note that knowledge of L(N) at in nite D and L(D) at in nite N fully determines all the parameters in L(N, D). The third principle is more speculative. There is a simple and general reason one might expect over tting to scale  1/D at very large D. Over tting should be related to the variance or the signal-to-noise ratio of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function, since we expect to be able to expand the loss about the D  limit. However, this argument assumes that 1/D corrections dominate over other sources of variance, such as the  nite batch size and other limits on the ef cacy of optimization. Without empirical con rmation, we would not be very con dent of its applicability. Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and would require the introduction of an additional parameter. In any case, we will see that our equation for L(N, D)  ts the data well, which is the most important justi - cation for our L(N, D) ansatz. 4.2 Results We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer decreasing. The results are displayed in Figure 9, including a  t to the four parameters  N,  D, Nc, Dc in Equation (1.5): Parameter  N  D Nc Dc Value 0.076 0.103 6.4   1013 1.8   1013 Table 2 Fits to L(N, D) We obtain an excellent  t, with the exception of the runs where the dataset has been reduced by a factor of 1024, to about 2   107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates. Perhaps such a tiny dataset represents a different regime for language modeling, as over tting happens very early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in Section 3, as here we are  tting the full L(N, D) rather than just L(N,  ) or L( , D). To chart the borderlands of the in nite data limit, we can directly study the extent of over tting. For all but the largest models, we see no sign of over tting when training with the full 22B token WebText2 dataset, so we can take it as representative of D =  . Thus we can compare  nite D to the in nite data limit by 4For example, one might have used L(N, D) = \u0002\u0000 Nc N \u0001 N + \u0000 Dc D \u0001 D\u0003 , but this does not have a 1/D expansion. 11 101 3   100 4   100 6   100 WebText2 Train Loss 103 104 105 106 Critical Batch Size (Tokens) Critical Batch Size vs. Performance Empirical Bcrit, N = 3M Empirical Bcrit, N = 85M Bcrit = 2.1   108 tokens L 4.8 Noise Scale Measurement Figure 10 The critical batch size Bcrit follows a power law in the loss as performance increase, and does not depend directly on the model size. We  nd that the critical batch size approximately doubles for every 13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly predicted by the gradient noise scale, as in [MKAT18]. de ning  L(N, D)  L(N, D) L(N,  )  1 (4.2) and studying it as a function of N, D. In fact, we see empirically that  L depends only a speci c combination of N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies  L   1 + \u0012 N Nc \u0013  N  D Dc D ! D  1 (4.3) Note that at large D this formula also has a series expansion in powers of 1/D. We estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to avoid over tting when training to within that threshold of convergence we require D  (5   103) N 0.74 (4.4) With this relation, models smaller than 109 parameters can be trained with minimal over tting on the 22B token WebText2 dataset, but our largest models will encounter some mild over tting. More generally, this relation shows that dataset size may grow sub-linearly in model size while avoiding over tting. Note however that this does not typically represent maximally compute-ef cient training. We should also emphasize that we have not optimized regularization (eg the dropout probability) while varying dataset and model size. 5 Scaling Laws with Model Size and Training Time In this section we will demonstrate that a simple scaling law provides a good description for the loss as a function of model size N and training time. First we will explain how to use the results of [MKAT18] to de ne a universal training step Smin, which accounts for the fact that most of our models have not been trained at an optimal batch size. Then we will demonstrate that we can  t the model size and training time dependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation of training compute between model size and training time, and then con rm that prediction. 5.1 Adjustment for Training at Bcrit(L) A simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also [SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit the batch size can be increased with very minimal degradation in compute-ef ciency, whereas for B > Bcrit increases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple 12 prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that has been attained. These results can be used to predict how training time and compute will vary with the batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch size B  Bcrit. Training at B  Bcrit minimizes the number of training steps, while B  Bcrit minimizes the use of compute. More speci cally, it was demonstrated that for a wide variety of neural network tasks, the number of training steps S and the number of data examples processed E = BS satisfy the simple relation \u0012 S Smin  1 \u0013 \u0012 E Emin  1 \u0013 = 1 (5.1) when training to any  xed value of the loss L. Here Smin is the minimum number of steps necessary to reach L, while Emin is the minimum number of data examples that must be processed. We demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation de nes the critical batch size Bcrit(L)  Emin Smin (5.2) which is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal time/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples. In Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for two different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So the predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can be  t with a power-law in the loss Bcrit(L)   B  L1/ B (5.3) where B 2   108 and  B  0.21. We have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin, the gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not know Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural language is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used a parameterization where Bcrit diverges as L  0. We will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch size B = 219 tokens and the number of training steps while training at B  Bcrit. This is simply Smin(S)   S 1 + Bcrit(L)/B (minimum steps, at B  Bcrit) (5.4) for any given target value L for the loss. This also de nes a critical value of the compute needed to train to L with a model of size N if we were to train at B  Bcrit(L). This is Cmin(C)   C 1 + B/Bcrit(L) (minimum compute, at B  Bcrit) (5.5) where C = 6NBS estimates the (non-embedding) compute used at batch size B. 5.2 Results for L(N, Smin) and Performance with Model Size and Compute Now we will use Smin de ned in Equation (5.4) to obtain a simple and universal  t for the dependence of the loss on model size and training time in the in nite data limit. We will  t the stable, Adam-optimized training runs using Equation (1.6), repeated here for convenience: L(N, Smin) = \u0012Nc N \u0013 N + \u0012 Sc Smin \u0013 S (5.6) for the loss. We include all training steps after the warmup period of the learning rate schedule, and  nd a  t to the data with the parameters: 5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of Bcrit from Figures 18 and 10 for all our later analyses. 13 104 106 108 Parameters (non-embedding) 2 3 4 5 6 7 8 Test Loss Performance vs Compute Budget 10 5 10 4 10 3 10 2 10 1 100 PF-dayss 106 107 108 109 Parameters (non-embedding) 2.4 3.0 3.6 4.2 4.8 5.4 Test Loss Performance vs Steps 104 105 Steps Figure 11 When we hold either total compute or number of training steps  xed, performance follows L(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that maximizes performance. Mediocre  ts at small S are unsurprising, as the power-law equation for the learning curves breaks down very early in training. Parameter  N  S Nc Sc Value 0.077 0.76 6.5   1013 2.1   103 Table 3 Fits to L(N, S) With these parameters, we obtain the learning curve  ts in Figure 4. Though the  ts are imperfect, we believe they are quite compelling given the simplicity of Equation (5.6). The data and  ts can be visualized in a different and more interesting way, as shown in Figure 11. There we study the test loss as a function of model size while  xing either the total non-embedding compute C used in training, or the number of steps S. For the  ts we use Equation (5.5) and (5.4) along with the parameters above and Equation (5.6). The power-law dependence of the loss on Smin re ects the interplay of optimizer dynamics and the loss landscape. Since the  ts are best late in training, when the loss may be approximately quadratic, the power- law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that the Hessian eigenvalue density is roughly independent of model size. 5.3 Lower Bound on Early Stopping Step The results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which early stopping should occur when training is data limited. It is motivated by the idea that  nite and in nite D learning curves for a given model will be very similar until we reach Smin  Sstop. Thus over tting should be proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because in reality the test loss will decrease more slowly when we have a  nite D, and therefore we will require more training steps to reach the optimal test loss at  nite D. This line of reasoning leads to the inequality Sstop(N, D)   Sc [L(N, D)  L(N,  )]1/ S (5.7) where L(N,  ) is the converged loss, evaluated with in nite available data. This inequality and its com- parison to the empirical data is displayed in Figure 16 in the appendix. In that  gure, the values of Sstop and L(N, D) are empirical (though Sstop is adjusted to mimic training at B  Bcrit), while L(N,  ) is computed from the  t to L(N, D) evaluated at D =  . 6 Optimal Allocation of the Compute Budget We displayed the empirical trend of performance as a function of the computation used during training in the top-right of Figure 1. However, this result involved training at a  xed batch size B, whereas we know 14 Models between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budget Smaller models require more steps to train, while larger models require fewer Our framework does not capture early training dynamics Figure 12 Left: Given a  xed compute budget, a particular model size is optimal, though somewhat larger or smaller models can be trained with minimal additional compute. Right: Models larger than the compute- ef cient size require fewer steps to train, allowing for potentially faster training if suf cient additional paral- lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve, after initial transient effects. 10 8 10 6 10 4 10 2 100 Compute (PF-days), non-embedding 2 3 4 5 6 7 Test Loss L = (Cmin/2.3 108) 0.050 L = (C/2.0 107) 0.057 Figure 13 When adjusting performance to simulate training far below the critical batch size, we  nd a somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous lump at 10 5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks in the power-law  ts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger compute. that in fact we could train more ef ciently6 by training at the batch size Bcrit discussed in Section 5.1. Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively, and correcting for this inef ciency by standardizing to the critical batch size results in cleaner and more predictable trends. In this section we will adjust for this oversight. More importantly, we will use the results of Section 5 to determine the optimal allocation of compute between model size N and the quantity of data processed during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by using the equation for L(N, Smin), and we will demonstrate that these methods agree. 6.1 Optimal Performance and Allocations Let us  rst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is plotted in Figure 13, along with a power-law  t. We see that as compared to the compute plot of Figure 1, the new  t with Cmin is somewhat improved. Given L(Cmin), it is natural to ask for the optimal model size N(Cmin) that provides the minimal loss with a given quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin) 6One might ask why we did not simply train at Bcrit in the  rst place. The reason is that it depends not only on the model but also on the target value of the loss we wish to achieve, and so is a moving target. 15 10 7 10 5 10 3 10 1 Compute (PF-days), non-embedding 103 105 107 Parameters (non-embedding) N = (1.3 109) C0.73 min N = (1.6 109) C0.88 10 7 10 5 10 3 10 1 Compute (PF-days), excluding embeddings 0 5000 10000 15000 Steps Smin (adjusted) Smin = (5.4 103) C0.03 min S (fixed-batch) Figure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x. Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most of the growth in data examples processed can be used for increased batch sizes. can be  t very well with a power-law N(Cmin)  (Cmin)0.73. (6.1) In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4). By de nition Cmin  6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since prior  ts show B  L 4.8 and L  C 0.05 min , we can conclude that Bcrit  C0.24 min . This leads us to conclude that the optimal number of steps will only grow very slowly with compute, as Smin  (Cmin)0.03, (6.2) matching the empirical results in Figure 14. In fact the measured exponent is suf ciently small that our results may even be consistent with an exponent of zero. Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size N, while simultaneously scaling up the batch size via B   Bcrit with negligible increase in the number of serial steps. Since compute-ef cient training uses relatively few optimization steps, additional work on speeding up early training dynamics may be warranted. 6.2 Predictions from L(N, Smin) The results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in Section 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin 6NB and then  nd the minimum of the loss as a function of N, while  xing the training compute. We carry out this procedure in detail in Appendix B, where we also provide some additional predictions. For the loss as a function of training compute, we predict that L(Cmin) = \u0012Cmin c Cmin \u0013 min C (6.3) where  min C   1 1/ S + 1/ B + 1/ N  0.054 (6.4) in excellent agreement with the exponent of Figure 13. We also predict that N(Cmin)  (Cmin) min C / N  (Cmin)0.71 (6.5) which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive framework for the performance of language modeling. 16 The intersection point is sensitive to the precise power-law parameters Figure 15 Far beyond the model sizes we study empirically, we  nd a contradiction between our equations for L(Cmin) and L(D) due to the slow growth of data needed for compute-ef cient training. The intersection marks the point before which we expect our predictions to break down. The location of this point is highly sensitive to the precise exponents from our power-law  ts. 6.3 Contradictions and a Conjecture We observe no signs of deviation from straight power-law trends at large values of compute, data, or model size. Our trends must eventually level off, though, since natural language has non-zero entropy. Indeed, the trends for compute-ef cient training described in this section already contain an apparent contra- diction. At scales several orders of magnitude above those documented here, the performance predicted by the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with compute. This implies that our scaling laws must break down before this point, but we conjecture that the intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language models reach maximal performance. Since the amount of data used by compute-ef cient training grows slowly with the compute budget, the performance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15). Let us work this out in more detail. To keep over tting under control, the results of Section 4 imply that we should scale the dataset size as D  N 0.74  C0.54 min (6.6) where we have used the compute-ef cient N(Cmin) from Figure 14. Let us compare this to the data requirements of compute-ef cient training. If we train at the critical batch size (i.e. C = 2Cmin) and never re-use data during training, we  nd that data usage grows with compute as D(Cmin) = 2Cmin 6N(Cmin)   \u00004   1010 tokens \u0001 (Cmin/PF-Day)0.26 (6.7) This is the maximum rate at which the dataset size can productively grow with compute, since it means that we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6). It appears to imply that compute-ef cient training will eventually run into a problem with over tting, even if the training process never re-uses any data! According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by over tting), the loss should scale as L(D)  D 0.095. This implies that the loss would scale with compute as L(D(Cmin))   C 0.03 min once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin)  C 0.050 min . The intersection point of L(D(Cmin)) and L(Cmin) occurs at C 104 PF-Days N  1012 parameters, D 1012 tokens, L 1.7 nats/token (6.8) though the numerical values are highly uncertain, varying by an order or magnitude in either direction de- pending on the precise values of the exponents from the power-law  ts. The most obvious interpretation is that our scaling laws break down at or before we reach this point, which is still many orders of magnitude away in both compute and model size. 17 One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model size beyond N  without qualitatively different data requirements, perhaps this means that once we reach C  min and N  , we have extracted all of the reliable information available in natural language data. In this interpretation, L would provide a rough estimate for the entropy-per-token7 of natural language. In this scenario, we would expect the loss trend to level off at or before L . We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training dataset with added noise. For example, we could append a random string of tokens to each context shown to the model to arti cially boost the loss by a constant additive factor. Then, the distance from the noise  oor L  Lnoise would be a more meaningful performance metric, with even a small decrease in this distance potentially representing a signi cant boost in qualitative performance. Since the arti cial noise would affect all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L ), and may be meaningful even if it occurs after the leveling off. 7 Related Work Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results. These models suggest that power-law exponents may have a very rough interpretation as the inverse of the number of relevant features in the data. Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is perhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of dataset size with model size, whereas we  nd a sub-linear scaling. There are some parallels between our  ndings on optimal allocation of compute and [Kom19], including power-law learning curves. Ef cientNets [TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and  ts an ansatz similar to ours. Ef cientNet [TL19] advocates scaling depth and width exponentially (with different coef cients) for optimal performance of image models, resulting in a power-law scaling of width as a function of depth. We  nd that for language models this power should be roughly one when scaling up (as width/depth should remain  xed). But more importantly, we  nd that the precise architectural hyperparameters are unimportant compared to the overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles of shallower models, which could potentially explain this  nding. Earlier work [ZK16] has compared width and depth, and found that wide ResNets can outperform deep ResNets on image classi cation. Some studies  x computation per data example, which tends to scale in proportion to the number of model parameters, whereas we investigate scaling with both model size and the quantity of training computation. Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models,  nd- ing a  jamming transition  [GJS+19] when the model size reaches the dataset size (this may require training many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do not observe such a transition, and  nd that the necessary training data scales sublinearly in the model size. Expansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions [ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the Hessian spectrum [Pap18, GKX19, GARD18]. 8 Discussion We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and (1.6). Conversely, we  nd very weak dependence on many architectural and optimization hyperparameters. Since scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale. 7De ning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token. 8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of loss on both model and dataset size. 18 We were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude of over tting, early stopping step, and data requirements when training large language models. So our scaling relations go beyond mere observation to provide a predictive framework. One might interpret these relations as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way, independent of most of the details of its microscopic consituents. It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to test these relations on other domains, such as images, audio, and video models, and perhaps also for random network distillation. At this point we do not know which of our results depend on the structure of natural language data, and which are universal. It would also be exciting to  nd a theoretical framework from which the scaling relations can be derived: a  statistical mechanics  underlying the  thermodynamics  we have observed. Such a theory might make it possible to derive other more precise predictions, and provide a systematic understanding of the limitations of the scaling laws. In the domain of natural language, it will be important to investigate whether continued improvement on the loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major qualitative improvements:  more is different . For example, the smooth aggregate growth of the economy provides no indication of the speci c technological developments that underwrite it. Similarly, the smooth improvements in language model loss may hide seemingly qualitative changes in capability. Our results strongly suggest that larger models will continue to perform better, and will also be much more sample ef cient than has been previously appreciated. Big models may be more important than big data. In this context, further investigation into model parallelism is warranted. Deep models can be trained using pipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased batch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization [SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity [CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through increased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train, it might be possible to remain on the compute-ef cient frontier for an entire training run. Acknowledgements We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner, Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed- back on drafts of this work. 19 Appendices A Summary of Power Laws For easier reference, we provide a summary below of the key trends described throughout the paper. Parameters Data Compute Batch Size Equation N     Fixed L (N) = (Nc/N) N   D Early Stop Fixed L (D) = (Dc/D) D Optimal   C Fixed L (C) = (Cc/C) C (naive) Nopt Dopt Cmin B  Bcrit L (Cmin) = \u0000Cmin c /Cmin \u0001 min C N D Early Stop Fixed L (N, D) = \u0014\u0000 Nc N \u0001  N  D + Dc D \u0015 D N   S steps B L (N, S) = \u0000 Nc N \u0001 N + \u0010 Sc Smin(S,B) \u0011 S Table 4 The empirical  tted values for these trends are: Power Law Scale (tokenization-dependent)  N = 0.076 Nc = 8.8   1013 params (non-embed)  D = 0.095 Dc = 5.4   1013 tokens  C = 0.057 Cc = 1.6   107 PF-days  min C = 0.050 Cmin c = 3.1   108 PF-days  B = 0.21 B = 2.1   108 tokens  S = 0.76 Sc = 2.1   103 steps Table 5 The optimal parameters for compute ef cient training are given by: Compute-Ef cient Value Power Law Scale Nopt = Ne   CpN min pN = 0.73 Ne = 1.3   109 params B  Bcrit = B  L1/ B = BeCpB min pB = 0.24 Be = 2.0   106 tokens Smin = Se   CpS min (lower bound) pS = 0.03 Se = 5.4   103 steps Dopt = De   CpD min (1 epoch) pD = 0.27 De = 2   1010 tokens Table 6 B Empirical Model of Compute-Ef cient Frontier Throughout this appendix all values of C, S, and  C are adjusted for training at the critical batch size Bcrit. We have left off the  adj  label to avoid cluttering the notation. B.1 De ning Equations The power-law  t to the learning curves implies a simple prescription for compute-ef cient training. In this appendix, we will derive the optimal performance, model size, and number of training steps as a function of 20 the compute budget. We start with the Equation (1.6), repeated here for convenience: L (N, S) = \u0012Nc N \u0013 N + \u0012Sc S \u0013 S . (B.1) Here, S represents the number of parameter updates when training at the critical batch size [MKAT18], which was de ned in Equation (5.2)9: B (L) = B  L1/ B . (B.2) We would like to determine optimal training parameters for a  xed compute budget, so we replace S = C/ (6NB (L)), where C is the number of FLOPs used in the training run: L (N, C) = \u0012Nc N \u0013 N + \u0012 6B Sc N L1/ BC \u0013 S . (B.3) Now, we set  NL C = 0 to  nd the condition for optimality: 0 =  L  N C =  N N \u0012Nc N \u0013 N +  S N \u0012 6B Sc N L1/ BC \u0013 S \u0012 1  5N L\u001a\u001a\u001a  L  N C \u0013 = N  S \u0012Nc N \u0013 N = \u0012 6B Sc N L1/ BC \u0013 S (B.4) Equation (B.3) and (B.4) together determine the compute-ef cient frontier. B.2 Ef cient Training Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields L (Ne (C) , C) = \u0012 1 +  N  S \u0013 L (Ne ,  ) , (B.5) which implies that for compute-ef cient training, we should train to a  xed percentage  N  S  10% above the converged loss. Next, let s determine how the optimal loss depends on the compute budget. Eliminating N yields a power-law dependence of performance on compute: L (C) = \u0012Cc C \u0013 C (B.6) where we de ned  C = 1/ (1/ S + 1/ B + 1/ N)  0.052 (B.7) Cc = 6NcB Sc \u0012 1 +  N  S \u00131/ S+1/ N \u0012  S  N \u00131/ S . (B.8) Similarly, we can eliminate L to  nd N (C): N (C) Nc = \u0012 C Cc \u0013 C/ N \u0012 1 +  N  S \u00131/ N (B.9) and S (C) = Cc 6NcB  \u0012 1 +  N  S \u0013 1/ N \u0012 C Cc \u0013 C/ S (B.10) 9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could instead train at a variable batch size  B (L), where  B is the instantaneous critical batch size (as opposed to B, which is the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see [MKAT18]). 21 B.3 Comparison to Inef cient Typically, researchers train models until they appear to be close to convergence. In this section, we compare the ef cient training procedure described above to this more typical setup. We de ne a the convergence factor f as the percent deviation from the converged loss: L (N, C) = (1 + f) L (N,  ) . (B.11) For compute-ef cient training we have f =  N/ S  10% from the previous section, but researchers typically use a much smaller value. Here, we choose f   = 2% as an estimate. For a  xed value of the loss, we predict: Nf Nf   = \u0012 1 + f 1 + f   \u00131/ N  2.7 (B.12) Sf Sf   = 1 + 1 f 1 + 1 f   !1/ S  0.13 (B.13) Cf Cf   = Nf Nf   Sf Sf    0.35 (B.14) So that compute-ef cient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less compute to reach the same loss. B.4 Suboptimal Model Sizes We can solve A.1 to  nd an expression for the amount of compute needed to reach a given value of the loss L with a model of size N: C (N, L) = \u0012 6B Sc N L1/ B \u0013 \u0012 L   \u0012Nc N \u0013 N \u0013 1/ S . (B.15) Using A.6 and A.9, we can eliminate L in favor of Ne (L), the model size which reaches L most ef ciently. From there, we  nd an expression for the excess compute needed as a consequence of using a suboptimal model size: C (N, Ne ) C (Ne , Ne ) = N Ne  \u0014 1 +  S  N \u0012 1   \u0012Ne  N \u0013 N \u0013\u0015 1/ S . (B.16) The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a 20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism and faster training if suf cient harware is available (see Figure Y): S (N, Ne ) S (Ne , Ne ) = \u0014 1 +  S  N \u0012 1   \u0012Ne  N \u0013 N \u0013\u0015 1/ S . (B.17) A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve after initial transient effects. C Caveats In this section we list some potential caveats to our analysis.   At present we do not have a solid theoretical understanding for any of our proposed scaling laws. The scaling relations with model size and compute are especially mysterious. It may be possible to understand scaling at very large D holding model size  xed [AS17], and also the shape of learning curves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very large model size still remains mysterious. Without a theory or a systematic understanding of the corrections to our scaling laws, it s dif cult to determine in what circumstances they can be trusted. 22 103 104 105 Sc   [L(N, D) L(N, )] 1/ S 103 104 105 Sstop Early Stopping Step Data Size 21M 43M 86M 172M 344M 688M 1.4B 103 104 105 Step 2 3 4 5 6 Loss Test Loss Train Loss 108 109 1010 Dataset Size (Tokens) Figure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of over tting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right: We display train and test loss for a series of 300M parameter models trained on different sized dataset sub- samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the degree of over tting (as compared to the in nite data limit) is signi cantly overestimated by Ltest  Ltrain (denoted by a black bar for each run).   We are not especially con dent in the prediction of Bcrit(L) for values of the loss far outside the range we have explored. Changes in Bcrit could have a signi cant impact on trade-offs between data parallelism and the number of serial training steps required, which would have a major impact on training time.   We did not thoroughly investigate the small data regime, and our  ts for L(N, D) were poor for the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did not experiment with regularization and data augmentation. Improvements in these could alter our results, quantitatively or qualitatively.   We used the estimated training compute C  6NBS, which did not include contributions propor- tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the regime of very large nctx, speci cally where nctx  12dmodel.   We tuned learning rates, and we experimented with learning rate schedules. But we may have neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important effect on scaling.   The optimal choice of learning rate is sensitive to the target loss. When training close to convergence, it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did not experiment with higher learning rates for training runs that did not proceed to convergence. D Supplemental Figures D.1 Early Stopping and Test vs Train In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on the early stopping step. We also show the train and test loss for a given model size when training on different sized datasets. D.2 Universal Transformers We compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17. These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a function of compute C. We include several different different possibilities for parameter re-use. D.3 Batch Size We measure the critical batch size using the data displayed in  gure 18. This made it possible to estimate Bcrit(L) in  gure 10. 23 105 106 107 108 109 Parameters, including reuse (non-embedding) 2.5 3.0 3.5 4.0 4.5 Test Loss 2x Reuse 4x Reuse 8x Reuse Non-recurrent Models 105 106 107 108 109 Parameters (non-embedding) 2.5 3.0 3.5 4.0 4.5 Test Loss 2x Reuse 4x Reuse 8x Reuse Non-recurrent Models Figure 17 We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans- formers. Recurrent Transformers perform slightly better when comparing models with equal parameter count, but slightly worse when accounting for reuse and comparing per FLOP. 102 103 104 105 Step 106 107 108 109 1010 1011 Tokens Processed Batch Size Scan - 3M Params 4 6 8 10 Test Loss 101 102 103 104 105 Step 106 108 1010 Tokens Processed Batch Size Scan - 85M Params 4 6 8 10 Test Loss Figure 18 These  gures demonstrate  ts to Equation (5.1) for a large number of values of the loss L, and for two different Transformer model sizes. These  ts were used to measure Bcrit(L) for Figure 10. D.4 Sample Ef ciency vs Model Size It is easy to see from  gure 2 that larger models train faster, and are therefore more sample ef cient. We provide another way of looking at this phenomenon in  gure 19, which shows when different models reach various  xed values of the loss. 106 107 108 Parameters (non-embedding) 103 104 105 Minimum Steps (Smin) 2.5 3.0 3.5 4.0 4.5 5.0 5.5 Loss 106 107 108 Parameters (non-embedding) 108 109 1010 1011 Minimum Examples (Emin) 2.5 3.0 3.5 4.0 4.5 5.0 5.5 Loss Figure 19 The number of minimum serial steps needed to reach any  xed value of the test loss decreases precipitously with model size. Sample ef ciency (show here for training far below the critical batch size) improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model to a very large one. 24 100 101 102 103 Token Index 3 4 5 6 7 8 Per-Token Test Loss 4.0 + 3.2 T 0.47 3.4 + 4.0 T 0.56 2.9 + 4.5 T 0.56 2.7 + 4.9 T 0.60 2.4 + 5.1 T 0.61 2.3 + 5.4 T 0.62 106 107 108 Model Parameters 101 103 105 Step 2 4 6 8 10 Test Loss Per-token Loss (774M Params) 100 101 102 103 Token Index Figure 20 This  gure provides information about the performance per token as a function of model size and training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales predictably as a power-law in T. Right: Test loss per token as a function of training step. 104 105 106 107 108 109 Parameters (excl. embedding) 3.0 4.5 6.0 7.5 Test Loss Token 1/1024 Token 2/1024 Token 4/1024 Token 8/1024 Token 16/1024 Token 64/1024 Token 256/1024 Token 1024/1024 Token 1/8 Token 2/8 Token 4/8 Token 8/8 Figure 21 In addition to the averaged loss, individual tokens within the 1024-token context also improve smoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better on early tokens, since they can allocate all of their capacity to them. D.5 Context Dependence The trends for loss as a function of model size are displayed for different tokens in the context in Figure 21. We see that models trained on nctx = 1024 show steady improvement with model size on all but the  rst token. Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12, LT16], or a more general feature of the model architecture and optimization. It provides some suggestion for the potential bene ts (or lack thereof) from training on larger contexts. Not only do larger models converge to better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger models are more ef cient at detecting patterns with less contextual information. In the right-hand plot we show how per-token performance varies for a  xed model as a function of the training step. The model begins by learning short-range information, and only learns longer-range correlations later in training. We have also included models trained with a tiny context nctx = 8 in order to compare with our longer context models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024 models on very early tokens. This also suggests that further improvements should be possible with much larger models trained on large contexts. D.6 Learning Rate Schedules and Error Analysis We experimented with a variety of learning rates and schedules. A host of schedules and resulting test performances for a small language model are plotted in Figure 22. We conclude that the choice of learning rate schedule is mostly irrelevant, as long as the total summed learning rate is suf ciently large, and the schedule includes a warmup period and a  nal decay to near-vanishing learning rate. Variations among 25 0 50000 100000 150000 200000 250000 Step 0.0000 0.0002 0.0004 0.0006 0.0008 0.0010 Learning Rate 50 100 150 200 250 LR Summed Over Steps 3.65 3.70 3.75 3.80 3.85 3.90 Loss Figure 22 We test a variety of learning rate schedules including cosine decay, linear decay, as well as other faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we do not decay to zero, since we  nd that this tends to give a  xed improvement close to the end of training. We  nd that, as long as the learning rate is not too small and does not decay too quickly, performance does not depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging multiple runs is necessary to validate performance changes smaller than this level. 104 105 106 107 108 109 Parameters (non-embedding) 2 3 4 5 6 Test Loss (at convergence) L = (N/8.8 1013) 0.076 L = 0.25log(N/7.1 1012) Figure 23 The trend for performance as a function of parameter count, L(N), is  t better by a power law than by other functions such as a logarithm at a qualitative level. schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different training runs. Experiments on larger models suggest that the variation in the  nal test loss between different random seeds is roughly constant in magnitude for different model sizes. We found that larger models require a smaller learning rate to prevent divergence, while smaller models can tolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs: LR(N)  0.003239 +  0.0001395 log(N) (D.1) We expect that this formula could be improved. There may be a dependence on network width, likely set by the initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that it works suf ciently well for the models we considered. D.7 Fit Details and Power Law Quality We experimented with a number of functional forms for the  ts to L(N), L(C), and L(D); the power-law  ts were qualitatively much more accurate than other functions such as logarithms (see Figure 23). For L(C), we do not include small models with only 1 layer in the  t, as the transition from 1 to 2 layers causes a noticable lump in the data. For L(N) we also do not include very small models with only 1 layer in the  t, and we exclude the largest models that have not trained fully to convergence. Fit parameters change marginally if we do include them, and the trend extrapolates well in both directions regardless. D.8 Generalization and Architecture In  gure 24 we show that generalization to other data distributions does not depend on network depth when we hold the total parameter count  xed. It seems to depend only on the performance on the training distribution. 26 101 102 Depth 2.3 2.4 2.5 2.6 2.7 2.8 Test Loss Wikipedia Books Internet Books Common Crawl WebText2 (Train) WebText2 (Test) Figure 24 We show evaluations on a series of datasets for models with approximately 1.5 Billion param- eters. We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance. The 12-layer model over t the Internet Books dataset and we show the early-stopped performance; we have not seen this surprising result in other experiments. List of Figures 1 Summary of simple power laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2 Illustration of sample ef ciency and compute ef ciency. . . . . . . . . . . . . . . . . . . . . 4 3 How to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . . 4 4 Performance when varying model and data size, or model and training steps, simultaneously 5 5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . . 8 6 Comparison of performance trend when including or excluding embeddings . . . . . . . . . 8 7 LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . . 9 8 Generalization to other test datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 9 Universality of over tting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 10 Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 11 Performance versus compute budget or number of parameter updates . . . . . . . . . . . . . 14 12 Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 13 Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . . 15 14 Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . . 16 15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . . 17 16 Early stopping lower bound and training curves for over t models . . . . . . . . . . . . . . 23 17 Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 18 Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 19 Another look at sample ef ciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 20 Power-law dependence of performance on position in context . . . . . . . . . . . . . . . . . 25 21 Performance at different context positions versus model size . . . . . . . . . . . . . . . . . 25 22 Learning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 23 Comparison of Power-Law and Logarithmic Fits . . . . . . . . . . . . . . . . . . . . . . . 26 24 Generalization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 27 List of Tables 1 Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . . 7 2 Fits to L(N, D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3 Fits to L(N, S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4 Key trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 5 Key parameters to trend  ts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 6 Trends for compute-ef cient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 References [ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long- range correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582  11587, 2012. 25 [AS17] Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv, 2017, 1710.03667. 11, 18, 22 [BB01] Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disam- biguation. In Proceedings of the 39th annual meeting on association for computational linguis- tics, pages 26 33. Association for Computational Linguistics, 2001. 18 [BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18 [Bia12] G rard Biau. Analysis of a random forests model. Journal of Machine Learning Research, 13(Apr):1063 1095, 2012. 18 [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/ abs/1904.10509. 19 [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2 [DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni- versal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/ abs/1807.03819. 6, 9, 23, 24 [EP94] Werner Ebeling and Thorsten P schel. Entropy and long-range correlations in literary english. EPL (Europhysics Letters), 26(4):241, 1994. 25 [Fou] The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7 [GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. 2018, arXiv:1812.04754. 18 [GJS+19] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St phane d Ascoli, Giulio Biroli, Cl ment Hongler, and Matthieu Wyart. Scaling description of generalization with number of parameters in deep learning. arXiv, 2019, 1901.01608. 18 [GKX19] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op- timization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL http://arxiv.org/abs/1901.10159. 18 [Goo01] Joshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL http://arxiv.org/abs/cs.CL/0108005. 18 [GRK17] Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope- nai.com, 2017. 19 [HAD19] Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu- tational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming, PPoPP  19, pages 1 14, New York, NY, USA, 2019. ACM. doi:10.1145/3293883.3295710. 18 28 [HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. Gpipe: Ef cient training of giant neural networks using pipeline parallelism. CoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19 [HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia- ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre- dictable, empirically, 2017, 1712.00409. 18 [JGH18] Arthur Jacot, Franck Gabriel, and Cl ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571 8580, 2018. 18 [KB14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014, 1412.6980. 7 [Kom19] Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18 [KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi cation with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS 12, pages 1097 1105, USA, 2012. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19 [LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations, 2019, 1909.11942. 9 [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain- ing approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/ 1907.11692. 2 [LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs], 2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6 [LT16] Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv preprint arXiv:1606.06737, 2016. 25 [LXS+19] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent, 2019, arXiv:1902.06720. 18 [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21 [Pap18] Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size. CoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18 [RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6 [RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019, 1909.12673. 18 [RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019, arXiv:1909.12673. 18 [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uni ed text-to-text transformer, 2019, arXiv:1910.10683. 2 [RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8 [SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan- takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensor ow: Deep learning for supercomputers, 2018, 1811.02084. 19 [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. CoRR, 2015, 1508.07909. 6 29 [SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018, arXiv:1811.03600. 12 [SS18] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235. 7 [THK18] Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems. Oxford University Press, 2018. 18 [TL19] Mingxing Tan and Quoc V. Le. Ef cientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905. 11946. 18 [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998 6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6 [VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18 [Was06] Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006. 18 [WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2019, 1905.00537. 2 [WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in- creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19 [WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional networks, 2019, 1906.02909. 19 [YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2019, arXiv:1906.08237. 2 [ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18 [ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. 2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7 [ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL http://arxiv.org/abs/1907.04164. 12, 18 30"
    },
    {
        "title": "2106.00001",
        "year": "2021",
        "file_name": "2106.00001.pdf",
        "full_text": "Privately Learning Subspaces Vikrant Singhal * Thomas Steinke  Abstract Private data analysis suffers a costly curse of dimensionality. However, the data often has an underlying low-dimensional structure. For example, when optimizing via gradient de- scent, the gradients often lie in or near a low-dimensional subspace. If that low-dimensional structure can be identi ed, then we can avoid paying (in terms of privacy or accuracy) for the high ambient dimension. We present differentially private algorithms that take input data sampled from a low- dimensional linear subspace (possibly with a small amount of error) and output that subspace (or an approximation to it). These algorithms can serve as a pre-processing step for other procedures. 1 Introduction Differentially private algorithms generally have a poor dependence on the dimensionality of their input. That is, their error or sample complexity grows polynomially with the dimension. For example, for the simple task of estimating the mean of a distribution supported on [0,1]d, we have per-coordinate error  (   d/n) to attain differential privacy, where n is the number of samples. In contrast, the non-private error is  ( p log(d)/n). This cost of dimensionality is inherent [BUV14; SU17; DSSUV15]. Any method with lower error is susceptible to tracing attacks (a.k.a. membership inference attacks). However, these lower bounds only apply when the data distribution is  high-entropy.  This leaves open the posssibility that we can circumvent the curse of dimensionality when the data has an underlying low-dimensional structure. Data often does possess an underlying low-dimensional structure. For example, the gradients that arise in deep learning tend to be close to a low-dimensional subspace [ACGMMTZ16; LXTSG17; GARD18; LFLY18; LGZCB20; ZWB20; FT20]. Low dimensionality can arise from meaningful relationships that are at least locally linear, such as income versus tax paid. It can also arise because we are looking at a function of data with relatively few attributes. A long line of work [BLR08; HT10; HR10; Ull15; BBNS19; BCMNUW20; ZWB20; KRRT20, etc.] has shown how to exploit structure in the data to attain better privacy and accuracy. *Northeastern University. Part of this work was done during an internship at IBM Research   Al- maden. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . singhal.vi@northeastern.edu  Google Research, Brain Team. Part of this work was done at IBM Research   Al- maden. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . subspace@thomas-steinke.net 1 arXiv:2106.00001v3 [cs.CR] 10 Aug 2021 However, these approaches assume that this structure is known a priori or that it can be learned from non-private sources. This raises the question: Can we learn low-dimensional structure from the data subject to differential privacy? We consider the simple setting where the data lies in Rd but is in, or very close to a linear sub- space, of dimension k. We focus on the setting where k  d and we develop algorithms whose sample complexity does not depend on the ambient dimension d; a polynomial dependence on the true dimension k is unavoidable. Our algorithms identify the subspace in question or, if the data is perturbed slightly, an approximation to it. Identifying the subspace structure is interesting in its own right, but it also can be used as a pre-processing step for further analysis   by projecting to the low-dimensional subspace, we ensure subsequent data analysis steps do not need to deal with high-dimensional data. 1.1 Our Contributions: Privately Learning Subspaces   Exact Case We  rst consider the exact case, where the data X1,  ,Xn  Rd are assumed to lie in a k-dimensional subspace (rather than merely being near to it)   i.e., rank(A) = k, where A = Pn i XiXT i  Rd d. In this case, we can also recover the subspace exactly. However, we must also make some non-degeneracy assumptions. We want to avoid a pathological input dataset such as the following. Suppose X1,  ,Xk are linearly independent, but Xk = Xk+1 = Xk+2 =   = Xn. While we can easily reveal the repeated data point, we cannot reveal anything about the other points due to the privacy constraint. A natural non-degeneracy assumption would be to assume that the data points are in  general position    that is, that there are no non-trivial linear dependencies among the data points. This means that every set of k data points spans the subspace or, equivalently, no subspace of dimension k  1 contains more than k  1 data points. This is a very natural assumption   if the data consists of n samples from a continuous distribution on the subspace, then this holds with probability 1. We relax this assumption slightly and assume that no subspace of dimension k  1 contains more than  data points. We also assume that all points are non-zero. Note that we de ne subspaces to pass through the origin; our results can easily be extended to af ne subspaces. Theorem 1.1 (Main Result   Exact Case). For all n,d,k, N and  ,  > 0 satisfying n  O \u0010  + log(1/ )   \u0011 , there exists a randomized algorithm M : Rd n  Sk d satisfying the following. Here Sk d denotes the set of all k-dimensional subspaces of Rd.   M is ( , )-differentially private with respect to changing one column of its input.   Let X = (X1,  ,Xn)  Rd n. Suppose there exists a k-dimensional subspace S Sk d that contains all but  of the points   i.e., |{i  [n] : Xi  S }|  n . Further suppose that any (k 1)-dimensional subspace contains at most  points   i.e., for all S  Sk 1 d , we have |{i  [n] : Xi  S}|  . Then P[M(X) = S ] = 1. 2 The parameter  in Theorem 1.1 can be thought of as a robustness parameter. Ideally the data points are in general position, in which case  = k  1. If a few points are corrupted, then we increase  accordingly; our algorithm can tolerate the corruption of a small constant fraction of the data points. Theorem 1.1 is optimal in the sense that n   \u0010  + log(1/ )   \u0011 samples are required. 1.2 Our Contributions: Privately Learning Subspaces   Approximate Case Next we turn to the substantially more challenging approximate case, where the data X1,  ,Xn  Rd are assumed to be close to a k-dimensional subspace, but are not assumed to be contained within that subspace. Our algorithm for the exact case is robust to changing a few points, but very brittle if we change all the points by a little bit. Tiny perturbations of the data points (due to numerical errors or measurement imprecision) could push the point outside the subspace, which would cause the algorithm to fail. Thus it is important to for us to cover the approximate case and our algorithm for the approximate is entirely different from our algorithm for the exact case. The approximate case requires us to precisely quantify how close the input data and our output are to the subspace and we also need to make quantitative non-degeneracy assumptions. It is easiest to formulate this via a distributional assumption. We will assume that the data comes from a Gaussian distribution where the covariance matrix has a certain eigenvalue gap. This is a strong assumption and we emphasize that this is only for ease of presentation; our algorithm works under weaker assumptions. Furthermore, we stress that the differential privacy guarantee is worst-case and does not depend on any distributional assumptions. We assume that the data is drawn from a multivariate Gaussian N (0, ). Let  1( )    2( )    d( ) be the eigenvalues of    Rd d. We assume that there are k large eigen- values  1( ),  , k( )   these represent the  signal  we want   and d  k small eigenvalues  k+1( ),  , d( )   these are the  noise . Our goal is to recover the subspace spanned by the eigenvectors corresponding to the k largest eigenvalues  1( ),  , k( ). Our assumption is that there is a large multiplicative gap between the large and small eigenvalues. Namely, we assume  k+1( )  k( )   1 poly(d). Theorem 1.2 (Main Result   Approximate Case). For all n,d,k  N and  , , ,  > 0 satisfying n  k log(1/ )   + ln(1/ )ln(ln(1/ )/ )   ! and  2    2n d2k log(1/ )  min (1 k , 1 log(k log(1/ )/ ) )! , there exists an algorithm M : Rd n  Sk d satisfying the following. Here Sk d is the set of all k-dimensional subspaces of Rd represented as projection matricies   i.e., Sk d = {   Rd d :  2 =   =  T ,rank( ) = k}.   M is ( , )-differentially private with respect to changing one column of its input.   Let X1,  ,Xn be independent samples from N (0, ). Let  1( )  2( )    d( ) be the eigenvalues of    Rd d. Suppose  k+1( )  2    k( ). Let    Sk d be the projection matrix onto the subspace spanned by the eigenvectors corresponding to the k largest eigenvalues of  . Then P[ M(X)  ]  0.7. 3 The sample complexity of our algorithm n = O(k log(1/ )/ ) is independent of the ambient dimension d; this is ideal. However, there is a polynomial dependence on d in  , which controls the multiplicative eigenvalue gap. This multiplicative eigenvalue gap is a strong assumption, but it is also a necessary assumption if we want the sample complexity n to be independent of the dimension d. In fact, it is necessary even without the differential privacy constraint [CZ16]. That is, if we did not assume an eigenvalue gap that depends polynomially on the ambient dimension d, then it would be impossible to estimate the subspace with sample complexity n that is independent of the ambient dimension d even in the non-private setting. Our algorithm is based on the subsample and aggregate framework [NRS07] and a differ- entially private histogram algorithm. These methods are generally quite robust and thus our algorithm is, too. For example, our algorithm can tolerate o(n/k) input points being corrupted arbitrarily. We also believe that our algorithm s utility guarantee is robust to relaxing the Gaussianity assumption. All that we require in the analysis is that the empirical covariance matrix of a few samples from the distribution is suf ciently close to its expectation   with high probability. 1.3 Related Work To the best of our knowledge, the problem of privately learning subspaces, as we formulate it, has not been studied before. However, a closely-related line of work is on Private Principal Component Analysis (PCA) and low-rank approximations. We brie y discuss this extensive line of work below, but  rst we note that, in our setting, all of these techniques have a sample complexity n that grows polynomially with the ambient dimension d. Thus, they do not evade privacy s curse of dimensionality. However, we make a stronger assumption than these prior works   namely, we assume a large multiplicative eigenvalue gap. (Many of the prior works consider an additive eigenvalue gap, which is a weaker assumption.) There has been a lot of interest in Private PCA, matrix completion, and low-rank approx- imation. One motivation for this is the infamous Net ix prize, which can be interpreted as a matrix completion problem. The competition was cancelled after researchers showed that the public training data revealed the private movie viewing histories of many of Net ix s customers [NS06]. Thus privacy is a real concern for matrix analysis tasks. Many variants of these problems have been considered: Some provide approximations to the data matrix X = (X1,  ,Xn)  Rd n; others approximate the covariance matrix A = Pn i XiXT i   Rd d (as we do). There are also different forms of approximation   we can either produce a subspace or an approximation to the entire matrix, and the approximation can be measured by different norms (we consider the operator norm between projection matrices). Importantly, we de ne differential privacy to allow one data point Xi to be changed arbitrarily, whereas most of the prior work assumes a bound on the norm of the change or even assumes that only one coordinate of one vector can be changed. In the discussion below we focus on the techniques that have been considered for these problems, rather than the speci c results and settings. Dwork, Talwar, Thakurta, and Zhang [DTTZ14] consider the simple algorithm which adds independent Gaussian noise to each of entries of the covariance matrix A, and then perform analysis on the noisy matrix. (In fact, this algorithm predates the development of differential 4 privacy [BDMN05] and was also analyzed under differential privacy by McSherry and Mironov [MM09] and Chaudhuri, Sarwate, and Sinha [CSS12].) This simple algorithm is versatile and several bounds are provided for the accuracy of the noisy PCA. The downside of this is that a polynomial dependence on the ambient dimension d is inherent   indeed, they prove a sample complexity lower bound of n =  (   d) for any algorithm that identi es a useful approximation to the top eigenvector of A. This lower bound does not contradict our results because the relevant inputs do not satisfy our near low-rank assumption. Hardt and Roth [HR12] and Arora, Braverman, and Upadhyay [ABU18] apply techniques from dimensionality reduction to privately compute a low-rank approximation to the input matrix X. Hardt and Roth [HR13] and Hardt and Price [HP13] use the power iteration method with noise injected at each step to compute low-rank approximations to the input matrix X. In all of these, the underlying privacy mechanism is still noise addition and the results still require the sample complexity to grow polynomially with the ambient dimension to obtain interesting guarantees. (However, the results can be dimension-independent if we de ne differential privacy so that only one entry   as opposed to one column   of the matrix X can be changed by 1. This is a signi cantly weaker privacy guarantee.) Blocki, Blum, Datta, and Sheffet [BBDS12] and Sheffet [She19] also use tools from dimen- sionality reduction; they approximate the covariance matrix A. However, they show that the dimensionality reduction step itself provides a privacy guarantee (whereas the aforementioned results did not exploit this and relied on noise added at a later stage). Sheffet [She19] analyzes two additional techniques   the addition of Wishart noise (i.e., Y Y T where the columns of Y are independent multivariate Gaussians) and sampling from an inverse Wishart distribution (which has a Bayesian interpretation). Chaudhuri, Sarwate, and Sinha [CSS12], Kapralov and Talwar [KT13], Wei, Sarwate, Coran- der, Hero, and Tarokh [WSCHT16], and Amin, Dick, Kulesza, Medina, and Vassilvitskii [AD- KMV18] apply variants of the exponential mechanism [MT07] to privately select a low-rank approximation to the covariance matrix A. This method is nontrivial to implement and analyse, but it ultimately requires the sample complexity to grow polynomially in the ambient dimension. Gonem and Gilad-Bachrach [GGB18] exploit smooth sensitivity [NRS07] to release a low- rank approximation to the matrix A. This allows adding less noise than worst case sensitivity, under an eigenvalue gap assumption. However, the sample complexity n is polynomial in the dimension d. Limitations of Prior Work Given the great variety of techniques and analyses that have been applied to differentially private matrix analysis problems, what is missing? We see that almost all of these techniques are ultimately based on some form of noise addition or the exponential mechanism. With the singular exception of the techniques of Sheffet [She19], all of these prior techniques satisfy pure1 or concentrated differential privacy [BS16]. This is enough to conclude that these techniques cannot yield the dimension-independent guarantees that we seek. No amount of postprocessing or careful analysis can avoid this limitation. This is because pure and concentrated differential privacy have strong group privacy properties, which means  packing  lower bounds [HT10] apply. 1Pure differential privacy (a.k.a. pointwise differential privacy) is ( , )-differential privacy with   = 0. 5 We brie y sketch why concentrated differential privacy is incompatible with dimension- independent guarantees. Let the input be X1 = X2 =   = Xn =  /   d for a uniformly random    { 1,+1}d. That is, the input is one random point repeated n times. If M satis es O(1)- concentrated differential privacy, then it satis es the mutual information bound I(M(X);X)   O(n2) [BS16]. But, if M provides a meaningful approximation to X or A = XXT , then we must be able to recover an approximation to   from its output, whence I(M(X);X)  (d), as the entropy of X is d bits. This gives a lower bound of n  (   d), even though X and A have rank k = 1. The above example shows that, even under the strongest assumptions (i.e., the data lies exactly in a rank-1 subspace), any good approximation to the subspace, to the data matrix X, or to the covariance matrix A = XXT must require the sample complexity n to grow polynomially in the ambient dimension d if we restrict to techniques that satisfy concentrated differential privacy. Almost all of the prior work in this general area is subject to this restriction. To avoid a sample complexity n that grows polynomially with the ambient dimension d, we need fundamentally new techniques. 1.4 Our Techniques For the exact case, we construct a score function for subspaces that has low sensitivity, assigns high score to the correct subspace, and assigns a low score to all other subspaces. Then we can simply apply a GAP-MAX algorithm to privately select the correct subspace [BDRS18]. The GAP-MAX algorithm satis es ( , )-differential privacy and outputs the correct subspace as long as the gap between its score and that of any other subspace is larger than O(log(1/ )/ ). This works even though there are in nitely many subspaces to consider, which would not be possible under concentrated differential privacy. The simplest score function would simply be the number of input points that the subspace contains. This assigns high score to the correct subspace, but it also assigns high score to any larger subspace that contains the correct subspace. To remedy this, we subtract from the score the number of points contained in a strictly smaller subspace. That is, the score of subspace S is the number of points in S minus the maximum over all subspaces S   S of the number of points contained in S . This GAP-MAX approach easily solves the exact case, but it does not readily extend to the approximate case. If we count points near to the subspace, rather than in it, then (in nitely) many subspaces will have high score, which violates the assumptions needed for GAP-MAX to work. Thus we use a completely different approach for the approximate case. We apply the  subsample and aggregate  paradigm of [NRS07]. That is, we split the dataset X1,  ,Xn into n/O(k) sub-datasets each of size O(k). We use each sub-dataset to compute an approximation to the subspace by doing a (non-private) PCA on the sub-dataset. Let   be the projection matrix onto the correct subspace and  1,  , n/O(k) the projection matrices onto the approximations derived from the sub-datasets. With high probability  j  is small for most j. (Exactly how small depends on the eigengap.) Now we must privately aggregate the projection matrices  1,  , n/O(k) into a single projection matrix. Rather than directly trying to aggregate the projection matrices, we pick a set of reference points, project them onto the subspaces, and then aggregate the projected points. We draw 6 p1,  ,pO(k) independently from a standard spherical Gaussian. Then  jpi  pi j   O(   k) is also small for all i and most j. We wish to privately approximate  pi and to do this we have n/O(k) points  jpi most of which are close to  pi. This is now a location or mean estimation problem, which we can solve privately. Thus we obtain points  pi such that  pi  pi  is small for all i. From a PCA of these points we can obtain a projection   with    being small, as required. Finally, we discuss how to privately obtain (  p1,  p2,  ,  pO(k)) from ( 1p1,  , 1pO(k)),  , ( n/O(k)p1,  , n/O(k)pO(k)). It is better here to treat (  p1,  p2,  ,  pO(k)) as a single vector in RO(kd), rather than as O(k) vectors in Rd. We split RO(kd) into cells and then run a differentially private histogram algorithm. If we construct the cells carefully, for most j we have that ( jp1,  , jpO(k)) is in the same histogram cell as the desired point ( p1,  , pO(k)). The histogram algorithm will thus identify this cell, and we take an arbitrary point from this cell as our estimate (  p1,  p2,  ,  pO(k)). The differentially private histogram algorithm is run over expo- nentially many cells, which is possible under ( , )-differential privacy if n/O(k)  O(log(1/ )/ ). (Note that under concentrated differential privacy the histogram algorithm s sample complexity n would need to depend on the number of cells and, hence, the ambient dimension d.) The main technical ingredients in the analysis of our algorithm for the approximate case are matrix perturbation and concentration analysis and the location estimation procedure using differentially private histograms. Our matrix perturbation analysis uses a variant of the Davis-Kahan theorem to show that if the empirical covariance matrix is close to the true covariance matrix, then the subspaces corresponding to the top k eigenvalues of each are also close; this is applied to both the subsamples and the projection of the reference points. The matrix concentration results that we use show that the empirical covariance matrices in all the subsamples are close to the true covariance matrix. This is the only place where the multivariate Gaussian assumption arises. Any distribution that concentrates well will work. 2 Notations, De nitions, and Background Results 2.1 Linear Algebra and Probability Preliminaries Here, we mention a few key technical results that we will be using to prove the main theorem for the approximate case. Throughout this document, we assume that the dimension d is larger than some absolute constant, and adopt the following notation: for a matrix A of rank r, we use s1(A)    sr(A) to denote the singular values of A in decreasing order, and use  1(A)    r(A) to denote the eigenvalues of A in decreasing order; let smin(A) denote the least, non-zero singular value of A. We omit the parentheses when the context is clear. We begin by stating two results about matrix perturbation theory. The  rst result says that if two matrices are close to one another in operator norm, then their corresponding singular values are also close to one another. De ne  M := sup{ Mx 2 : x  Rd,  x 2  1} to be the operator norm with respect to the Euclidean vector norm. 7 Lemma 2.1 (Singular Value Inequality). Let A,B  Rd n and let r = min{d,n}. Then for 1  i,j  r, si+j 1(A + B)  si(A) + sj(B). The following result gives a lower bound on the least singular value of sum of two matrices. Lemma 2.2 (Least Singular Value of Matrix Sum). Let A,B  Rd n. Then smin(A + B)  smin(A)  B . The next result bounds the angle between the subspaces spanned by two matrices that are close to one another. Let X  Rd n have the following SVD. X = h U U  i   \"  1 0 0  2 #   \" V T V T   # In the above, U,U are orthonormal matrices such that U  Rd r and U Rd (d r),  1, 2 are diagonal matrices, such that  1  Rr r and  2  R(d r) (n r), and V ,V are orthonormal matrices, such that V  Rn r and V Rn (n r). Let Z  Rd n be a perturbation matrix, and  X = X + Z, such that  X has the following SVD.  X = h  U  U  i   \"  1 0 0  2 #   \"  V T  V T   # In the above,  U,  U ,  1,  2,  V ,  V have the same structures as U,U , 1, 2,V ,V respectively. Let Z21 = U UT  ZV V T and Z12 = UUT ZV V T  . Suppose  1    r  0 are the singular values of UT  U. Let  (U,  U)  Rr r be a diagonal matrix, such that  ii(U,  U) = cos 1( i). Lemma 2.3 (Sin( ) Theorem [CZ16]). Let X,  X,Z,Z12,Z21 be de ned as above. Denote   = smin(UT  XV ) and   =  UT  XV . If  2 >  2 + min{ Z12 2, Z21 2}, then we have the following.  Sin( )(U,  U)   Z21 +  Z12   2  2  min{ Z12 2, Z21 2} The next result bounds  Sin( )(U,  U) in terms of the distance between UUT and  U  UT . Lemma 2.4 (Property of  Sin( ) [CZ16]). Let U,  U  Rd r be orthonormal matrices, and let  (U,  U) be de ned as above in terms of  U,U. Then we have the following.  Sin( )(U,  U) U  UT  UUT  2 Sin( )(U,  U)  The next result bounds the singular values of a matrix, whose columns are independent vectors from a mean zero, isotropic distribution in Rd. We  rst de ne the sub-Gaussian norm of a random variable. De nition 2.5. Let X be a sub-Gaussian random variable. The sub-Gaussian norm of X, denoted by  X 2, is de ned as,  X 2 = inf{t > 0 : E h exp(X2/t2) i  2}. 8 Lemma 2.6 (Theorem 4.6.1 [Ver18]). Let A be an n   m matrix, whose columns Ai are independent, mean zero, sub-Gaussian isotropic random vectors in Rn. Then for any t  0, we have   m  CK2(   n + t)  sn(A)  s1(A)     m + CK2(   n + t) with probability at least 1  2exp( t2). Here, K = maxi  A 2 (sub-Gaussian norm of A). In the above,  A 2  O(1) if the distribution in question is N ( 0,I). The following corollary generalises the above result for arbitrary Gaussians. Corollary 2.7. Let A be an n   m matrix, whose columns Ai are independent, random vectors in Rn from N ( 0, ). Then for any t  0, we have (   m  CK2(   n + t)) p sn( )  sn(A)  (   m + CK2(   n + t)) p sn( ) and s1(A)  (   m + CK2(   n + t)) p s1( ) with probability at least 1  2exp( t2). Here, K = maxi  A 2 (sub-Gaussian norm of A). Proof. First, we prove the lower bound on sn(A). Note that sn(A) = min  x >0  Ax   x , and that the columns of  1 2 A are distributed as N ( 0,I). Therefore, we have the following. min  x >0  Ax   x = min  x >0   1 2  1 2 Ax   x  = min  x >0   1 2  1 2 Ax   1 2 Ax   1 2 Ax   x   min  x >0   1 2  1 2 Ax   1 2 Ax  min  x >0  1 2 Ax   x   min  y >0   1 2 y   y  min  x >0  1 2 Ax   x   (   m  CK2(   n + t)) p sn( ) (Lemma 2.6) Next, we prove the upper bound on sn(A). For this, we  rst show that for X  Rm d and Y  Rd n, smin(XY )  smin(X)    Y  . smin(XY ) = min  z =1 XY z   min  z =1 X Y z  =  X  min  z =1 Y z  =  X  smin(Y ) Now, smin(XY ) = smin(Y T XT )  Y  smin(X) by the above reasoning. Using this results, we have the following. sn(A) = sn( 1/2    1/2A) 9  sn( 1/2) 1/2A   (   m + CK2(   n + t)) p sn( ) (Lemma 2.6) Now, we show the upper bound on s1(A). Note that s1(A) =  A .  A =   1 2  1 2 A    1 2    1 2 A   (   m + CK2(   n + t)) p s1( ) (Lemma 2.6) This completes the proof. Now, we state a concentration inequality for  2 random variables. Lemma 2.8. Let X be a  2 random variable with k degrees of freedom. Then, P h X > k + 2   kt + 2t i  e t. Next, we state the well-known Bernstein s inequality for sums of independent Bernoulli random variables. Lemma 2.9 (Bernstein s Inequality). Let X1,...,Xm be independent Bernoulli random variables taking values in {0,1}. Let p = E[Xi]. Then for m  5p 2 2 ln(2/ ) and    p/4, P \u0014 1 m X Xi  p   \u0015  2e 2m/2(p+ )  . We  nally state a result about the norm of a vector sampled from N ( 0,I). Lemma 2.10. Let X1,...,Xq  N ( 0, ) be vectors in Rd, where   is the projection of Id d on to a subspace of Rd of rank k. Then P h  i, Xi 2  k + 2   kt + 2t i  1  qe t. Proof. Since   is of rank k, we can directly use Lemma 2.8 for a  xed i  [q], and the union bound over all i  [q] to get the required result. This is because for any i,  Xi 2 is a  2 random variable with k degrees of freedom. 2.2 Privacy Preliminaries De nition 2.11 (Differential Privacy (DP) [DMNS06]). A randomized algorithm M : X n  Y satis es ( , )-differential privacy (( , )-DP) if for every pair of neighboring datasets X,X   X n (i.e., datasets that differ in exactly one entry),  Y  Y P[M(X)  Y ]  e    P\u0002M(X )  Y \u0003 +  . When   = 0, we say that M satis es  -differential privacy or pure differential privacy. 10 Neighbouring datasets are those that differ by the replacement of one individual s data. In our setting, each individual s data is assumed to correspond to one point in X = Rd, so neighbouring means one point is changed arbitrarily. Throughout the document, we will assume that   is smaller than some absolute constant less than 1 for notational convenience, but note that our results still hold for general  . Now, this privacy de nition is closed under post-processing. Lemma 2.12 (Post Processing [DMNS06]). If M : X n  Y is ( , )-DP, and P : Y  Z is any randomized function, then the algorithm P  M is ( , )-DP. 2.3 Basic Differentially Private Mechanisms. We  rst state standard results on achieving privacy via noise addition proportional to sensitivity [DMNS06]. De nition 2.13 (Sensitivity). Let f : X n  Rd be a function, its  1-sensitivity and  2-sensitivity are  f ,1 = max X X X n  f (X)  f (X ) 1 and  f ,2 = max X X X n  f (X)  f (X ) 2, respectively. Here, X  X  denotes that X and X  are neighboring datasets (i.e., those that differ in exactly one entry). One way of introducing ( , )-differential privacy is via adding noise sampled from the truncated Laplace distribution, proportional to the  1 sensitivity. Lemma 2.14 (Truncated Laplace Mechanism [GDGK20]). De ne the probability density function (p) of the truncated Laplace distribution as follows. p(x) =   Be |x|   if x  [ A,A] 0 otherwise In the above,   =     , A =     log 1 + e   1 2  ! , B = 1 2 (1  e A   ) . Let TLap( , , ) denote a draw from the above distribution. Let f : X n  Rd be a function with sensitivity  . Then the truncated Laplace mechanism M(X) = f (X) + TLap( , , ) satis es ( , )-DP. In the above A    f ,1   log(1/ ) since   is smaller than some absolute constant less than 1. Now, we introduce differentially private histograms. Lemma 2.15 (Private Histograms). Let n  N,  , ,  > 0, and X a set. There exists M : X n  RX which is ( , )-differentially private and, for all x  X n, we have P M    sup y X M(x)y  1 n|{i  [n] : xi = y}|  O log(1/ )  n !   1  . 11 The above holds due to [BNS16; Vad17]. Finally, we introduce the GAP-MAX algorithm from [BDRS18] that outputs the element from the output space that has the highest score function, given that there is a signi cant gap between the scores of the highest and the second to the highest elements. Lemma 2.16 (GAP-MAX Algorithm [BDRS18]). Let SCORE : X n   Y  R be a score function with sensitivity 1 in its  rst argument, and let  ,  > 0. Then there exists a ( , )-differentially private algorithm M : X n  Y and   =  (log(1/ )/ n) with the following property. Fix an input X  X n. Let y = argmax y Y {SCORE(X,y)}. Suppose  y  Y,y , y = SCORE(X,y) < SCORE(X,y )  n. Then M outputs y with probability 1. 3 Exact case Here, we discuss the case, where all n points lie exactly in a subspace s of dimension k of Rd. Our goal is to privately output that subspace. We do it under the assumption that all strict subspaces of s contain at most  points. If the points are in general position, then  = k 1, as any strictly smaller subspace has dimension < k and cannot contain more points than its dimension. Let Sk d be the set of all k-dimensional subspaces of Rd. Let Sd be the set of all subspaces of Rd. We formally de ne that problem as follows. Problem 3.1. Assume (i) all but at most  , input points are in some s Sk d, and (ii) every subspace of dimension < k contains at most  points. (If the points are in general position   aside from being contained in s  then  = k  1.) The goal is to output a representation of s . We call these  points that do not lie in s ,  adversarial points . With the problem de ned in Problem 3.1, we will state the main theorem of this section. Theorem 3.2. For any  ,  > 0,  k  1  0, and n  O  + log(1/ )   ! , there exists an ( , )-DP algorithm M : Rd n  Sk d, such that if X is a dataset of n points satisfying the conditions in Problem 3.1, then M(X) outputs a representation of s with probability 1. We prove Theorem 3.2 by proving the privacy and the accuracy guarantees of Algorithm 1. The algorithm performs a GAP-MAX (cf. Lemma 2.16). It assigns a score to all the relevant subspaces, that is, the subspaces spanned by the points of the dataset X. We show that the only subspace that has a high score is the true subspace s , and the rest of the subspaces have low scores. Then GAP-MAX outputs the true subspace successfully because of the gap between the scores of the best subspace and the second to the best one. For GAP-MAX to work all the time, 12 we de ne a default option in the output space that has a high score, which we call NULL. Thus, the output space is now Y = Sd  {NULL}. Also, for GAP-MAX to run in  nite time, we  lter Sd to select  nite number of subspaces that have at least 0 scores on the basis of X. Note that this is a preprocessing step, and does not violate privacy as, we will show, all other subspaces already have 0 probability of getting output. We de ne the score function u : X n   Y  N as follows. u(x,s) :=   |x  s|  sup{|x  t| : t  Sd,t  s} if s  Sd  + 4log(1/ )   + 1 if s = NULL Note that this score function can be computed in  nite time because for any m points and i > 0, if the points are contained in an i-dimensional subspace, then the subspace that contains all m points must lie within the set of subspaces spanned by \u0000 m i+1 \u0001 subsets of points. Algorithm 1: DP Exact Subspace Estimator DPESE , ,k, (X) Input: Samples X  Rd n. Parameters  , ,k, > 0. Output:  s  Sk d. Set Y  {NULL} and sample noise  (NULL) from TLap(2, , ). Set score u(X,NULL) =  + 4log(1/ )   + 1. // Identify candidate outputs. For each subset S of X of size k Let s be the subspace spanned by S. Y  Y  {s}. Sample noise  (s) from TLap(2, , ). Set score u(X,s) = |x  s|  sup{|x  t| : t  Sd,t  s}. // Apply GAP-MAX. Let s1 = argmaxs Y u(X,s) be the candidate with the largest score. Let s2 = argmaxs Y\\{s1} u(X,s) be the candidate with the second-largest score. Let  s = argmaxs Y max{0,u(X,s)  u(X,s2)  1} +  (s). // Truncated Laplace noise    TLap(2, , ); see Lemma 2.14 Return  s. We split the proof of Theorem 1.1 into sections for privacy (Lemma 3.3) and accuracy (Lemma 3.5). 3.1 Privacy Lemma 3.3. Algorithm 1 is ( , )-differentially private. The proof of Lemma 3.3 closely follows the privacy analysis of GAP-MAX by [BDRS18]. The only novelty is that Algorithm 1 may output NULL in the case that the input is malformed (i.e., doesn t satisfy the assumptions of Problem 3.1). The key is that the score u(X,s) is low sensitivity. Thus max{0,u(X,s)  u(X,s2)  1} also has low sensitivity. What we gain from subtracting the second-largest score and taking this 13 maximum is that these values are also sparse   only one (s = s1) is nonzero. This means we can add noise to all the values without paying for composition. We now prove Lemma 3.3. Proof. First, we argue that the sensitivity of u is 1. The quantity |X  s| has sensitivity 1 and so does sup{|X  t| : t  Sd,t  s}. This implies sensitivity 2 by the triangle inequality. However, we see that it is not possible to change one point that simultaneously increases |X  s| and decreases sup{|X  t| : t  Sd,t  s} or vice versa. Thus the sensitivity is actually 1. We also argue that u(X,s2) has sensitivity 1, where s2 is the candidate with the second-largest score. Observe that the second-largest score is a monotone function of the collection of all scores   i.e., increasing scores cannot decrease the second-largest score and vice versa. Changing one input point can at most increase all the scores by 1, which would only increase the second-largest score by 1. This implies that max{0,u(X,s)  u(X,s2)  1} has sensitivity 2 by the triangle inequality and the fact that the maximum does not increase the sensitivity. Now we observe that for any input X there is at most one s such that max{0,u(X,s) u(X,s2)  1} , 0, namely s = s1. We can say something even stronger: Let X and X  be neighbouring datasets with s1 and s2 the largest and second-largest scores on X and s  1 and s  2 the largest and second-largest scores on X . Then there is at most one s such that max{0,u(X,s) u(X,s2) 1} , 0 or max{0,u(X ,s)  u(X ,s  2)  1} , 0. In other words, we cannot have both u(X,s1)  u(X,s2) > 1 and u(X ,s  1)  u(X ,s  2) > 1 unless s1 = s  1. This holds because u(X,s)  u(X,s2) has sensitivity 2. With these observations in hand, we can delve into the privacy analysis. Let X and X  be neighbouring datasets with s1 and s2 the largest and second-largest scores on X and s  1 and s  2 the largest and second-largest scores on X . Let Y be the set of candidates from X and let Y  be the set of candidates from X . Let  Y = Y  Y  and  Y = Y  Y . We note that, for s  Y, if u(X,s)  , then there is no way that  s = s. This is because | (s)|  2log(1/ )   for all s and hence, there is no way we could have argmaxs Y max{0,u(X,s)   u(X,s2)  1} +  (s)  argmaxs Y max{0,u(X,NULL)  u(X,s2)  1} +  (NULL). If s  Y \\  Y, then u(X,s)  |X  s|  k + 1  and u(X ,s)  . This is because s <  Y implies |X  s| < k or |X   s| < k, but |X  s|  |X   s| + 1. Thus, there is no way these points are output and, hence, we can ignore these points in the privacy analysis. (This is the reason for adding the NULL candidate.) Now we argue that the entire collection of noisy values max{0,u(X,s)  u(X,s2)  1} +  (s) for s  Y is differentially private. This is because we are adding noise to a vector where (i) on the neighbouring datasets only 1 coordinate is potentially different and (ii) this coordinate has sensitivity 2. 3.2 Accuracy We start by showing that the true subspace s has a high score, while the rest of the subspaces have low scores. Lemma 3.4. Under the assumptions of Problem 3.1, u(x,s )  n  2 and u(x,s )  2 for s  , s . Proof. We have u(x,s ) = |x  s |  |x  s | for some s   Sd with s   s . The dimension of s  is at most k  1 and, by the assumption (ii), |x  s |  . 14 Let s   Sd \\ {s }. There are three cases to analyse: 1. Let s   s . Then u(x,s )  |x  s |  |x  s |  because the  adverserial points and the  n  non-adversarial points may not together lie in a subspace of dimension k. 2. Let s   s . Let k  be the dimension of s . Clearly k  < k. By our assumption (ii), |s   x|  . Then u(x,s ) = |x s | |x t|  for some t because the  adversarial points already don t lie in s , so they will not lie in any subspace of s . 3. Let s  be incomparable to s . Let s  = s   s . Then u(x,s )  |x  s |  |x  s |  because the adversarial points may not lie in s , but could be in s  \\ s . This completes the proof. Now, we show that the algorithm is accurate. Lemma 3.5. If n  3 + 8log(1/ )   + 2, then Algorithm 1 outputs s for Problem 3.1. Proof. From Lemma 3.4, we know that s has a score of at least n 2 , and the next best subspace can have a score of at most  . Also, the score of NULL is de ned to be  + 4log(1/ )   + 1. This means that the gap satis es max{0,u(X,s ) u(X,s2) 1}  n 3 4log(1/ )    1. Since the noise is bounded by 2log(1/ )   , our bound on n implies that  s = s  3.3 Lower Bound Here, we show that our upper bound is optimal up to constants for the exact case. Theorem 3.6. Any ( , )-DP algorithm that takes a dataset of n points satisfying the conditions in Problem 3.1 and outputs s with probability > 0.5 requires n   \u0010  + log(1/ )   \u0011 . Proof. First, n  +k. This is because we need at least k points to span the subspace, and  points could be corrupted. Second, n  (log(1/ )/ ) by group privacy. Otherwise, the algorithm is (10,0.1)-differentially private with respect to changing the entire dataset and it is clearly impossible to output the subspace under this condition. 4 Approximate Case In this section, we discuss the case, where the data  approximately  lies in a k-dimensional subspace of Rd. We make a Gaussian distributional assumption, where the covariance is approximately k-dimensional, though the results could be extended to distributions with heavier tails using the right inequalities. We formally de ne the problem: Problem 4.1. Let    Rd d be a symmetric, PSD matrix of rank  k  {1,...,d}, and let 0 <    1, such that  k+1  k  2. Suppose   is the projection matrix corresponding to the subspace spanned by the eigenvectors of   corresponding to the eigenvalues  1,..., k. Given sample access to N ( 0, ), and 0 <   < 1, output a projection matrix b , such that    b . 15 We solve Problem 4.1 under the constraint of ( , )-differential privacy. Throughout this section, we would refer to the subspace spanned by the top k eigenvectors of   as the  true  or  actual  subspace. Algorithm 2 solves Problem 4.1 and proves Theorem 1.2. Here    is the operator norm. Remark 4.2. We scale the eigenvalues of   so that  k = 1 and  k+1  2. Also, for the purpose of the analysis, we will be splitting   =  k +  d k, where  k is the covariance matrix formed by the top k eigenvalues and the corresponding eigenvectors of   and  d k is remainder. Also, we assume the knowledge of   (or an upper bound on  ). Our solution is presented in Algorithm 2. The following theorem is the main result of the section. Theorem 4.3. Let    Rd d be an arbitrary, symmetric, PSD matrix of rank  k  {1,...,d}, and let 0 <   < 1. Suppose   is the projection matrix corresponding to the subspace spanned by the vectors of  k. Then given  2  O  2n d2k ln(1/ )   min (1 k , 1 ln(k ln(1/ )/ ) )! , such that  k+1( )  2 k( ), for every  ,  > 0, and 0 <   < 1, there exists and ( , )-DP algorithm that takes n  O k log(1/ )   + log(1/ )log(log(1/ )/ )   ! samples from N ( 0, ), and outputs a projection matrix b , such that    b  with probability at least 0.7. Algorithm 2 is a type of  Subsample-and-Aggregate  algorithm [NRS07]. Here, we consider multiple subspaces formed by the points from the same Gaussian, and privately  nd a subspace that is close to all those subspaces. Since the subspaces formed by the points would be close to the true subspace, the privately found subspace would be close to the true subspace. A little more formally, we  rst sample q public data points (called  reference points ) from N ( 0,I). Next, we divide the original dataset X into disjoint datasets of m samples each, and project all reference points on the subspaces spanned by every subset. Now, for every reference point, we do the following. We have t = n m projections of the reference point. Using DP histogram over Rd, we aggregate those projections in the histogram cells; with high probability all those projections will be close to one another, so they would lie within one histogram cell. We output a random point from the histogram cell corresponding to the reference point. With a total of q points output in this way, we  nally output the projection matrix spanned by these points. In the algorithm C0, C1, and C2 are universal constants. We divide the proof of Theorem 4.3 into two parts: privacy (Lemma 4.4) and accuracy (Lemma 4.9). 4.1 Privacy We analyse the privacy by understanding the sensitivities at the only sequence of steps invok- ing a differentially private mechanism, that is, the sequence of steps involving DP-histograms. 16 Algorithm 2: DP Approximate Subspace Estimator DPASE , , , ,k(X) Input: Samples X1,...,Xn  Rd. Parameters  , , , ,k > 0. Output: Projection matrix b   Rd d of rank k. Set parameters: t  C0 ln(1/ )   m  n/t  q  C1k   C2    dk(   k+  ln(kt))  m Sample reference points p1,...,pq from N ( 0,I) independently. // Subsample from X, and form projection matrices. For j  1,...,t Let Xj = (X(j 1)m+1,...,Xjm)  Rd m. Let  j  Rd d be the projection matrix onto the subspace spanned by the eigenvectors of Xj(Xj)T  Rd d corresponding to the largest k eigenvalues. For i  1,...,q pj i  jpi // Create histogram cells with random offset. Let   be a random number in [0,1). Divide Rqd into  = {...,[ + i , + (i + 1) ),...}qd, for all i  Z. Let each disjoint cell of length  be a histogram bucket. // Perform private aggregation of subspaces. For each i  [q], let Qi  Rd t be the dataset, where column j is pj i. Let Q  Rqd t be the vertical concatenation of all Qi s in order. Run ( , )-DP histogram over  using Q to get    that contains at least t 2 points. If no such   exists Return   // Return the subspace. Let bp = (bp1,...,bpd,...,bp(q 1)d+1,...,bpqd) be a random point in  . For each i  [q] Let bpi = (bp(i 1)d+1,...,bpid). Let b  be the projection matrix of the top-k subspace of (bp1,...,bpq). Return b . Lemma 4.4. Algorithm 2 is ( , )-differentially private. Proof. Changing one point in X can change only one of the Xj s. This can only change one point in Q, which in turn can only change the counts in two histogram cells by 1. Therefore, the sensitivity is 2. Because the sensitivity of the histogram step is bounded by 2 (Lemma 4.4), an application of DP-histogram, by Lemma 2.15, is ( , )-DP. Outputting a random point in the privately found histogram cell preserves privacy by post-processing (Lemma 2.12). Hence, the claim. 17 4.2 Accuracy Now we delve into the utility analysis of the algorithm. For 1  j  t, let Xj be the subsets of X as de ned in Algorithm 2, and  j be the projection matrices of their respective subspaces. We now show that  j and the projection matrix of the subspace spanned by  k are close in operator norm. Lemma 4.5. Let   be the projection matrix of the subspace spanned by the vectors of  k, and for each 1  j  t, let  j be the projection matrix as de ned in Algorithm 2. If m  O(k + ln(qt)), then P    j,   j O         d  m        0.95 Proof. We show that the subspaces spanned by Xj and the true subspace spanned by   are close. Formally, we invoke Lemmata 2.3 and 2.4. This closeness follows from standard matrix concentration inequalities. Fix a j  [t]. Note that Xj can be written as Y j + H, where Y j is the matrix of vectors distributed as N ( 0, k), and H is a matrix of vectors distributed as N ( 0, d k), where  k and  d k are de ned as in Remark 4.2. By Corollary 2.7, with probability at least 1  0.02 t , sk(Y j)    (( m +   k)( p sk( k))) =  ( m +   k) > 0. Therefore, the subspace spanned by Y j is the same as the subspace spanned by  k. So, it suf ces to look at the subspace spanned by Y j. Now, by Corollary 2.7, we know that with probability at least 1  0.02 t ,  Xj  Y j =  H  O(( m +   d) p s1( d k))  O( ( m +   d) p sk( k))  O( ( m +   d)). We wish to invoke Lemma 2.3. Let UDV T be the SVD of Y j, and let  U  D  V T be the SVD of Xj. Now, for a matrix M, let  M denote the projection matrix of the subspace spanned by the columns of M. De ne quantities a,b,z12,z21 as follows. a = smin(UT XjV ) = smin(UT Y jV + UT HV ) = smin(UT Y jV ) (Columns of U are orthogonal to columns of H) = sk(Y j)  (   m +   k)  (   m) b =  UT  XjV  =  UT  Y jV + UT  HV  =  UT  HV  (Columns of U are orthogonal to columns of Y j)  H   O( (   m +   d)) z12 =  UH V  = 0 z21 =  U H V   =  U 1/2 d k( 1/2 d k H) V   18 Now, in the above,  1/2 d k H  Rd m, such that each of its entry is an independent sample from N (0,1). Right-multiplying it by  V makes it a matrix in a k-dimensional subspace of Rm, such that each row is an independent vector from a spherical Gaussian. Using Corol- lary 2.7,  1/2 d k H O(   d +   k)  O(   d) with probability at least 1  0.01 t . Also,  U 1/2 d k  O(  p sk( k))  O( ). This gives us: z21  O(    d). Since a2 > 2b2, we get the following by Lemma 2.3.  Sin( )(U,  U)  az21 + bz12 a2  b2  min{z2 12,z2 21}  O         d  m     Therefore, using Lemma 2.4, and applying the union bound over all j, we get the required result. Let   = O \u0012     d  m \u0013 . We show that the projections of any reference point are close. Corollary 4.6. Let p1,...,pq be the reference points as de ned in Algorithm 2, and let   and  j (for 1  j  t) be projections matrices as de ned in Lemma 4.5. Then P h  i,j, (   j)pi O( (   k + p ln(qt))) i  0.9. Proof. We know from Lemma 4.5 that    j  for all j with probability at least 0.95. For j  [t], let b j be the projection matrix for the union of the jth subspace and the subspace spanned by  k. Lemma 2.10 implies that with probability at least 0.95, for all i,j,  b jpi O(   k+ p ln(qt)). Therefore,  (   j)pi =  (   j)b jpi   j   b jpi O( (   k + p ln(qt))). Hence, the claim. The above corollary shows that the projections of each reference point lie in a ball of radius O(    k). Next, we show that for each reference point, all the projections of the point lie inside a histogram cell with high probability. For notational convenience, since each point in Q is a concatenation of the projection of all reference points on a given subspace, for all i,j, we refer to (0,...,0,Qj (i 1)d+1,...,Qj id,0,...,0)  Rqd (where there are (i  1)d zeroes behind Qj (i 1)d+1, and (q  i)d zeroes after Qj id) as pj i. Lemma 4.7. Let  and   be the length of a histogram cell and the random offset respectively, as de ned in Algorithm 2. Then P[|   Q| = t]  0.8. Thus there exists    that, such that all points in Q lie within  . 19 Proof. Let r = O( (   k + p ln(qt))). This implies that  = 20r q. The random offset could also be viewed as moving along a diagonal of a cell by   p dq. We know that with probability at least 0.8, for each i, all projections of reference point pi lie in a ball of radius r. This means that all the points in Q lie in a ball of radius r q. Then P[|   Q| = t]  P \u0014 1 20      19 20 \u0015 = 1 10. Taking the union bound over all q and the failure of the event in Corollary 4.6, we get the claim. Now, we analyse the sample complexity due to the private algorithm, that is, DP-histograms. Lemma 4.8. Let   be the histogram cell as de ned in Algorithm 2. Suppose Count( ) is the noisy count of   as a result of applying the private histogram. If t  O \u0010log(1/ )   \u0011 , then P \u0014 |Count( )|  t 2 \u0015  0.75. Proof. Lemma 4.7 implies that with probability at least 0.8, for each i, all projections of pi lie in a histogram cell, that is, all points of Q lie in a histogram cell in  . Because of the error bound in Lemma 2.15 and our bound on t, we see at least t 2 points in that cell with probability at least 1  0.05. Therefore, by taking the union bound, the proof is complete. We  nally show that the error of the projection matrix that is output by Algorithm 2 is small. Lemma 4.9. Let b  be the projection matrix as de ned in Algorithm 2, and n be the total number of samples. If  2  O  2n d2k ln(1/ )   min (1 k , 1 ln(k ln(1/ )/ ) )! , n  O(k log(1/ )   + ln(1/ )ln(ln(1/ )/ )   ), and q  O(k) the with probability at least 0.7,  b   . Proof. For each i  [q], let p  i be the projection of pi on to the subspace spanned by  k, bpi be as de ned in the algorithm, and pj i be the projection of pi on to the subspace spanned by the jth subset of X. From Lemma 4.8, we know that all pj i  s are contained in a histogram cell of length  . This implies that p  i is also contained within the same histogram cell. Now, let P = (p  1,...,p  q) and bP = (bp1,...,bpq). Then by above, bP = P + E, where  E F  2  p dq. Therefore,  E 2  p dq. Let E = EP + EP , where EP is the component of E in the subspace spanned by P , and EP be the orthogonal component. Let P   = P + EP . We will be analysing bP with respect to P  . Now, with probability at least 0.95, sk(P )  (   k) due to our choice of q and using Corol- lary 2.7, and sk+1(P ) = 0. So, sk+1(P  ) = 0 because EP is in the same subspace as P . Now, using Lemma 2.2, we know that sk(P  )  sk(P )  EP  (   k) > 0. This means that P   has rank k, so the subspaces spanned by  k and P   are the same. As before, we will try to bound the distance between the subspaces spanned by P   and bP . Note that using Lemma 2.1, we know that sk(P  )  sk(P ) +  EP  O(   k). 20 We wish to invoke Lemma 2.3 again. Let UDV T be the SVD of P  , and let  U  D  V T be the SVD of bP . Now, for a matrix M, let  M denote the projection matrix of the subspace spanned by the columns of M. De ne quantities a,b,z12,z21 as follows. a = smin(UT bP V ) = smin(UT P  V + UT EP V ) = smin(UT P  V ) (Columns of U are orthogonal to columns of EP ) = sk(P  )  (   k) b =  UT  bP V  =  UT  P  V + UT  EP V  =  UT  EP V  (Columns of U are orthogonal to columns of P  )  EP    O(  p dq) z12 =  UEP  V  = 0 z21 =  U EP  V    EP    O(  p dq) Using Lemma 2.3, we get the following.  Sin( )(U,  U)  az21 + bz12 a2  b2  min{z2 12,z2 21}  O \u0010     dk \u0011   This completes our proof. 4.3 Boosting In this subsection, we discuss boosting of error guarantees of Algorithm 2. The approach we use is very similar to the well-known Median-of-Means method: we run the algorithm multiple times, and choose an output that is close to all other  good  outputs. We formalise this in Algorithm 3. Now, we present the main result of this subsection. Theorem 4.10. Let    Rd d be an arbitrary, symmetric, PSD matrix of rank  k  {1,...,d}, and let 0 <   < 1. Suppose   is the projection matrix corresponding to the subspace spanned by the vectors of  k. Then given  2  O  2n d2k ln(1/ )   min (1 k , 1 ln(k ln(1/ )/ ) )! , 21 Algorithm 3: DP Approximate Subspace Estimator Boosted DPASEB , , , , ,k(X) Input: Samples X1,...,Xn  Rd. Parameters  , , , , ,k > 0. Output: Projection matrix b   Rd d of rank k. Set parameters: t  C3 log(1/ ) m  n/t  Split X into t datasets of size m: X1,...,Xt. // Run DPASE t times to get multiple projection matrices. For i  1,...,t b i  DPASE , , , ,k(Xi) // Select a good subspace. For i  1,...,t ci  0 For j  [t] \\ {i} If  b i  b j 2  ci  ci + 1 If ci  0.6t  1 Return b i. // If there were not enough good subspaces, return  . Return  . such that  k+1( )  2 k( ), for every  ,  > 0, and 0 <  ,  < 1, there exists and ( , )-DP algorithm that takes n  O k log(1/ )log(1/ )   + log(1/ )log(log(1/ )/ )log(1/ )   ! samples from N ( 0, ), and outputs a projection matrix b , such that    b  with probability at least 1  . Proof. Privacy holds trivially by Theorem 4.3. We know by Theorem 4.3 that for each i, with probability at least 0.7,  b i  . This means that by Lemma 2.9, with probability at least 1  , at least 0.6t of all the computed projection matrices are accurate. This means that there has to be at least one projection matrix that is close to 0.6t  1 > 0.5t of these accurate projection matrices. So, the algorithm cannot return  . Now, we want to argue that the returned projection matrix is accurate, too. Any projection matrix that is close to at least 0.6t  1 projection matrices must be close to at least one accurate projection matrix (by pigeonhole principle). Therefore, by triangle inequality, it will be close to the true subspace. Therefore, the returned projection matrix is also accurate. 22 References [ABU18] R. Arora, V. Braverman, and J. Upadhyay.  Differentially private robust low- rank approximation . In: Advances in neural information processing systems (2018). [ACGMMTZ16] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang.  Deep learning with differential privacy . In: Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016, pp. 308 318. [ADKMV18] K. Amin, T. Dick, A. Kulesza, A. M. Medina, and S. Vassilvitskii.  Private covariance estimation via iterative eigenvector sampling . In: 2018 NIPS workshop in Privacy-Preserving Machine Learning. Vol. 250. 2018. [BBDS12] J. Blocki, A. Blum, A. Datta, and O. Sheffet.  The Johnson-Lindenstrauss Trans- form Itself Preserves Differential Privacy . In: Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science. FOCS  12. Washington, DC, USA: IEEE Computer Society, 2012, pp. 410 419. [BBNS19] J. B asiok, M. Bun, A. Nikolov, and T. Steinke.  Towards instance-optimal private query release . In: Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM. 2019, pp. 2480 2497. [BCMNUW20] R. Bassily, A. Cheu, S. Moran, A. Nikolov, J. Ullman, and S. Wu.  Private query release assisted by public data . In: International Conference on Machine Learning. PMLR. 2020, pp. 695 703. [BDMN05] A. Blum, C. Dwork, F. McSherry, and K. Nissim.  Practical Privacy: The SuLQ Framework . In: Proceedings of the 24th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems. PODS  05. New York, NY, USA: ACM, 2005, pp. 128 138. [BDRS18] M. Bun, C. Dwork, G. N. Rothblum, and T. Steinke.  Composable and Versatile Privacy via Truncated CDP . In: Proceedings of the 50th Annual ACM Symposium on the Theory of Computing. STOC  18. New York, NY, USA: ACM, 2018, pp. 74  86. [BLR08] A. Blum, K. Ligett, and A. Roth.  A Learning Theory Approach to Non- Interactive Database Privacy . In: STOC. 2008. [BNS16] M. Bun, K. Nissim, and U. Stemmer.  Simultaneous Private Learning of Multi- ple Concepts . In: Proceedings of the 7th Conference on Innovations in Theoretical Computer Science. ITCS  16. New York, NY, USA: ACM, 2016, pp. 369 380. [BS16] M. Bun and T. Steinke.  Concentrated Differential Privacy: Simpli cations, Extensions, and Lower Bounds . In: Proceedings of the 14th Conference on Theory of Cryptography. TCC  16-B. Berlin, Heidelberg: Springer, 2016, pp. 635 658. 23 [BUV14] M. Bun, J. Ullman, and S. Vadhan.  Fingerprinting Codes and the Price of Approximate Differential Privacy . In: Proceedings of the 46th Annual ACM Symposium on the Theory of Computing. STOC  14. New York, NY, USA: ACM, 2014, pp. 1 10. [CSS12] K. Chaudhuri, A. Sarwate, and K. Sinha.  Near-optimal differentially private principal components . In: Advances in Neural Information Processing Systems 25 (2012), pp. 989 997. [CZ16] T. Cai and A. Zhang.  Rate-Optimal Perturbation Bounds for Singular Sub- spaces with Applications to High-Dimensional Statistics . In: The Annals of Statistics 46 (May 2016). [DMNS06] C. Dwork, F. McSherry, K. Nissim, and A. Smith.  Calibrating Noise to Sensi- tivity in Private Data Analysis . In: Proceedings of the 3rd Conference on Theory of Cryptography. TCC  06. Berlin, Heidelberg: Springer, 2006, pp. 265 284. [DSSUV15] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan.  Robust Traceability from Trace Amounts . In: Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science. FOCS  15. Washington, DC, USA: IEEE Computer Society, 2015, pp. 650 669. [DTTZ14] C. Dwork, K. Talwar, A. Thakurta, and L. Zhang.  Analyze Gauss: Optimal Bounds for Privacy-Preserving Principal Component Analysis . In: Proceed- ings of the 46th Annual ACM Symposium on the Theory of Computing. STOC  14. New York, NY, USA: ACM, 2014, pp. 11 20. [FT20] Y. Feng and Y. Tu.  How neural networks  nd generalizable solutions: Self- tuned annealing in deep learning . In: arXiv preprint arXiv:2001.01678 (2020). [GARD18] G. Gur-Ari, D. A. Roberts, and E. Dyer.  Gradient descent happens in a tiny subspace . In: arXiv preprint arXiv:1812.04754 (2018). [GDGK20] Q. Geng, W. Ding, R. Guo, and S. Kumar.  Tight Analysis of Privacy and Utility Tradeoff in Approximate Differential Privacy . In: Proceedings of the Twenty Third International Conference on Arti cial Intelligence and Statistics. Ed. by S. Chiappa and R. Calandra. Vol. 108. Proceedings of Machine Learning Research. PMLR, 2020, pp. 89 99. [GGB18] A. Gonem and R. Gilad-Bachrach.  Smooth Sensitivity Based Approach for Differentially Private PCA . In: Algorithmic Learning Theory. ALT  18. JMLR, Inc., 2018, pp. 438 450. [HP13] M. Hardt and E. Price.  The noisy power method: A meta algorithm with applications . In: arXiv preprint arXiv:1311.2495 (2013). [HR10] M. Hardt and G. N. Rothblum.  A multiplicative weights mechanism for privacy-preserving data analysis . In: 2010 IEEE 51st Annual Symposium on Foundations of Computer Science. IEEE. 2010, pp. 61 70. 24 [HR12] M. Hardt and A. Roth.  Beating randomized response on incoherent matrices . In: Proceedings of the forty-fourth annual ACM symposium on Theory of computing. 2012, pp. 1255 1268. [HR13] M. Hardt and A. Roth.  Beyond worst-case analysis in private singular vector computation . In: Proceedings of the forty- fth annual ACM symposium on Theory of computing. 2013, pp. 331 340. [HT10] M. Hardt and K. Talwar.  On the Geometry of Differential Privacy . In: Proceedings of the 42nd Annual ACM Symposium on the Theory of Computing. STOC  10. New York, NY, USA: ACM, 2010, pp. 705 714. [KRRT20] P. Kairouz, M. Ribero, K. Rush, and A. Thakurta. Fast Dimension Independent Private AdaGrad on Publicly Estimated Subspaces. 2020. [KT13] M. Kapralov and K. Talwar.  On Differentially Private Low Rank Approxi- mation . In: Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms. SODA  13. Philadelphia, PA, USA: SIAM, 2013, pp. 1395 1414. [LFLY18] C. Li, H. Farkhoor, R. Liu, and J. Yosinski.  Measuring the intrinsic dimension of objective landscapes . In: arXiv preprint arXiv:1804.08838 (2018). [LGZCB20] X. Li, Q. Gu, Y. Zhou, T. Chen, and A. Banerjee.  Hessian based analysis of sgd for deep nets: Dynamics and generalization . In: Proceedings of the 2020 SIAM International Conference on Data Mining. SIAM. 2020, pp. 190 198. [LXTSG17] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein.  Visualizing the loss landscape of neural nets . In: arXiv preprint arXiv:1712.09913 (2017). [MM09] F. McSherry and I. Mironov.  Differentially private recommender systems: Building privacy into the net ix prize contenders . In: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009, pp. 627 636. [MT07] F. McSherry and K. Talwar.  Mechanism Design via Differential Privacy . In: Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Sci- ence. FOCS  07. Washington, DC, USA: IEEE Computer Society, 2007, pp. 94  103. [NRS07] K. Nissim, S. Raskhodnikova, and A. Smith.  Smooth Sensitivity and Sam- pling in Private Data Analysis . In: Proceedings of the 39th Annual ACM Sympo- sium on the Theory of Computing. STOC  07. New York, NY, USA: ACM, 2007, pp. 75 84. [NS06] A. Narayanan and V. Shmatikov.  How to break anonymity of the net ix prize dataset . In: arXiv preprint cs/0610105 (2006). [She19] O. Sheffet.  Old techniques in differentially private linear regression . In: Algorithmic Learning Theory. PMLR. 2019, pp. 789 827. [SU17] T. Steinke and J. Ullman.  Between Pure and Approximate Differential Pri- vacy . In: The Journal of Privacy and Con dentiality 7.2 (2017), pp. 3 22. 25 [Ull15] J. Ullman.  Private multiplicative weights beyond linear queries . In: Pro- ceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems. 2015, pp. 303 312. [Vad17] S. Vadhan.  The Complexity of Differential Privacy . In: Tutorials on the Foun- dations of Cryptography: Dedicated to Oded Goldreich. Ed. by Y. Lindell. Cham, Switzerland: Springer International Publishing AG, 2017. Chap. 7, pp. 347  450. [Ver18] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. [WSCHT16] L. Wei, A. D. Sarwate, J. Corander, A. Hero, and V. Tarokh.  Analysis of a privacy-preserving PCA algorithm using random matrix theory . In: 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE. 2016, pp. 1335 1339. [ZWB20] Y. Zhou, Z. S. Wu, and A. Banerjee.  Bypassing the ambient dimension: Private sgd with gradient subspace identi cation . In: arXiv preprint arXiv:2007.03813 (2020). 26"
    },
    {
        "title": "2106.01361",
        "year": "2021",
        "file_name": "2106.01361.pdf",
        "full_text": "Mixed-parity octupolar pairing and corner Majorana modes in three dimensions Bitan Roy1,  and Vladimir Juri ci c2, 3 1Department of Physics, Lehigh University, Bethlehem, Pennsylvania, 18015, USA 2Nordita, KTH Royal Institute of Technology and Stockholm University, Roslagstullsbacken 23, 10691 Stockholm, Sweden 3Departamento de F sica, Universidad T ecnica Federico Santa Mar a, Casilla 110, Valpara so, Chile (Dated: November 11, 2021) We identify time-reversal symmetry breaking mixed-parity superconducting states that feature eight Majorana corner modes in properly cleaved three-dimensional cubic crystals. Namely, when an odd-parity isotropic p-wave pairing coexists with cubic symmetry preserving even-parity octupo- lar dx2 y2 + id3z2 r2 pairing, the gapless surface Majorana modes of the former get localized at the eight corners, thus yielding an intrinsic third-order topological superconductor (TOTSC). A cousin dxy +id3z2 r2 pairing also accommodating eight corner Majorana modes, by virtue of break- ing the cubic symmetry, in contrast, yields an extrinsic TOTSC. We identify a doped octupolar (topological or trivial) Dirac insulator as a suitable platform to sustain such unconventional super- conductors, realized from an intraunit cell pairing. Finally, we argue that the proposed TOTSC can be experimentally realizable in NaCl and other structurally similar compounds under high pressure. Introduction. Localized Majorana zero modes are of paramount importance for braiding and non-Abelian statistics, and their applications in topological quantum computation [1 3]. For these purposes, one-dimensional quantum nanowires o er a great potential as they can host topologically robust endpoint Majorana zero modes at low temperatures: a hallmark of the traditional bulk- boundary correspondence. Nonetheless, its recently dis- covered higher-order generalization manifesting through robust gapless modes localized on even lower-dimensional boundaries, such as corners and hinges [4 16], when ex- tended to the territory of neutral Bogoliubov-de Gennes (BdG) quasiparticles, boosts in this regard the promi- nence of higher-dimensional higher-order topological su- perconductors (HOTSCs) [17 40]. For example, in con- trast to conventional (or  rst-order) topological p + ip and d + id pairings, supporting one-dimensional Majo- rana edge modes, a two-dimensional p+id HOTSC hosts four pointlike corner localized Mojorana modes [17, 33]. However, thus far the proposed three-dimensional (3D) HOTSCs only encompass Majorana hinge modes, while the mechanism and the platforms for the realizations of 3D corner Majorana modes remained elusive. In this Let- ter, we therefore venture the following set of questions, and provide de nite answers to them. (1) What is the underlying pairing symmetry of 3D HOTSCs that sup- ports corner Majorana modes? (2) What are the suitable material platforms where such pairings can be realized? Key results. Here, we identify two candidate mixed- parity time-reversal symmetry breaking octupolar pair- ings, each of which supports eight zero-energy Majo- rana corner modes in suitably cleaved cubic crystals [Figs. 1 and 2]. Speci cally, we show that when an odd-parity spin-triplet isotropic p-wave pairing (analog of the B-phase of 3He) coexists with an even-parity sin- glet dx2 y2 +id3z2 r2 pairing, the resulting mixed parity superconducting state supports eight Majorana corner modes. This pairing is a prototypical example of oc- tupolar pairing in a cubic system, transforming under the irreducible Eg representation. It breaks the time- reversal symmetry, but preserves the cubic symmetry. Thus p  (dx2 y2 + id3z2 r2) pairing stands as an intrin- sic HOTSC [41]. A cousin p  (dxy + id3z2 r2) pairing, transforming under the mixed T2g and Eg representa- tions, although supporting eight corner Majorana modes, breaks the cubic symmetry. It thus stands as an extrin- sic HOTSC. Since pointlike corner Majorana modes with dimensionality dB = 0 in three dimensions (d = 3) are characterized by the codimension dc = d  dB = 3, these two paired states represent third-order topological super- conductors (TOTSCs). They can be realized around an underlying Fermi surface with an additional two-fold sub- lattice degeneracy besides the conventional Kramers de- generacy. The corner Majorana modes are stable even in the presence of a weak s-wave pairing that gets induced naturally in the presence of dominant d-wave pairings. We identify a doped octupolar Dirac insulator (de ned later) as a suitable platform where such unconventional pairings stem from a unique fully gapped local pair- ing. While an intrinsic TOTSC possesses a quantized octupolar moment Qxyz = 0.5, for an extrinsic TOTSC Qxyz = 0. See the phase diagrams in Fig. 3. Finally, the proposed TOTSC may be experimentally realizable in NaCl and structurally similar compounds InTe, SnAs and SnSb under high pressure [42 45]. HOTSCs around Fermi surface. The e ective single particle BdG Hamiltonian in the presence of p  (d  + id3z2 r2) pairings, with   = x2  y2 and xy, around the Fermi surface (FS), possessing Kramers and two-fold sublattice degeneracy reads HFS octu = \u0012 k2 2m    \u0013  300 +  p 3 X j=1 kj kF  13j +  1 d1(k)  110 +  2 d2(k)  200 +  s 100,(1) where   =  . Three sets of Pauli matrices { }, arXiv:2106.01361v2 [cond-mat.supr-con] 10 Nov 2021 2 { }, and { } respectively act on the Nambu or particle- hole, sublattice, and spin or Kramers indices, m is the e ective mass,   is the chemical doping, and kF is the Fermi momentum. Throughout we consider m ,   > 0, such that the pairing of sharp normal state quasiparticles takes place around a Fermi surface. We are then in the weak-coupling regime. The triplet p-wave pairing with amplitude  p is odd under parity k  k, while it pre- serves the time-reversal symmetry. The two components of the cubic d-wave pairings (with explicit forms de ned below) are even under parity, i.e., d1,2( k) = d1,2(k). But the component d2(k) is odd under the reversal of time, generated by T =  002K, where K is the complex conjugation and T 2 =  1. In addition, we also include an s-wave pairing with amplitude  s, which preserves the time reversal symmetry and gets naturally induced in the presence of a d-wave pairing, as both pairing chan- nels are even under parity. The above e ective single particle Hamiltonian enjoys the particle-hole symmetry, generated by the antiunitary operator   =  202K with  2 = +1 and  HFS octu 1 =  HFS octu. In the absence of d- and s-wave pairings, HFS octu de- scribes a fully gapped isotropic odd-parity p-wave pair- ing (class DIII). As such, it supports two copies of gap- less Majorana states on all six surfaces of a cubic crystal, irrespective of its speci c cut [46]. When only the d- wave pairings are included, all the matrices appearing in HFS octu mutually anticommute. Since then HFS octu involves six mutually anticommuting matrices, their minimal di- mensionality has to be eight, which in turn demands an additional two-fold sublattice degeneracy of the Fermi surface. We now address the role of the d-wave pairings for the realization of Majorana corner modes. Intrinsic TOTSC. From  ve possible cubic d-wave pairings, one can construct only one combination with d1(k) =   3 2k2 F (k2 x  k2 y), d2(k) = 1 2k2 F (2k2 z  k2 x  k2 y) (2) that preserves the cubic symmetry, but breaks the time- reversal symmetry. The resulting dx2 y2 + id3z2 r2 state is an octupolar pairing and supports eight Majorana- Weyl nodes at  kx =  ky =  kz = kF /   3 (in the absence of other superconducting orders). Even though both d-wave components transform under the irreducible doublet Eg representation of the cubic or Oh point group, their amplitudes in Eq. (1) are set to be di erent, since these two pairings cannot be transformed into each other by an arbitrary SO(3) rotation. Nonetheless, their tran- sition temperatures are the same, as expected [47, 48]. In the presence of such octupolar pairing, the gapless surface states of the isotropic p-wave pairing get partially gapped, since all the involved matrices in Eq. (1) then mutually anticommute. In other words, the dx2 y2 + id3z2 r2 pairing acts as a mass for gapless surface Ma- jorana fermions of the p-wave superconductor. However, 0 4000 8000 -10 0 10 n En 4000 -1 0 1 (a) (b) 0.00 0.04 0.08 FIG. 1: (a) Eigenvalue spectra (En) for an intrinsic TOTSC, realized around a Fermi surface, on a cubic lattice. In- set: Eight near (due to  nite system size) zero-energy cor- ner modes (red dots), well separated from nearby bulk states (black dots). (b) Local density of states for the zero energy states in (a), displaying sharp localization around the corners in the  111 directions. These results remain qualitatively un- changed in the presence of a small s-wave component, and for the local pairing shown in Eq. (7) in an octupolar (topologi- cal or trivial) Dirac insulator (doped or undoped) [46]. The linear dimension of the system is L = 10 in each direction, and t1 = t0 =  1 =  2 = m0/2 = 1 and  s = 0 in Eq. (5). such a BdG Wilson-Dirac mass vanishes along the high- symmetry eight body-diagonal  111 directions. As a re- sult, the surface states of isotropic p-wave pairing are left gapless only at eight corners of a cubic crystal cleaved so that they are placed at ( 1,  1,  1)L/2, where L is the linear dimension of the system in each direction, see Fig. 1. The resulting p (dx2 y2+id3z2 r2) pairing there- fore stands as an intrinsic TOTSC that supports eight zero-energy Majorana corner modes. On the other hand, when  2 = 0, the xy surfaces and four hinges along the z direction host gapless Majorana modes, and we realize a second-order topological superconductor [46]. Extrinsic TOTSC. Another octupolar pairing with d1(k) =   3 k2 F (kxky), d2(k) = 1 2k2 F (2k2 z  k2 x  k2 y) (3) that also supports eight Majorana Weyl nodes at (    2, 0,  1)kF /   3 and (0,     2,  1)kF /   3 (in the ab- sence of any other pairings), partially gaps out the sur- face Majorana modes of the isotropic p-wave supercon- ductor. Such an octupolar pairing leaves eight corners gapless, which, as dictated by the dxy pairing compo- nent in Eq. (3), are pinned at the four side centers on each of the two xy planes in real space, see Fig. 2. The above two components of the d-wave pairings respectively transform under the T2g and Eg representations, thereby breaking the cubic symmetry and the corresponding two amplitudes in Eq. (1) are generically di erent. The re- sulting mixed-parity p  (dxy + id3z2 r2) pairing then stands as an extrinsic TOTSC. Once again if we switch 3 0 3400 6800 -10 0 10 n En 3400 -1 0 1 (a) (b) 0.00 0.05 0.10 FIG. 2: (a) Eigenvalue spectra (En) for an extrinsic TOTSC, realized around a Fermi surface, on a cubic lattice (dashed cube), cleaved in such a way (solid cube) that eight corners are now placed at the four side centers on each of the two xy planes. Inset: Eight near zero-energy corner modes (red dots), well separated from nearby bulk states (black dots). (b) Local density of states for the zero-energy states, displaying sharp corner localization. These results remain qualitatively unchanged in the presence of a small s-wave component, and for the local pairing shown in Eq. (7) in an octupolar Dirac material [46]. The linear dimensions of the system are Lx = 13, Ly = 13, and Lz = 10 in the x, y and z directions, respectively. The parameter values are the same as in Fig. 1. o the d3z2 r2-wave pairing, a second-order topological superconductor is realized [46]. Induced s-wave pairing. Now we address the impact of the induced s-wave component on the fully gapped TOTSC. For a small amplitude of such parasitic s-wave pairing the spectra of BdG quasiparticles remain fully gapped, and the system continues to support eight lo- calized corner Majorana modes [46]. However, beyond a critical amplitude of the s-wave pairing, which for the in- trinsic (int) and extrinsic (ext) TOTSCs are respectively  ,int s =  p,  ,ext s = q  2p +  2 1/3 , (4) the fully gapped paired state becomes topologically triv- ial and thus no longer supports corner modes. Lattice model and numerical results. To anchor the above outlined key results, next we perform a numeri- cal analysis on a cubic lattice. The lattice-regularized Hamiltonian corresponding to Eq. (1), which, pending the representation of the   matrices, also describes the octupolar Dirac insulator (de ned below) and the real- ization of the TOTSC therein, reads [49] Hlat octu = t1 X j=1,2,3 sin(kja) j +  4 mlat 1 (k)  1  5   3 dlat 1 (k)  2  6 dlat 2 (k) +  s . (5) Here a is the lattice spacing and mlat 1 (k) = m0  6t0 + 2t0[cos(kxa)+cos(kya)+cos(kza)] is the  rst-order Dirac mass. For intrinsic and extrinsic TOTSCs dlat 1 (k) = cos(kxa)  cos(kya) and sin(kxa) sin(kya), respectively, while dlat 2 (k) = 2 cos(kza)  cos(kxa)  cos(kya) in both cases. Here j = 1, 2 and 3 correspond to x, y and z, respectively. Comparing with Eq. (1), we  nd the fol- lowing correspondences among the matrices  j =  13j for j = 1, 2, 3,  4 =  300,  5 =  110,  6 =  200 and   =  100. When expanded around the   = (0, 0, 0) point of the cubic Brillouin zone, for example, Hlat octu takes the form of HFS octu with m = (2t0a2) 1,   = m0, kF = a 1, and  p = t1. We implement the above tight binding model on a cubic lattice with open boundary con- dition and numerically diagonalize it for di erent cuts of the crystal. The results are displayed in Figs. 1 and 2. Eight zero energy Majorana corner modes are found when 0 < m0/t0 < 12. On the other hand, for m0/t0 < 0 and m0/t0 > 12, the paired state is topologically trivial [46]. In the following, we identify the octupolar Dirac insulator as a suitable platform for the realization of the TOTSC and the corresponding Majorana corner modes. Octupolar Dirac insulators. The lattice model for an octupolar Dirac insulator takes the form shown in Eq. (5) when  s = 0, with eight-component mutually anticom- muting Hermitian   matrices now given by  j =  1 1 j for j = 1, 2, 3,  4 =  1 3 0,  5 =  1 2 0, and  6 =  2 0 0. Three sets of Pauli matrices { }, { } and { } respectively act on the spin ( ,  ), parity ( ) and sublat- tice (A,B) indices. The Hamiltonian is invariant under a composite PT symmetry, where T = ( 0 0 2)K, P =  1 3 0, and under P: k  k. Here T and P respec- tively play the role of time-reversal and parity operators, with (T P)2 =  1. Furthermore, the Hamiltonian is in- variant under an additional parity operator P  =  2 1 0 and P  : k  k, and enjoys a unitary particle-hole or chiral symmetry, generated by  7 =  3 0 0. Even though the above model for 0 < m0/t0 < 12 supports a topological octupolar insulator with charged corner modes, here we consider the trivial regimes, m0/t0 < 0 and m0/t0 > 12. The normal state then does not support any topological boundary modes. Therefore, the appear- ance of Majorana bound states can solely be attributed to pairing, which we discuss next. To select the pairing realizing the TOTSC in an oc- tupolar insulator, we  rst notice that the system supports 28 (the number of purely imaginary eight-component Hermitian matrices) local (onsite or intra-unit) cell pair- ings, due to the Pauli exclusion principle. To capture all the pairings in a uni ed framework we Nambu-double the original eight-component spinor, and absorb the uni- tary part of the time-reversal operator (T ) in the hole part of the Nambu spinor. In such a basis the octupo- lar Dirac insulator takes the form shown in Eq. (5), with sixteen-dimensional   matrices taking the explicit forms  1 =  3 1 1 1,  2 =  3 1 1 2,  3 =  3 1 1 3,  4 =  3 1 3 0,  5 =  0 1 2 0,  6 =  0 2 0 0. (6) 4 The chemical potential term is given by  ( 3 0 0 0). A local pairing (with a constant amplitude) supporting Majorana corner modes satis es the following algebraic constraints. It anticommutes with the Dirac kinetic en- ergy (proportional to t1) and commutes with the  rst- order Dirac mass [50]. The paired state then represents a fully gapped topological pairing with two-dimensional dispersive massless Majorana modes occupying all six surfaces of a cubic crystal, when  1 =  2 = 0. In addition, the paired state must also simultaneously anti- commute with two higher-order Wilson-Dirac insulating masses (proportional to  5 and  6), such that surface states get partially gapped, leaving eight corners gapless. Only one pairing satis es all these constraints [46], for which the e ective single-particle Hamiltonian is Hoctu =  ( 1 cos   +  2 sin  )  1 1 0, (7) where   is the U(1) superconducting phase and  is the pairing amplitude. This pairing is a spin-singlet, but mixes even and odd parity bands, and two sublattices. We numerically diagonalize Hlat octu corresponding to the octupolar insulator in the presence of this pairing and  nd the eight zero-energy corner Majorana modes in a cubic system, cleaved according to the chosen form of dlat 1 (k), similar to Figs. 1 and 2, thus yielding a TOTSC. If, on the other hand, we set  2 = 0, the same paired state corresponds to a second-order topological supercon- ductor with gapless hinge modes along the z direction and surface states occupying the xy surfaces [46]. These observations can be supported by projecting the above local pairing onto the Fermi surface using the band basis of the single-particle Hamiltonian in Eq. (5), and neglecting the interband pairing components. The re- duced Hamiltonian (after a suitable global unitary ro- tation) assumes the form of HFS octu in Eq. (1), when ex- panded around the   or R point of the Brillouin zone. Furthermore, with appropriate choices of the insulating mass form factor dlat 1 (k) the same local pairing from Eq. (7) yields either intrinsic or extrinsic TOTSC [46]. Therefore, the local pairing Hoctu imposes a nontrivial octupolar topology when projected onto the Fermi sur- face, in spite of the parent insulating phase being trivial. These conclusions remain qualitatively unchanged when the normal state is a topological octupolar insulator. Topological invariant. Intrinsic and extrinsic TOTSCs can be distinguished besides by symmetry, also in terms of a bulk topological invariant, the octupolar moment Qxyz [51 53]. To extract Qxyz, we treat holelike excita- tions as independent particlelike excitations and compute n = Re \"  i 2  Tr ln ( U   exp \" 2 i X r  qxyz(r) # U )!# , (8) where  qxyz(r) = xyz n(r)/L3,  n(r) is the number operator at r = (x, y, z) of a periodic cubic system of linear dimen- -6 0 6 12 18 0 6 12 m0/t0   Trivial pairing Third-order topological pairing Trivial pairing (a) -6 0 6 12 18 0 6 12 m0/t0   Trivial pairing Third-order top ological pairing (b) FIG. 3: Phase diagrams of TOTSCs (always supporting eight corner Majorana modes) for (a) lattice regularized BdG Hamiltonian and (b) local pairing in Eq. (7) in an octupolar Dirac insulator for t1 = 1. For intrinsic (extrinsic) TOTSC Qxyz = 0.5 (0.0). Trivial pairing does not support any corner modes and Qxyz = 0 therein. In (a)  1 =  2 =  , while in (b)  1 =  2 = 1.0 and  denotes amplitude of the local pair- ing in Eq. (7). The octupolar Dirac insulator is topological (trivial) for 0 < m0/t0 < 12 (m0/t0 < 0 and m0/t0 > 12). sion L in each direction, and U is constructed by colum- nwise arranging the eigenvectors for the negative energy states. The octupolar moment is de ned as Qxyz = n  nal (modulo 1), where nal = (1/2) P r xyz/L3 repre- sents n in the atomic limit and at half  lling. We compute Qxyz for the lattice regularized BdG Hamiltonian and the local pairing in an octupolar Dirac insulator [Eqs. (5) and (7)], which depending on the form factor dlat 1 (k) yields in- trinsic or extrinsic TOTSC. While the octupolar moment is quantized Qxyz = 0.5 in an intrinsic TOTSC, Qxyz = 0 in an extrinsic TOTSC. In terms of the corner modes and Qxyz, we construct cuts of the phase diagram for intrinsic and extrinsic TOTSCs in Fig. 3. Summary and discussions. We show that time-reversal symmetry breaking mixed parity octupolar p  (d  + id3z2 r2) pairing supports eight corner localized Majo- rana modes in properly cleaved cubic crystals [Figs. 1 and 2]. There are two such orders, representing intrinsic (for   = x2  y2) and extrinsic (for   = xy) TOTSCs. The corner modes can be detected by scanning tunnel- ing microscopy, for example. We furthermore identify a doped octupolar (topological or trivial) Dirac insulator as the suitable material platform where such supercon- ducting order can arise from local or on-site Cooper pairs. Remarkably, among all possible local pairings in this sys- tem, the unique pairing supporting the Majorana corner modes is also energetically most favored over a wide range of m0/t0, covering both topological and trivial Dirac in- sulating phases in the normal state [46]. In addition, the TOTSC and its associated corner modes remain stable in the presence of a weak induced s-wave pairing. Presently, NaCl is the only known candidate material for octupolar topological Dirac insulator [54] and it may be a superconductor under pressure with transition tem- 5 perature Tc  2-7K [42]. Nonetheless, structurally anal- ogous binary compounds such as InTe, SnAs and SnSb under high pressure also show superconductivity with Tc  1-3K [43 45]. Given that our analysis suggests that the doped octupolar Dirac insulator does not need to be topological to accommodate TOTSC, which is at the same time energetically most favorable topological pairing in this system [46], we expect that te topologi- cal nature of superconductivity in these materials will be scrutinized more thoroughly in the future. Our proposal should also stimulate the search for new octupolar Dirac materials. Indeed, a recent study [55] reported possi- ble candidate materials for the realization of the octupo- lar Dirac insulator in Ti4XTe3, with X=Pb, Sn. When doped, these materials will constitute an ideal platform to harbor TOTSCs. Acknowledgments. B.R. was supported by the startup grant from Lehigh University and thanks Andr as L. Szab o for useful discussions. V.J. acknowledges support of the Swedish Research Council (VR 2019-04735). Note added. After completing this work we became aware of a study where proximity-induced TOTSC in doped third-order topological insulator with preexisting charged corner modes has been discussed [56].  Corresponding author: bitan.roy@lehigh.edu [1] C. Nayak, S. H. Simon, A. Stern, M. Freedman, and S. Das Sarma, Rev. Mod. Phys. 80, 1083 (2008). [2] C. W. J. Beenakker, Annu. Rev. Cond. Mat. Phys. 4, 113 (2013). [3] Y. Oreg and F. von Oppen, Annu. Rev. Cond. Mat. Phys. 11, 397 (2020). [4] W. A. Benalcazar, B. A. Bernevig, and T. L. Hughes, Science 357, 61 (2017). [5] W. A. Benalcazar, B. A. Bernevig, and T. L. Hughes, Phys. Rev. B 96, 245115 (2017). [6] Z. Song, Z. Fang, and C. Fang, Phys. Rev. Lett. 119, 246402 (2017). [7] F. Schindler, Z. Wang, M. G. Vergniory, A. M. Cook, A. Murani, S. Sengupta, A. Y. Kasumov, R. Deblock, S. Jeon, I. Drozdov, H. Bouchiat, S. Gu eron, A. Yazdani, B. A. Bernevig, and T. Neupert, Nat. Phys. 14, 918 (2018). [8] J. Langbehn, Y. Peng, L. Trifunovic, F. von Oppen, and P. W. Brouwer, Phys. Rev. Lett. 119, 246401 (2017). [9] L. Li, M. Umer, and J. Gong, Phys. Rev. B 98, 205422 (2017). [10] D. C alug aru, V. Juri ci c, and B. Roy, Phys. Rev. B 99, 041301(R) (2019). [11] D. Varjas, A. Lau, K. P oyh onen, A. R. Akhmerov, D. I. Pikulin, I. C. Fulga, Phys. Rev. Lett. 123, 196401 (2019). [12] A. L. Szab o, R. Moessner, and B. Roy, Phys. Rev. B 101, 121301(R) (2020). [13] B. Wang, X. Zhou, H. Lin, A. Bansil, Phys. Rev. B 104, L121108 (2021). [14] A. C. Tyner, S. Sur, Q. Zhou, D. Puggioni, P. Darancet, J. M. Rondinelli, and P. Goswami, arXiv:2102.06207 [15] Q. Wei, X. Zhang, W. Deng, J. Lu, X. Huang, M. Yan, G. Chen, Z. Liu, and S. Jia, Nat. Mater. 20, 817 (2021). [16] C.-A. Li, S.-B. Zhang, J. Li, and B. Trauzettel, Phys. Rev. Lett. 127, 026803 (2021). [17] Y. Wang, M. Lin, and T. L. Hughes, Phys. Rev. B 98, 165144 (2018). [18] Z. Wu, Z. Yan, and W. Huang, Phys. Rev. B 99, 020508(R) (2019). [19] Q. Wang, C.-C. Liu, Y.-M. Lu, and F. Zhang, Phys. Rev. Lett. 121, 186801 (2018). [20] T. Liu, J. J. He, and F. Nori, Phys. Rev. B 98, 245413 (2018). [21] Y. Volpez, D. Loss, and J. Klinovaja, Phys. Rev. Lett. 122, 126402 (2019). [22] Z. Yan, Phys. Rev. Lett. 123, 177001 (2019). [23] X. Zhu, Phys. Rev. Lett. 122, 236401 (2019). [24] X.-H. Pan, K.-J. Yang, L. Chen, G. Xu, C.-X. Liu, and X. Liu, Phys. Rev. Lett. 123, 156801 (2019). [25] S. A. A. Ghorashi, X. Hu, T. L. Hughes, and E. Rossi, Phys. Rev. B 100, 020509(R) (2019). [26] S. Franca, D. V. Efremov, and I. C. Fulga, Phys. Rev. B 100, 075415 (2019). [27] B. Roy, Phys. Rev. Research 1, 032048 (2019). [28] S.-B. Zhang and B. Trauzettel, Phys. Rev. Research 2, 012018(R) (2020). [29] J. Ahn and B.-J. Yang, Phys. Rev. Research 2, 012060 (2020). [30] R.-X. Zhang, Y.-T. Hsu, and S. Das Sarma, Phys. Rev. B 102, 094503 (2020). [31] S. J. De, U. Khanna, S. Rao, Phys. Rev. B 101, 125429 (2020). [32] R. W. Bomantara, Phys. Rev. Research 2, 033495 (2020). [33] B. Roy, Phys. Rev. B 101, 220506(R) (2020). [34] M. Kheirkhah, Z. Yan, Y. Nagai, and F. Marsiglio, Phys. Rev. Lett. 125, 017001 (2020). [35] T. E. Pahomi, M. Sigrist, and A. A. Soluyanov, Phys. Rev. Research 2, 032068(R) (2020). [36] X. Wu, W. A. Benalcazar, Y. Li, R. Thomale, C-X. Liu, and J. Hu, Phys. Rev. X 10, 041014 (2020). [37] A. Tiwari, A. Jahin, and Y. Wang, Phys. Rev. Research 2, 043300 (2020). [38] A. K. Ghosh, T. Nag, and A. Saha, Phys. Rev. B 103, 045424 (2021). [39] B. Fu, Z.-A. Hu, C.-A. Li, J. Li, and S.-Q. Shen, Phys. Rev. B 103, L180504 (2021). [40] X-J. Luo, X-H. Pan, and X. Liu, Phys. Rev. B 104, 104510 (2021). [41] The   symbol indicates that pairing matrices in the odd and even parity channels fully anticommnute. Two d-wave components also always mutually anticommute as they break the time-reversal symmetry. [42] G. N. Stepanov, E. N. Yakovlev, and T. V. Valyanskaya, JETP Letters 29, 418 (1979). [43] S. Geller and G. W. Hull, Jr., Phys. Rev. Lett. 13, 127 (1964). [44] Md. R. Kasem, K. Hoshi, R. Jha, M. Katsuno, A. Ya- mashita, Y. Goto, T. D. Matsuda, Y. Aoki, and Y. Mizuguchi, Appl. Phys. Express 13, 033001 (2020). [45] M. Katsuno, R. Jha, K. Hoshi, R. Sogabe, Y. Goto, and Y. Mizuguchi, Condens. Matter 5, 14 (2020). [46] See Supplemental Materials at XXX-XXXX for band di- agonalization and mean- eld analysis of competing local pairings in an octupolar Dirac insulator, additional nu- merical results, and parameter range for corner modes. [47] M. Sigrist and K. Ueda, Rev. Mod. Phys. 63, 239 (1991). 6 [48] B. Roy, S. A. A. Ghorashi, M. S. Foster and A. H. Nev- idomskyy, Phys. Rev. B 99, 054505 (2019). [49] T. Nag, Juri ci c and B. Roy, Phys. Rev. B 103, 115308 (2021). [50] Any pairing order with a constant amplitude that an- ticommutes with the  rst-order uniform Dirac mass is topologically trivial as its surface states are fully gapped. [51] W. A. Wheeler, L. K. Wagner, and T. L. Hughes, Phys. Rev. B 100, 245135 (2019). [52] B. Kang, K. Shiozaki, and G. Y. Cho, Phys. Rev. B 100, 245134 (2019). [53] A. Agarwala, V. Juri ci c, and B. Roy, Phys. Rev. Research 2, 012067(R) (2020). [54] H. Watanabe and H.-C. Po, arXiv:2009.04845 [55] N. Mao, H. Wang, Y. Dai, B. Huang, and C. Niu, arXiv:2108.07946. [56] A. K. Ghosh, T. Nag, and A. Saha, Phys. Rev. B 104, 134508 (2021)."
    },
    {
        "title": "2303.00145",
        "year": "2023",
        "file_name": "2303.00145.pdf",
        "full_text": "MOBILE DISKS IN HYPERBOLIC SPACE AND MINIMIZATION OF CONFORMAL CAPACITY HARRI HAKULA, MOHAMED M. S. NASSER, AND MATTI VUORINEN Abstract. Our focus is to study constellations of disjoint disks in the hyperbolic space, the unit disk equipped with the hyperbolic metric. Each constellation corresponds to a set E which is the union of m > 2 disks with hyperbolic radii rj > 0, j = 1, ..., m. The centers of the disks are not fixed and hence individual disks of the constellation are allowed to move under the constraints that they do not overlap and their hyperbolic radii remain invariant. Our main objective is to find computational lower bounds for the conformal capacity of a given constellation. The capacity depends on the centers and radii in a very complicated way even in the simplest cases when m = 3 or m = 4. In the absence of analytic methods our work is based on numerical simulations using two different numerical methods, the boundary integral equation method and the hp-FEM method, resp. Our simulations combine capacity computation with minimization methods and produce extremal cases where the disks of the constellation are grouped next to each other. This resembles the behavior of animal colonies minimizing heat flow in arctic areas. 1. Introduction Many extremal problems of physics, exact sciences, and mathematics have solutions which exhibit varying degree of symmetry. A typical situation is to minimize or maximize a set functional of a planar set under the constraint that some other functional is constant. The classical isoperimetric problem [31] is an example. Here one maximizes the area of a planar set given its perimeter and the extremal domain is the disk. G. P lya and G. Szeg  [31] initiated a systematic study of a large class of isoperimetric type problems of mathematical physics for domain functionals such as moment of inertia, principal frequency, torsional rigidity, and, in particular, capacities of condensers. Certain geometric transformations, known under the general name  symmetrization  have the property that they decrease the value of domain functionals and thus can give hints about the extremal configuration of isoperimetric problems [3, 8]. We study here new types of transformations which decrease the value of conformal capacity. In a very interesting recent paper, A. Solynin [34] describes capacity problems, motivated by the behavior of herds of arctic animals which keep close together to minimize the total loss of heat of the herd or to defend against predators (see figures in [34]). Such a herd behavior seems to suggest the heuristic idea that  minimization of herd s outer perimeter  minimizes the loss of heat or danger from predators. This kind of extremal problem can be classified as special type of isoperimetric problem. As an illustration of the connection between the kind of transformations we are interested in and the observed behavior in nature, see Figure 1. File: mcd2arXiv2.tex, printed: 2023-11-30, 2.34 Key words and phrases. Multiply connected domains, condenser capacity, capacity computation. 1 arXiv:2303.00145v2 [math.CV] 29 Nov 2023 2 H. HAKULA, M. NASSER, AND M. VUORINEN (a) (b) (c) Figure 1. Examples of constrained optimisation. (a) Tree swallows huddle on a branch during a spring snowstorm [39]. (b) Minimal capacity configuration for four hyperbolic disks on a diameter. (c) Minimal capacity configuration for four hyperbolic disks on a hyperbolic circle. In (b) and (c) the hyperbolic disks are inside the unit disk equipped with the hyperbolic metric. In a recent paper [29], isoperimetric inequalities in hyperbolic geometry were applied to estimate the conformal capacity of condensers of the form (B2, E) where E is a union of finitely many disjoint closed disks Ej, j = 1, ..., m, in the unit disk B2. Thus E is a constellation of disks. Gehring s lower bound [9] (see also [29]) is given by condensers of the form (B2, E ) where E is a disk with the hyperbolic area equal to that of  m j=1Ej. Further recent investigations of condenser capacity in the framework of hyperbolic geometry include [28, 26, 27], where pointers MOBILE DISKS IN HYPERBOLIC SPACE 3 to earlier work can be found. It should be noticed that due to the conformal invariance of the conformal capacity, the hyperbolic geometry provides the natural setup for this study. We continue here this work and our goal is to analyse extremal cases of the aforementioned capacity and how the capacity depends on the geometry of the disk constellation. The constraint that the disks do not overlap leads to problems of combinatorial geometry. Some examples of such geometric problems, related to this work and the herd behavior mentioned above, are Descartes  problem of four circles with each circle tangent to three circles, Apollonian circle packing, and Soddy s  complex kiss precise  problem for configurations of mutually tangent circles [21]. Combinatorial geometry extremal problems motivated by biochemistry research and drug development are described in [23]. A very interesting discussion of many topics of combinatorial geometry including packing problems is given in the encyclopedic work of M. Berger [6]. The three dimensional case is much more difficult than the planar case and it is the subject of the extensive review paper [20] where topics range from optimal packing of spheres to constrained motion of small spheres on the surface of the unit sphere. For an extensive survey of potential theoretic extremal problems see [7]. Analysing the extremal cases of the lower bound for cap(B2,  m j=1Ej) for a constellation of disjoint hyperbolic disks Ej seems to be very difficult even in the simplest cases m = 3, 4. Therefore we consider this problem in special cases such as the case when the circle centers are at the same distance from origin or analyse constrained motion of one circle along three other fixed circles (see Figure 1). Simulations indicate that several constellations yield local minima of the capacity. Throughout, the hyperbolic geometry provides the natural geometric framework for our study, because of the conformal invariance of the capacity. We use two numerical methods for computing the capacity, the hp-FEM and the boundary integral equation (BIE) method. The numerical results lead to a number of conjectures and improved bounds. Indeed, the existing lower bound for constellations considered here is improved of the order of 10% for disks of unit hyperbolic radius. Moreover, the asymptotic nature of the theoretical lower bound as the hyperbolic radii rj  is easily understood in the context of hyperbolic geometry. In modern physics, in particular in condensed matter physics, there has been a lot of interest in geometric settings with negative curvature [19, 22], that is, exactly our natural setup. The purpose of this paper is also to show how computations can be formulated and performed in both Euclidean and hyperbolic geometries, even with the possibility of moving from one to another. This is highlighted in the last section where the optimal configurations in hyperbolic geometry are found by successive transformations to a Euclidean coordinate system employed in the optimization routines. For information about potential theory and its applications, see [7, 31, 32, 36]. The contents are organized into sections as follows. Section 2 contains the key facts about hyperbolic geometry, including the transformation formulae from Euclidean disks to Poincar  disks and back. Section 3 covers the preliminary notations of conformal capacity, collected from various sources, e.g. from [4, 8, 10, 11, 17, 16]. These are the cornerstones of the geometric setup of the computations in the sequel. Section 3 also provides an overview of the hp-FEM [15, 14] adjusted to the present computational tasks, our second computational work horse, 4 H. HAKULA, M. NASSER, AND M. VUORINEN Figure 2. Visualisations on Poincar  disk. Left: Images of hyperbolic disks with hyperbolic radii = 1, 2, 3, 4, 5. Right: Hyperbolic disks on three diameters all with equal radii. Notice the lens-shaped regions containing the disks on each diameter. the BIE method [25, 28], and the interior-point method used in optimization. The numerical experiments are discussed in Sections 4 and 5. In Section 4 the selected configurations have been designed a priori, with the goal of forming an understanding of the identifiable geometric features of the minimal capacity configurations. In Section 5 that understanding is challenged by searching for the minimal capacity configurations using numerical optimization starting with random initial configurations. Finally, the conclusions are drawn in Section 6. 2. From Euclidean Disk to Poincare and Back In this section the central transformation formulae collected from various sources are pre- sented. In Figure 2 different properties of geometry on the Poincar  disk have been illustrated. In particular, the facts that for all   > 0, M > 0 there are hyperbolic disks with radii M but Euclidean diameter <   and hyperbolic disks with equal radii have different Euclidean radii depending on their location are important for our discussion below. For a point x  Rn and a radius r > 0, define an open Euclidean ball Bn(x, r) = {y   Rn | |x  y| < r} and its boundary sphere Sn 1(x, r) = {y  Rn | |x  y| = r}. For the unit ball and sphere, we use the simplified notations Bn = Bn(0, 1) and Sn 1 = Sn 1(0, 1). The segment joining two points x, y  Rn is denoted [x, y]. Define the hyperbolic metric in the Poincar  unit disk B2 as in [4], [5, (2.8) p. 15] sh2 B2(x, y) 2 = |x  y|2 (1  |x|2)(1  |y|2), x, y  B2. (2.1) We use the notation sh and arsh for the hyperbolic sine and its inverse, respectively, and similarly, th and arth for the hyperbolic tangent and its inverse. The hyperbolic midpoint of MOBILE DISKS IN HYPERBOLIC SPACE 5 x, y  B2 is given by [37] mH(x, y) = y (1  |x|2) + x (1  |y|2) 1  |x|2|y|2 + A[x, y] p (1  |x|2)(1  |y|2) where A[x, y] = p |x  y|2 + (1  |x|2)(1  |y|2). We use the notation B (x, M) = {z  B2 :  B2(x, z) < M} for the hyperbolic disk centered at x  B2 with radius M > 0 . It is a basic fact that they are Euclidean disks with the center and radius given by [16, p.56, (4.20)] (2.2)           B (x, M) = B2(y, r) , y = x(1  t2) 1  |x|2t2 , r = (1  |x|2)t 1  |x|2t2 , t = th(M/2) , Note the special case x = 0, (2.3) B (0, M) = B2(0, th(M/2)) . Conversely, the Euclidean disks can be considered as hyperbolic ones by [37] (2.4) ( B2(y, r) = B (x, M) , x = t y/|y| , M =  B2(x, z) , t = mH (|y|  r, |y| + r) , Lemma 2.5 ([4, Thm 7.2.2, p. 132]). The area of a hyperbolic disc of radius r is 4  sh2(r/2) and the length of a hyperbolic circle of radius r is 2  sh(r). 3. Conformal Capacity and Numerical Methods A condenser is a pair (G, E), where G  B2 is a domain and E is a compact non-empty subset of G. The conformal capacity of this condenser is defined as [8, 10, 11, 16, 17] cap(G, E) = inf u A Z G | u|2dm, (3.1) where A is the class of C  0 (G) functions u : G  [0,  ) with u(x)  1 for all x  E and dm is the 2-dimensional Lebesgue measure. In this paper we assume that G = B2 is the unit disk and E =  m j=1Ej where E1, . . . , Em are m closed disjoint disks in the unit disk. Hence  = G\\E is a multiply connected circular domain of connectivity m+1. In this case, the infimum is attained by a function u which is harmonic in  and satisfies the boundary conditions u = 0 on  G and u = 1 on  E [8]. The capacity can be expressed in terms of this extremal function as (3.2) cap(G, E) = ZZ   | u|2dm. The conformal capacity of a condenser is one of the key notions of potential theory of elliptic partial differential equations [17, 11] and it has numerous applications to geometric function theory, both in the plane and in higher dimensions, [8, 10, 11, 16, 17]. Numerous variants of 6 H. HAKULA, M. NASSER, AND M. VUORINEN Figure 3. Discretization and optimization. For the given set of four hyperbolic disks with centers constrained on a diameter, the configuration shown here mini- mizes the capacity. Left: Configuration and hp-FEM mesh. Center: Potential in 2D. Right: Potential in 3D. the definition (3.1) of capacity are given in [10, 11]. First, the family A may be replaced by several other families by [10, Lemma 5.21, p. 161]. Furthermore, cap(G, E) = M( (E,  G; G)), (3.3) where  (E,  G; G) is the family of all curves joining E with the boundary  G in the domain G and M stands for the modulus of a curve family [10, Thm 5.23, p. 164]. For the basic facts about capacities and moduli, the reader is referred to [10, 11, 16, 17]. 3.1. Numerical Methods. In this section the numerical methods used in the numerical ex- periments are briefly described. The capacities are computed using the hp-version of the finite element method (FEM) and the boundary integral equation with the generalized Neumann ker- nel method (BIE). The minimization problems are computed using the interior-point method as implemented in MATLAB and Mathematica. Since the Dirichlet problem (3.1) is one of the primary numerical model problems, any stan- dard solution technique can be viewed as having been validated. Verification of the results is discussed in connection with one of the numerical experiments below. 3.1.1. hp-FEM. What is of particular interest in the context of this paper is that the hp-FEM allows for large curved elements without significant loss of accuracy. Since the number of elements can be kept relatively low given that additional refinement can always be added via elementwise polynomial degree, variation in the boundary can be addressed directly at the level of the boundary representation in some exact parametric form. This is illustrated in Figure 3. The following theorem due to Babu ka and Guo [2] sets the limit to the rate of convergence. Notice that construction of the appropriate spaces is technical. For rigorous treatment of the theory involved see Schwab [33] and references therein. Theorem 3.4. Let  R2 be a polygon, v the FEM-solution of (3.1), and let the weak solution u0 be in a suitable countably normed space where the derivatives of arbitrarily high order are MOBILE DISKS IN HYPERBOLIC SPACE 7 controlled. Then inf v  u0  v H1( )  C exp( b 3  N), where C and b are independent of N, the number of degrees of freedom. Here v is computed on a proper geometric mesh, where the order of an individual element is set to be its element graph distance to the nearest singularity. (The result also holds for meshes with constant polynomial degree.) Consider the abstract problem setting with u defined on the standard piecewise polynomial finite element space on some discretization T of the computational domain  . Assuming that the exact solution u  H1 0(D) has finite energy, we arrive at the approximation problem: Find  u  V such that (3.5) a( u, v) = l(v) (= a(u, v)) ( v  V ), where a(   ,   ) and l(   ), are the bilinear form and the load potential, respectively. Additional degrees of freedom can be introduced by enriching the space V . This is accomplished via introduction of an auxiliary subspace or  error space  W  H1 0(D) such that V  W = {0}. We can then define the error problem: Find    W such that (3.6) a( , v) = l(v)  a( u, v)(= a(u  u, v)) ( v  W). This can be interpreted as a projection of the residual to the auxiliary space. The main result on this kind of estimators for the Dirichlet problem (3.1) is the following theorem. Theorem 3.7 ([14]). There is a constant K depending only on the dimension d, polynomial de- gree p, continuity and coercivity constants C and c, and the shape-regularity of the triangulation T such that c C  1  u  u 1  K ( 1 + osc(R, r, T )) , where the residual oscillation depends on the volumetric and face residuals R and r, and the triangulation T . 3.1.2. BIE method. We review a BIE method from [28] for computing the capacity cap(B, E). The method is based on the BIE with the generalized Neumann kernel. The domains considered in this paper are circular domains, i.e., domains whose boundary components are circles. The external boundary is the unit circle, denoted by C0, is parametrized by  0(t) = eit for t   J0 = [0, 2 ]. The inner circles Cj are parametrized by  j(t) = zj + rje it, t  Jj = [0, 2 ], for j = 1, 2, . . . , m, where zj is the center of the circle Cj and rj is its radius. Let J be the disjoint union of the m + 1 intervals Jj = [0, 2 ], j = 0, 1, . . . , m. We define a parametrization of the whole boundary C =  m j=0Cj on J by (see [25] for the details)  (t) =                0(t), t  J0,  1(t), t  J1, ...  m(t), t  Jm. 8 H. HAKULA, M. NASSER, AND M. VUORINEN With the parametrization  (t) of the whole boundary C, we define a complex function A by (3.8) A(t) =  (t)  , where   is a given point in the domain G. The generalized Neumann kernel N(s, t) is defined for (s, t)  J   J by (3.9) N(s, t) := 1  Im  A(s) A(t)  (t)  (t)  (s)   . We define also the following kernel (3.10) M(s, t) := 1  Re  A(s) A(t)  (t)  (t)  (s)   , (s, t)  J   J. The kernel N(s, t) is continuous and the kernel M(s, t) is singular where the singular part involves the cotangent function. Hence, the integral operator N with the kernel N(s, t) is compact and the integral operator M with the kernel M(s, t) is singular. Further details can be found in [38]. For each k = 1, 2, . . . , m, let the function  k be defined by (3.11)  k(t) = log | (t)  zk|, let  k be the unique solution of the BIE (3.12)  k  N k =  M k, and let the piecewise constant function hk = (h0,k, h1,k, . . . , hm,k) be given by (3.13) hk = [M k  (I  N) k]/2. For each k = 1, 2, . . . , m, the solution  k of the BIE (3.12) and the piecewise constant function hk in (3.13) will be computed using the MATLAB fbie from [25]. In the function fbie, the integral equation (3.12) is solved using the Nystr m method with the trapezoidal rule. Solving the integral equation is then reduced to solving an (m + 1)n   (m + 1)n linear system which is solved by the MATLAB function gmres. The matrix-vector product in gmres is computed by the MATLAB function zfmm2dpart from the MATLAB toolbox FMMLIB2D [12]. To use the MATLAB function fbie, we define a vector s = [s1, . . . , sn] where sk = 2(k  1) /n, k = 1, . . . , n, and n is a given even positive integer. Then we compute the (m+1)n 1 discretization vectors et and etp of the parametrization  (t) of the boundary C and its derivative  (t) by et = [ 0(s),  1(s), . . . ,  m(s)]T, etp = [  0(s),   1(s), . . . ,   m(s)]T. We also discretize the functions A(t) and  k(t) by A = et  and gamk =  k(et), k = 1, . . . , m. Then we compute (m + 1)n   1 approximate discretizations muk and hk of the functions  k(t) and hk(t) by calling [muk, hk] = fbie(et, etp, A, gamk, n, 5, [ ], 1e  14, 100), i.e., the tolerance of the FMM is 0.5 10 15, the GMRES is used without restart, the tolerance of the GMRES method is 10 14 and the maximal number of GMRES iterations is 100. By computing the (m + 1)n   1 vector hk, we obtain approximate discretizations of the piecewise constant function hk = (h0,k, h1,k, . . . , hm,k) in (3.13). Note that, for k = 1, . . . , m, MOBILE DISKS IN HYPERBOLIC SPACE 9 the constant hj,k is the value of the function hk on the boundary component  j. We approximate the values of the real constants hj,k by taking arithmetic means hj,k = 1 n (j+1)n X i=1+jn hki, j = 0, 1, . . . , m, k = 1, . . . , m. The values of the m real constants a1, . . . , am are then approximated by solving the (m + 1)   (m + 1) linear system [28] (3.14)     h0,1 h0,2       h0,m 1 h1,1 h1,2       h1,m 1 ... ... ... ... ... hm,1 hm,2       hm,m 1         a1 a2... am c     =     0 1 ... 1    . Since m + 1 is the number of boundary components of the domain  = G \\ E, we can assume that m is small and solve the linear system (3.14) using the Gauss elimination method. By solving the linear system, the capacity cap(B, E) will be computed by [28, Eq. (3.9)] (3.15) cap(B, E) = 2  m X k=1 ak. In this paper, the boundary components of the domain  are circles. Thus, the integrands in (3.12) and (3.13) will be 2 -periodic functions, and can be extended holomorphically to some parallel strip |Im t| <   in the complex plane. Hence, the trapezoidal rule will then converge exponentially with O(e n) [35] when it is used to discretize the integrals in (3.12) and (3.13). The numerical solution of the integral equation will converge with a similar rate of convergence [1, p. 322] (see Figure 5 (right) below). 3.1.3. Nonlinear Optimization: Interior-Point Method. The two methods outlined above are combined with a numerical optimization routine in the last set of numerical experiments below. The task is to find an optimal configuration for a set of hyperbolic disks E with fixed radii. We use the interior-point method as implemented in Mathematica (FindMinimum, [40]) and Matlab (fmincon, [24]). In the most general case the problem is defined as in (3.16), where the only constraint is a geometric one, that is, the disks are not allowed to overlap. Here, the radii are fixed and the optimization concerns only the locations of the disks. min E cap(G, E) subject to: Ei  Ej =    i, j = 1, . . . , m, i  = j (3.16) Ej G  j = 1, . . . , m. This nonlinear optimization problem can be solved using the interior-point method. This solution would be a local minimum. The standard textbook reference is Nocedal and Wright [30]. Notice, that the objective function is indeed the capacity of the constellation. Often opti- mization problems with geometric constraints are related to packing and fitting problems. The 10 H. HAKULA, M. NASSER, AND M. VUORINEN task here is orders of magnitude more demanding since, at every point evaluation one solution of the capacity problem has to be computed, and as the disks move the constraints change as well. The number of evaluations is greater than the number of iteration steps, since the gradients and Hessians must be approximated numerically. It should be noted that the success of the optimization depends on the high accuracy of the capacity solver, since otherwise the approximate derivatives are not sufficiently accurate. In the context of this work, there have been no attempts to devise a special method that would incorporate some of the insights gathered during this study. Instead, the numerical optimization is used to challenge those insights and therefore the optimizations have been computed with minimal input information. 4. Minimizing Capacity: Constrained Configurations As mentioned above, even with a small number of disks the combinatorial explosion of the number of configurations is evident. Therefore, we restrict ourselves to a series of experiments each with increasing complexity building toward an understanding of the fundamental geometric principles behind the minimal configurations. In each case we consider a set of hyperbolic disks Ej with radii rj, where some geometric constraint is placed on all or some of the disks in the constellation. An initial observation is that due to conformal invariance of the capacity, its numerical value remains invariant under a M bius transformation of the unit disk onto itself. Therefore we may assume that the disk with the largest radius r1 is centered at the origin. Further, consider a disk B (z2, r2) with center z2 on the segment (0, 1). The disk lies in the lens-shaped region W = B2(i ,   1 +   2)  B2( i ,   1 +   2),   > 0, with  B2(0, iv) = r2 where v =   1 +   2   and is tangent to both boundary arcs of W and  1  W, see Figure 2 (right). Every disk lies within its own associated lens-shaped domain. 4.1. Disks with collinear centers. Consider a set of m hyperbolic disks Ej with radii rj and centers on the diameter ( 1, 1) with Pm j=1 2rj = d1 =  B2( 0.6, 0.6). We choose the hyperbolic centers of these disks so that the hyperbolic distance between them is d  0 where d = 0 corresponds to the case when they touch each other. The goal is to establish upper and lower bounds for cap(B,  Ej). Since the hyperbolic radius of a hyperbolic disk is invariant under M bius transform, in view of (2.2), we have cap(B, Ej) = 2 / log(1/ th(rj/2)) for all Ej. The cases cap(B,  m j=1Ej) for m = 2, 3, 4 over the range 0.02  d  4 are shown in Figure 4. The conjectured lower bound with d = 0 is computed with hp-FEM (see the  red dot  in Figure 4 (right)), all other capacities are computed with BIE. From Figure 4 we also see that cap(B,  m j=1Ej)   m X j=1 cap(B, Ej), as the separation d becomes large. MOBILE DISKS IN HYPERBOLIC SPACE 11 Figure 4. The hyperbolic disks when the hyperbolic distance d between them is d = 0.02 (left) and d = 1 (middle). On the right, cap(B,  k j=1Ej) as a function d. In the first row: r1 = 0.55d1/2 and r2 = 0.45d1/2 where d1 =  B2( 0.6, 0.6). In the second row: r1 = 0.35d1/2, r2 = 0.25d1/2, and r3 = 0.40d1/2. In the third row: r1 = 0.35d1/2, r2 = 0.15d1/2, r3 = 0.20d1/2, and r4 = 0.30d1/2. 4.1.1. Verification of results. Let us consider the case with four disks and set E =  4 j=1Ej. The initial position is when the disks are contiguous, tangent to each other, and then the hyperbolic distance d between the disks increases from 0 to 0.3. The conclusion is that the value d = 0 yields the minimal value of the capacity of the constellation. 12 H. HAKULA, M. NASSER, AND M. VUORINEN Table 1. Disks with collinear centers: m hyperbolic disks Ej with radii rj and centers on the diameter ( 1, 1) with Pm j=1 2rj = d1 =  B2( 0.6, 0.6). Conjectured lower and upper bounds of the capacity cap(B,  Ej). m Lower Upper 2 8.515312094751020 11.463763614692954 3 7.450131756754710 12.744594178229441 4 7.017838565418236 14.282099489357595 Table 2. Computed values of cap(B2, E) when m = 4 for a constellation with disk radii (from left to right) r1 = 0.15d1/2, r2 = 0.35d1/2, r3 = 0.20d1/2, and r4 = d1/2  (r1 + r2 + r3) where d1 =  B2( 0.6, 0.6). The centers on the diameter ( 1, 1) as a function of the hyperbolic distance d between disks, i.e., c1 =  th((r2 + r1 + d)/2), c2 = 0, c3 = th((r2 + r3 + d)/2), c4 =  th((r2 + 2r3 + r4 + 2d)/2). d FEM BIE Agreement 0.00 7.017838565413617     0.05 7.230698262298420 7.230698262298405 1.51   10 14 0.10 7.442082617728579 7.442082617728490 8.88   10 14 0.15 7.651760366696882 7.651760366696745 1.37   10 13 0.20 7.859490827905997 7.859490827905935 6.22   10 14 0.25 8.064996233395842 8.064996233395734 1.08   10 13 0.30 8.267972932727597 8.267972932727497 9.95   10 14 The values of the capacity cap(B2, E) in Table 2 have been computed using both methods, the FEM and the BIE method. For the BIE, we use n = 27 and   = 0.8i. Table 2 shows the absolute differences between the computed values which indicates a good agreement between the two methods. As in [13], the values computed using the FEM will be considered as reference values and used to estimate the error in the values computed by the BIE method for several values of n. The BIE method cannot be used for d = 0. The error for d = 0.05, 0.1, . . . , 0.3 is presented in Figure 5 (right) which illustrates the exponential convergence with order of convergence O(e n) where   =  log | |  0.223. Numerical experiments (not presented here) with other values of   indicate that the order of convergence depends on   as well as the centers z1, . . . , zm and the radii r1, . . . , rm of the inner circles. A detailed analysis of the order of convergence for the above BIE method is a subject of future work. 4.2. Four disks: Permutation of contiguous disks. We consider next two cases where all the disks of the constellation have fixed hyperbolic radii A > B > C > D > 0 but their relative ordering is not constrained other than that each disk is tangent to at least one other disk of the constellation and their hyperbolic centers lie (a) either on the diameter ( 1, 1) or (b) on the circle {z : |z| = 1/2}. Now the question is what is the effect of the permutation of the disks on the capacity. There are 24 permutations with 12 different capacities due to symmetry. For every realisation, the MOBILE DISKS IN HYPERBOLIC SPACE 13               17 19 21 23 25 27 29 10-13 10-11 10-9 10-7 10-5 17 19 21 23 25 27 29 10-13 10-11 10-9 10-7 10-5 N 3 Error estimate Figure 5. The error for the constellation of four disks in Table 2. Left: The hp- FEM error estimate as a function of 3  N, where N is the number of d.o.f. (logplot) for four disk configuration with contacts (d = 0). The observed constant or the slope of the graph = 37.1. Right: The errors in the computed values of cap(B2, E) using the BIE method as functions of n, for   = 0.8i where   =  log | |  0.223. Table 3. Permutations of contiguous constellations. ED with centers on the segment ( 1, 1) and (A, B, C, D) = (1/2, 2/5, 1/4, 1/5). EC with centers on the circle {z : |z| = 1/2} and (A, B, C, D) = (1/2, 1/3, 1/4, 1/5). Case r1 r2 r3 r4 cap C(ED) cap C(EC) 1 D B A C 6.781488018927628 6.451424010111881 2 D A B C 6.788910565780309 6.455800945561348 3 D C A B 6.843774515059010 6.475070264106950 4 C D A B 6.882473842468833 6.485425869048534 5 A B C D 6.890544149275032 6.496389476635198 6 B C A D 6.897202225461369 6.500210100051595 7 C A D B 6.919626376828870 6.520197932005349 8 A B D C 6.928074481413122 6.523073055329720 9 A C B D 6.932436180755356 6.542555705939787 10 C B D A 6.962814943144452 6.542981227003898 11 A C D B 7.053764008325471 6.575258877036491 12 A D C B 7.055565195334228 6.576332514877286 radii are denoted by rj from left to right and the constellations are denoted by ED and EC, respectively. For ED we set (A, B, C, D) = (1/2, 2/5, 1/4, 1/5), and for EC slightly perturbed (A, B, C, D) = (1/2, 1/3, 1/4, 1/5). The results are collected in Table 3 and Figure 1 shows the observed extremal permutations. Interestingly, the resulting capacities have exactly the same dependence on the relative sizes of the radii. 4.3. Three immobile disks, one rolling disk. In the final experiment of the section we study the situation when one disk is free to roll on the remaining three contiguous immobile disks, centers on the diameter ( 1, 1) and tangent to each other. The route of the mobile disk 14 H. HAKULA, M. NASSER, AND M. VUORINEN Table 4. Hyperbolic radii used in Figure 6. Case r1 r2 r3 r4 1 0.4 0.2 0.5 0.25 2 0.2 0.5 0.3 0.5 3 0.5 0.5 0.5 0.2 4 0.2 0.7 0.4 0.1 Figure 6. Three immobile disks, one rolling disk. Cases 1 to 4 from left to right. Dependence of the capacity on the relative location of the rolling disk. The hyperbolic center z4 of the moving disk is on the red curve shown in the figure. is parametrized with a parameter    [0, 1] where the values 0 and 1 are for the case when also the mobile disk has its center on the diameter ( 1, 1) and the values 1/3 and 2/3 correspond to the intermediate points on the route when the rolling disk is tangent to two immobile disks. Depending on the radii, it might also happen that there is only one such point. In Figure 6 below we see that for the values 1/3 and 2/3 the capacity of the constellation attains a local minimum. The numerical results for this example are computed using the BIE method. So, instead of assuming that the disks are touching each other, we assume that the disks are close to each other such that the hyperbolic distance between them is d = 0.02. In all cases the hyperbolic centers of the three fixed disks are z1 =  th((r1  d)/2), z2 = th((r2 + 2d)/2), and z3 = th((r3 + 2r2 + 3d)/2). The hyperbolic center z4 of the moving disk is on the red curve shown in the figure. The observed results are summarized in the second row of Figure 6. MOBILE DISKS IN HYPERBOLIC SPACE 15 5. Minimizing Capacity: Optimization under Free Mobility In this section we consider a series of experiments, where some disks are given fixed positions but the others are free to move within constraints. The constraints can restrict the admissible configurations to specific regions. In the most general case, the only constraint is that the disks should not overlap. In all simulations it is assumed that the disks have a minimal separation   > 0. In those cases where the disks touch, that is,   = 0, only hp-FEM results are reported. 5.1. Three fixed disks. One freely moving disk. Consider three hyperbolic disks with equal hyperbolic radii = 0.2, and whose centers are at 0.5e2(k 1) i/3, k = 1, 2, 3. We consider a fourth hyperbolic disk whose hyperbolic radius is r and its hyperbolic center is z = x + iy such that the four disks are non-overlapping. Let a function u(x, y) be defined by u(x, y) = cap(B, E), where E is the union of the four disks. The level curves of the function u(x, y) for six cases of r are given in Figure 7. Notice, that the locations of the local minima depend on the chosen radius r of the free disk. Due to symmetry, there is a local minimum at the origin in every case. The results suggest that there exists a critical radius rc such that the global minimum is found at the origin for all sufficiently large r, that is, r > rc, but next to one of the fixed disks for r < rc. The interior-point method is guaranteed to converge to one of the local minima, and therefore for all r a local minimum may be attained when the mobile disk is centered at the origin. 5.2. One fixed disk. Two moving disks on a circle. Let us next consider three disks D1, D2, D3 with equal hyperbolic radii r = 0.3. The centers of these three disks are placed on the circle |z| = 0.5. We assume that the disk D1 is fixed with center on the positive real line, D2 is in the upper-half plane and D3 is in the lower-half plane. Starting when the three disks are touching each others (see Figure 8 (left)), these disks start moving away from each other such that the hyperbolic distance d between the hyperbolic centers of D1 and D2 is the same as for D1 and D3. When all these disks are touching each other, d = 2r. The maximum value dmax of d is obtained when the the disks D2 and D3 are touching each other (see Figure 8 (middle)). The values of the capacity as a function of d are shown in Figure 8 (right) where the values of the capacity for 2r < d < dmax are computed by the BIE method and; for d = 2r and d = dmax by the FEM. The minimal capacity is found when d = 2r and the maximal when the centers of the three disks form an equilateral triangle. 5.3. One fixed disk. Three moving disks on a circle. Staying on the circle |z| = 0.5 we consider four disks with centers on the circle and hyperbolic radii 3/30, 5/30, 7/30, and 9/30. Without any loss of generality, we will assume that the disk with hyperbolic radius 9/30 is fixed with its center on the positive real line at the point 0.5. Then, we search for the positions of the other three disks that minimize the capacity. The initial positions of these three disks are assumed to be 0.5e2k i/4 for k = 1, 2, 3. For the optimized positions, we have obtained six positions, with three different values of the capacity due to symmetry (see Figure 9). For the disks in the first column in Figure 9, the capacity is 4.6269. The capacity is 4.6193 for the second column and 4.6621 for the third column. 16 H. HAKULA, M. NASSER, AND M. VUORINEN (a) r = 0.6 (b) r = 0.5 (c) r = 0.4 (d) r = 0.3 (e) r = 0.2 (f) r = 0.1 Figure 7. The level curves of the capacity u(x, y) of a constellation of four disks as a function of the center z = x + iy of the fourth disk. Three hyperbolic disks with equal hyperbolic radii = 0.2 are at fixed locations, whereas the fourth one with a given radius r is free to move (the mobile fourth disk is not shown). The number of local minima depends on the radius of the fourth disk. 5.4. One fixed disk. Three moving disks. Finally, we consider four disks with hyperbolic radii 3/30, 5/30, 7/30, and 9/30. This time, we will assume that the disk with hyperbolic radius 9/30 is fixed with its center at the origin. The task is to find the positions of the three free disks that minimize the capacity where the initial positions of the three disks are assumed to be 0.5e2k i/3 for k = 0, 1, 2. For the optimized positions, we have obtained two configurations, as shown in Figure 10, with the capacity 4.2322 which is the global minimum. If we assume that the three disks with hyperbolic radii 5/30, 7/30, and 9/30 have fixed positions as in Figure 10 (left), and the small disk with hyperbolic radius 3/30 is moving. Assume that the center of the small disk is z = x + iy such that the four disks are non- overlapping. Let a function u(x, y) be defined by u(x, y) = cap(B, E), where E is the union of the four disks. The level curves of the function u(x, y) are given in Figure 10 (right). As we can see from the figure, the capacity has three local minima and the capacity for the position in Figure 10 (left) is the global minimum. This experiment has been repeated multiple times with different initial starting positions for the free disks and every one one of the local minima has been observed. MOBILE DISKS IN HYPERBOLIC SPACE 17 Figure 8. Three disks with equal hyperbolic radius = 0.3 on the circle |z| = 0.5. One disk is fixed on the positive real line and the other two move symmetrically on upper- and lower-half planes, respectively. The left and middle figures illustrate the minimal dmin and maximal dmax values of the hyperbolic distance d between the hyperbolic centers of the disk on the real line and the disk on the upper-half plane. The right figure shows the capacity for the range between these extreme valued dmin  d  dmax. Figure 9. Four disks with hyperbolic radii 3/30, 5/30, 7/30, and 9/30, and with centers on the circle |z| = 0.5. Representative configurations of the optimised cases. 5.4.1. On Computational Costs. Naturally, the optimisation problems are the computationally most expensive ones of all our numerical experiments. In Table 5 performance data on the four disks free mobility problem is presented. Comparison of the two methods is only qualitative, since both underlying hardware and the interior-point implementations are different. However, some conclusions can be derived. In all cases the interior-point tolerance is the same,   = 10 6, and within the hp-FEM simulations, meshing is performed with the same discretization control in every evaluation. For optimal performance, the individual solutions must be accurate enough so that the error induced by numerical approximation of the gradients and Hessians is balanced with other sources of error. For the hp-FEM it appears that the same mesh with p = 4 is not adequate in comparison with the one at p = 8. Even though the time spent in one individual 18 H. HAKULA, M. NASSER, AND M. VUORINEN Figure 10. Four disks with hyperbolic radii 3/30, 5/30, 7/30, and 9/30. The three largest disks have fixed positions, and the smallest one, centered at z = x + iy, is free to move. The level curves of u(x, y) = cap(B, E), where E is the union of the four disks, indicate three local minima. The two configurations on the left have converged to the global minimum. Table 5. Solution times for the minimization process when one disk is fixed and three disks are mobile. Number of steps is number of iterations in the interior- point algorithm. Number of evaluations is the total number of solves performed during the minimization. Method Discretization Time Number of steps Number of evaluations BIE n = 24 472.9 151 1455 n = 27 85.6 24 192 n = 210 150.7 24 192 hp-FEM p = 4 39600 202 39568 p = 6 11100 37 7494 p = 8 9100 20 4150 iteration step is doubled, the overall time for p = 8 is significantly lower. At every evaluation the number of degrees of freedom is roughly 13000 (initial configuration: 13542, and final: 12589). Similarly, for BIE the performance at n = 27 is superior to that at n = 24. The two implementations have very different requirements per iteration step. Observe that the number of iteration steps is comparable, yet the number of evaluations is not. The average time for one evaluation in BIE is four to five times faster than one evaluation in hp-FEM. Matlab and Mathematica results have been computed on modern Intel and Apple Silicon computers, respectively. 5.5. Hyperbolic area lower bound. Finally, we compute the capacity of a constellation of disjoint hyperbolic disks and compare the computed values with the Hyperbolic area lower bound [9]. Let Er be the union of m disjoint hyperbolic disks with equal hyperbolic radii r such the hyperbolic distance between any two disks is 0.02 (see Figure 11 for r = 0.5 and m = 2, 3, 4). For m = 4, we consider two cases (as shown in Figure 11) where the centers of MOBILE DISKS IN HYPERBOLIC SPACE 19 Figure 11. The four types of condensers (B, Er) for the hyperbolic radius r = 0.5. From left: m = 2, m = 3, m = 4 (Case I), m = 4 (Case II). the disks in Case I are on the real and imaginary axes. In Case II, the centers are on the rays ei  for   = 0,  /3, 2 /3, 4 /3. The hyperbolic area of these m disks is 4m  sh2(r/2). Consider the hyperbolic disk B (0, M) whose hyperbolic area is the same as the hyperbolic area of the m disks, then M = 2 arsh \u0000 m sh(r/2)\u0001 . Then L(r) = cap(B, B (0, M)) is the hyperbolic area lower bound of cap(B, Er). In view of (2.3), we have L(r) = cap(B, B (0, M)) = cap(B, B2(0, th(M/2))) = 2  log cth(M/2). The BIE method is then used to compute cap(B, Er) for several values of r with 0.02  r  2. Our computed minimum value of the capacity can be considered a lower bound of the capacity of the constellation of m disjoint hyperbolic disks. We compare the computed value with the hyperbolic area lower bound by defining Lr = cap(B, Er)  L(r) L(r) . The graph of Lr is shown in Figure 12 for 0.02  r  2 and m = 2, 3, 4. As r  it appears that the improvement tends to zero. This is a consequence of the nature of hyperbolic geometry. With one disk fixed in the centre the other three will have ever smaller contributions to the capacity since their Euclidean areas tend to zero as in Figure 2 (right). It is an indication of the complexity of the problem that the graphs in Figure 12 do not reveal any simple connection between the number of the disks and the minimal capacity. 6. Conclusions We study lower bounds for the conformal capacity of a constellation of disjoint hyperbolic disks Ej  B2, j = 1, ..., m, using a novel idea: instead of using a symmetrization transforma- tion, which usually leads to fusion of the disjoint disks, we are looking for a lower bound in terms of another constellation which yields a minimal value. The traditional symmetrization transformation [31], [3], [8], is now replaced by free mobility of individual disks with the con- straint that the hyperbolic radii of the disks are invariant and the disks are non-overlapping. 20 H. HAKULA, M. NASSER, AND M. VUORINEN Figure 12. The ratio Lr for the four types of condensers (B, Er). As r  the improvement relative to the lower bound L(r) tends to zero as expected. Left: For m = 2, m = 3, the improvements are very similar. Right: In line with our experiments above, the Case II is indeed optimal, and gives us an improved lower bound. In this process, due to the conformal invariance, the conformal capacity of each disk stays in- variant, whereas the capacity of the whole constellation may significantly vary. Moreover, the hyperbolic area of the constellation is also constant. The optimization methods we used produced (locally) minimal constellations such that the disks group together, as closely as possible. This coalescing is reminiscent of the behavior of some animal colonies in cold weather conditions for the purpose of heat flow minimization. Mathematical methods are not available for analytic treatment of the problems, but we are convinced that there is a strong connection with combinatorial geometry, topics like packing and covering problems. Such problems often have many local minima [6, p. 157]. We carried out numerical simulations using two different methods, the BIE and hp-FEM methods and the close agreement of the two computational methods confirmed the results. Because of the complexity of the problem we studied various subproblems where disk centers satisfied constraints such that the centers are on the interval ( 1, 1) or at the same distance from the origin. In both cases we observed the grouping phenomenon (cf. Figure 1) and, moreover, noticed that permutation of disks has influence on the capacity if the radii are different. Because the hyperbolic area of a constellation is a constant, it is now clear that the hyperbolic area alone does not define the constellation capacity. This observation led us to compare our computed lower bound to Gehring s sharp lower bound given in terms of hyperbolic area. The conclusion was that we obtained in some cases approximately 10% improvement when m = 4. The numerical agreement of the BIE and hp-FEM methods was very good, typically ten decimal places or better, and the expected exponential convergence was observed, see Figure 5. The performance of the BIE method was significantly faster than the hp-FEM method when it comes to computational time and flexilibity to modify the code to new situations. This is probably due to the heavy data structure of the hp-FEM method due to hierarchial triangulation refinement process of the method. MOBILE DISKS IN HYPERBOLIC SPACE 21 A vast territory of open problems remains. First, it would be interesting to study whether some kind heuristic methods would lead to \"close to extremal\" constellations, to be used as initial steps of the minimization. Such a method could be based on some computationally cheaper object function than the capacity itself: for instance, first, the maximization of the number of the mutual contact points of the constellation. Second, the case of m > 5 disks of equal radii seems to be completely open. Perhaps in this case the number of locally minimal constellations grows exponentially as a function of m. Third, one could study constellations of other types of geometric figures like hyperbolic triangles. References 1. K. E. Atkinson, The Numerical Solution of Integral Equations of the Second Kind, Cambridge University Press, Cambridge, 1997. 2. I. Babu ka and B. Guo, Regularity of the solutions of elliptic problems with piecewise analytical data, parts I and II, SIAM J. Math. Anal., 19, (1988), 172 203 and 20, (1989), pp. 763 781. 3. A. Baernstein, Symmetrization in Analysis. With David Drasin and Richard S. Laugesen. With a foreword by Walter Hayman. Cambridge University Press, Cambridge, 2019. 4. A. F. Beardon, The Geometry of Discrete Groups, Springer-Verlag, New York, 1983. 5. A. F. Beardon and D. Minda, The hyperbolic metric and geometric function theory. Quasiconformal mappings and their applications, 9 56, Narosa, New Delhi, 2007. 6. M. Berger, Geometry revealed. A Jacob s ladder to modern higher geometry. Translated from the French by Lester Senechal. Springer, Heidelberg, 2010. 7. S.V. Borodachov, D.P. Hardin, and E. B. Saff, Discrete Energy on Rectifiable Sets, Springer, New York, 2019. 8. V.N. Dubinin, Condenser Capacities and Symmetrization in Geometric Function Theory, Birkh user, 2014. 9. F.W. Gehring, Inequalities for condensers, conformal capacity, and extremal lengths. Michigan Math. J. 18 (1971), 1 20. 10. F. W. Gehring, G. J. Martin and B. Palka, An Introduction to the Theory of Higher-Dimensional Quasiconformal Mappings, American Mathematical Society, Providence, RI, 2017. 11. V.M. Goldshtein and Yu. G. Reshetnyak, Quasiconformal Mappings and Sobolev Spaces. Translated and revised from the 1983 Russian original. Translated by O. Korneeva. Kluwer Academic Publishers Group, Dordrecht, 1990. 12. L. Greengard and Z. Gimbutas, FMMLIB2D: A MATLAB toolbox for fast multipole method in two dimensions, version 1.2. 2019, www.cims.nyu.edu/cmcl/fmm2dlib/fmm2dlib.html. Accessed 6 Nov 2020. 13. H. Hakula, M. M.S. Nasser, and M. Vuorinen, Conformal capacity and polycircular domains, J. Comput. Appl. Math. 420 (2023), 114802. 14. H. Hakula, M. Neilan, and J. Ovall, A Posteriori Estimates Using Auxiliary Subspace Techniques, J. Sci. Comput. 72 no. 1 (2017), pp. 97 127. 15. H. Hakula, A. Rasila, and M. Vuorinen, On moduli of rings and quadrilaterals: algorithms and experiments. SIAM J. Sci. Comput. 33 (2011), no. 1, 279 302. 16. P. Hariri, R. Kl n, and M. Vuorinen, Conformally Invariant Metrics and Quasiconformal Mappings, Springer, Berlin, 2020. 17. J. Heinonen, T. Kilpel inen, and O. Martio, Nonlinear Potential Theory of Degenerate Elliptic Equations, Dover Publications, New York, 2006. 18. H. Kober, Dictionary of Conformal Representations, Dover Publications, New York, 1957. 19. A.J. Koll r, M. Fitzpatrick, and A.A. Houck, Hyperbolic lattices in circuit quantum electrodynam- ics. Nature 571, 45 50 (2019). 22 H. HAKULA, M. NASSER, AND M. VUORINEN 20. R. Kusner, W. Kusner, J.C. Lagarias, and S. Shlosman, Configuration spaces of equal spheres touching a given sphere: the twelve spheres problem. New trends in intuitive geometry, 219 277, Bolyai Soc. Math. Stud., 27, J nos Bolyai Math. Soc., Budapest, 2018. 21. J. C. Lagarias, C. L. Mallows, and A. R. Wilks, Beyond the Descartes Circle Theorem. Amer. Math. Monthly, (2002) 109:4, 338 361. 22. P.M. Lenggenhager, A. Stegmaier, L.K. Upreti, et al., Simulating hyperbolic space on a circuit board. Nat Commun 13, 4373 (2022). 23. R.H. Lewis and S. Bridgett, Conic tangency equations and Apollonius problems in biochemistry and pharmacology. (English summary) Math. Comput. Simulation 61 (2003), no. 2, 101 114. 24. MATLAB, 2022a. 9.12 (R2022a), Natick, Massachusetts: The MathWorks Inc. 25. M. M.S. Nasser, Fast solution of boundary integral equations with the generalized Neumann kernel.- Electron. Trans. Numer. Anal. 44 (2015), 189 229. 26. M. M.S. Nasser, O. Rainio, and M. Vuorinen, Condenser capacity and hyperbolic diameter. J. Math. Anal. Appl. 508(2022), 125870. 27. M. M.S. Nasser, O. Rainio, and M. Vuorinen, Condenser capacity and hyperbolic perimeter. Comput. Math. Appl. 105(2022), 54 74. 28. M. M.S. Nasser and M. Vuorinen, Numerical computation of the capacity of generalized condensers. J. Comput. Appl. Math. 377 (2020) 112865. 29. M. M.S. Nasser and M. Vuorinen, Isoperimetric properties of condenser capacity. J. Math. Anal. Appl. 499 (2021), 125050. 30. J. Nocedal and S. Wright, Numerical Optimization, Springer New York, NY, 2006. 31. G. P lya and G. Szeg , Isoperimetric Inequalities in Mathematical Physics. Princeton Univ. Press, 1952. 32. Th. Ransford, Potential Theory in the Complex Plane, Cambridge University Press, Cambridge, 1995. 33. Ch. Schwab, p- and hp-Finite Element Methods, Oxford University Press, 1998. 34. A. Yu. Solynin, Problems on the loss of heat: herd instinct versus individual feelings, St. Petersburg Math. J. 33 (2022), 739 775, Algebra i Analiz, tom 33 (2021), nomer 5. 35. L. N. Trefethen and J. A.C. Weideman, The exponentially convergent trapezoidal rule. SIAM Rev. 56 (2014), 385 458. 36. M. Tsuji, Potential Theory in Modern Function Theory. Chelsea Publishing Co., New York, 1975. 37. G. Wang, M. Vuorinen, and X. Zhang, On cyclic quadrilaterals in Euclidean and hyperbolic geometries. Publ. Math. Debrecen 99/1-2 (2021), 123 140. 38. R. Wegmann and M. M.S. Nasser, The Riemann-Hilbert problem and the generalized Neumann kernel on multiply connected regions. J. Comput. Appl. Math. 214 (2008), 36 57. 39. K. Williams, Tree swallows huddle in snow, https://www.fws.gov/media/tree-swallows-huddle-snow. May 12, 2011. 40. Wolfram Research, Inc., Mathematica, Version 13.2.1, Champaign, IL, 2023. Aalto University, Department of Mathematics and Systems Analysis, P.O. Box 11100, FI- 00076 Aalto, FINLAND Email address: harri.hakula@aalto.fi Department of Mathematics, Statistics, and Physics, Wichita State University, Wichita, KS 67260-0033, USA Email address: mms.nasser@wichita.edu Department of Mathematics and Statistics, University of Turku, FI-20014 Turku, Finland Email address: vuorinen@utu.fi"
    }
]