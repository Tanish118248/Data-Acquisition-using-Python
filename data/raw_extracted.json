[
    {
        "file_name": "2001.08361.pdf",
        "title": "2001.08361",
        "year": "2020",
        "full_text": "Scaling Laws for Neural Language Models\nJared Kaplan ∗\nJohns Hopkins University, OpenAI\njaredk@jhu.edu\nSam McCandlish∗\nOpenAI\nsam@openai.com\nTom Henighan\nOpenAI\nhenighan@openai.com\nTom B. Brown\nOpenAI\ntom@openai.com\nBenjamin Chess\nOpenAI\nbchess@openai.com\nRewon Child\nOpenAI\nrewon@openai.com\nScott Gray\nOpenAI\nscott@openai.com\nAlec Radford\nOpenAI\nalec@openai.com\nJeffrey Wu\nOpenAI\njeffwu@openai.com\nDario Amodei\nOpenAI\ndamodei@openai.com\nAbstract\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\nused for training, with some trends spanning more than seven orders of magnitude. Other\narchitectural details such as network width or depth have minimal effects within a wide\nrange. Simple equations govern the dependence of overﬁtting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to determine the\noptimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-\nefﬁcient, such that optimally compute-efﬁcient training involves training very large models\non a relatively modest amount of data and stopping signiﬁcantly before convergence.\n∗Equal contribution.\nContributions:\nJared Kaplan and Sam McCandlish led the research.\nTom Henighan contributed the LSTM ex-\nperiments.\nTom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\nguidance throughout the project.\narXiv:2001.08361v1  [cs.LG]  23 Jan 2020\nContents\n1\nIntroduction\n2\n2\nBackground and Methods\n6\n3\nEmpirical Results and Basic Power Laws\n7\n4\nCharting the Inﬁnite Data Limit and Overﬁtting\n10\n5\nScaling Laws with Model Size and Training Time\n12\n6\nOptimal Allocation of the Compute Budget\n14\n7\nRelated Work\n18\n8\nDiscussion\n18\nAppendices\n20\nA Summary of Power Laws\n20\nB\nEmpirical Model of Compute-Efﬁcient Frontier\n20\nC Caveats\n22\nD Supplemental Figures\n23\n1\nIntroduction\nLanguage provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-\ning tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of\ndata for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-\nguage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching\nhuman-level performance on many speciﬁc tasks [WPN+19], including the composition of coherent multi-\nparagraph prompted text samples [RWC+19].\nOne might expect language modeling performance to depend on model architecture, the size of neural models,\nthe computing power used to train them, and the data available for this training process. In this work we will\nempirically investigate the dependence of language modeling loss on all of these factors, focusing on the\nTransformer architecture [VSP+17, LSP+18]. The high ceiling and low ﬂoor for performance on language\ntasks allows us to study trends over more than seven orders of magnitude in scale.\nThroughout we will observe precise power-law scalings for performance as a function of training time, con-\ntext length, dataset size, model size, and compute budget.\n1.1\nSummary\nOur key ﬁndings for Transformer language models are are as follows:\n2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the\npurely empirical data.\n2\nDataset Size \ntokens\nParameters \nnon-embedding\nCompute \nPF-days, non-embedding\nTest Loss\nFigure 1\nLanguage modeling performance improves smoothly as we increase the model size, datasetset\nsize, and amount of compute2 used for training. For optimal performance all three factors must be scaled\nup in tandem. Empirical performance has a power-law relationship with each individual factor when not\nbottlenecked by the other two.\nPerformance depends strongly on scale, weakly on model shape:\nModel performance depends most\nstrongly on scale, which consists of three factors: the number of model parameters N (excluding embed-\ndings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,\nperformance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section\n3)\nSmooth power laws:\nPerformance has a power-law relationship with each of the three scale factors\nN, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude\n(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance\nmust ﬂatten out eventually before reaching zero loss. (Section 3)\nUniversality of overﬁtting:\nPerformance improves predictably as long as we scale up N and D in tandem,\nbut enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The\nperformance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the\nmodel size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)\nUniversality of training:\nTraining curves follow predictable power-laws whose parameters are roughly\nindependent of the model size. By extrapolating the early part of a training curve, we can roughly predict the\nloss that would be achieved if we trained for much longer. (Section 5)\nTransfer improves with test performance:\nWhen we evaluate models on text with a different distribution\nthan they were trained on, the results are strongly correlated to those on the training validation set with\na roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant\npenalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)\nSample efﬁciency:\nLarge models are more sample-efﬁcient than small models, reaching the same level of\nperformance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).\nConvergence is inefﬁcient:\nWhen working within a ﬁxed compute budget C but without any other restric-\ntions on the model size N or available data D, we attain optimal performance by training very large models\nand stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would\ntherefore be far more sample efﬁcient than one might expect based on training small models to convergence,\nwith data requirements growing very slowly as D ∼C0.27 with training compute. (Section 6)\nOptimal batch size:\nThe ideal batch size for training these models is roughly a power of the loss only,\nand continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million\ntokens at convergence for the largest models we can train. (Section 5.1)\nTaken together, these results show that language modeling performance improves smoothly and predictably\nas we appropriately scale up model size, data, and compute. We expect that larger language models will\nperform better and be more sample efﬁcient than current models.\n3\nLarger models require fewer samples \nto reach the same performance\n10\n8\n6\n4\nThe optimal model size grows smoothly \nwith the loss target and compute budget\nLine color indicates \nnumber of parameters\n107\n109\n1011\nTokens Processed\nCompute (PF-days)\n10-9\n10-6\n10-3\n100\nTest Loss\nCompute-eﬃcient \ntraining stops far \nshort of convergence\n103\n109\n106\n103 Params\n109 Params\n10\n8\n6\n4\nFigure 2\nWe show a series of language model training runs, with models ranging in size from 103 to 109\nparameters (excluding embeddings).\n100x Batch Size\n<10x Serial Steps\n>1,000,000x Model Size\nData requirements \ngrow relatively slowly\nOptimal model size \nincreases very quickly\nMinimum serial steps \nincreases negligibly\nFigure 3\nAs more compute becomes available, we can choose how much to allocate towards training larger\nmodels, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in\ncompute. For optimally compute-efﬁcient training, most of the increase should go towards increased model\nsize. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to\nincrease parallelism through larger batch sizes, with only a very small increase in serial training time required.\n1.2\nSummary of Scaling Laws\nThe test loss of a Transformer trained to autoregressively model language can be predicted using a power-law\nwhen performance is limited by only either the number of non-embedding parameters N, the dataset size D,\nor the optimally allocated compute budget Cmin (see Figure 1):\n1. For models with a limited number of parameters, trained to convergence on sufﬁciently large\ndatasets:\nL(N) = (Nc/N)αN ; αN ∼0.076,\nNc ∼8.8 × 1013 (non-embedding parameters)\n(1.1)\n2. For large models trained with a limited dataset with early stopping:\nL(D) = (Dc/D)αD ; αD ∼0.095,\nDc ∼5.4 × 1013 (tokens)\n(1.2)\n3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized\nmodel, and a sufﬁciently small batch size (making optimal3 use of compute):\nL(Cmin) =\n\u0000Cmin\nc\n/Cmin\n\u0001αmin\nC\n; αmin\nC\n∼0.050,\nCmin\nc\n∼3.1 × 108 (PF-days)\n(1.3)\n3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch\nsize, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).\n4\n107\n108\n109\n1010\nTokens in Dataset\n2.5\n3.0\n3.5\n4.0\n4.5\nLoss\nLoss vs Model and Dataset Size\nParams\n708M\n302M\n85M\n3M\n25M\n393.2K\n104\n105\nEstimated Smin\n2.4\n2.8\n3.2\n3.6\n4.0\n4.4\nLoss\nLoss vs Model Size and Training Steps\n106\n107\n108\nParameters (non-embed)\nFigure 4\nLeft: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model\nsize N according to Equation (1.5). Right: After an initial transient period, learning curves for all model\nsizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when\ntraining at large batch size (details in Section 5.1).\nThese relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two\norders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters\n(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2\ntraining set [RWC+19]. The power laws αN, αD, αmin\nC\nspecify the degree of performance improvement\nexpected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that\nis smaller by a factor 2−αN = 0.95. The precise numerical values of Nc, Cmin\nc\n, and Dc depend on the\nvocabulary size and tokenization and hence do not have a fundamental meaning.\nThe critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also\nroughly obeys a power law in L:\nBcrit (L) =\nB∗\nL1/αB ,\nB∗∼2 · 108 tokens, αB ∼0.21\n(1.4)\nEquation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset\nsize sublinearly according to D ∝N\nαN\nαD ∼N 0.74. In fact, we ﬁnd that there is a single equation combining\n(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:\nL(N, D) =\n\"\u0012Nc\nN\n\u0013 αN\nαD + Dc\nD\n#αD\n(1.5)\nwith ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the\ntrained log-likelihood for other generative modeling tasks.\nWhen training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after\nan initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)\nL(N, S) =\n\u0012Nc\nN\n\u0013αN\n+\n\u0012\nSc\nSmin(S)\n\u0013αS\n(1.6)\nwhere Sc ≈2.1 × 103 and αS ≈0.76, and Smin(S) is the minimum possible number of optimization steps\n(parameter updates) estimated using Equation (5.4).\nWhen training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the\nprediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size\nD should grow as\nN ∝Cαmin\nC\n/αN ,\nB ∝Cαmin\nC\n/αB,\nS ∝Cαmin\nC\n/αS,\nD = B · S\n(1.7)\nwith\nαmin\nC\n= 1/ (1/αS + 1/αB + 1/αN)\n(1.8)\nwhich closely matches the empirically optimal results N ∝C0.73\nmin , B ∝C0.24\nmin , and S ∝C0.03\nmin . As the\ncomputational budget C increases, it should be spent primarily on larger models, without dramatic increases\nin training time or dataset size (see Figure 3). This also implies that as models grow larger, they become\nincreasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would\n5\nbe maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total\ncompute as a power law (see Equation (1.3)).\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their\nimplications for training time, and a breakdown of our results per token. We also make some brief compar-\nisons to LSTMs and recurrent Transformers [DGV+18].\n1.3\nNotation\nWe use the following notation:\n• L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in\nsome cases we report the loss for speciﬁc tokens within the context.\n• N – the number of model parameters, excluding all vocabulary and positional embeddings\n• C ≈6NBS – an estimate of the total non-embedding training compute, where B is the batch size,\nand S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,\nwhere one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.\n• D – the dataset size in tokens\n• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the\ncritical batch size provides a roughly optimal compromise between time and compute efﬁciency.\n• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of\nthe loss. This is the training compute that would be used if the model were trained at a batch size\nmuch less than the critical batch size.\n• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.\nThis is also the number of training steps that would be used if the model were trained at a batch size\nmuch greater than the critical batch size.\n• αX – power-law exponents for the scaling of the loss as L(X) ∝1/XαX where X can be any of\nN, D, C, S, B, Cmin.\n2\nBackground and Methods\nWe train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized\nusing byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-\nsive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal\nperformance metric. We record the loss on the WebText2 test distribution and on a selection of other text\ndistributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though\nwe also train LSTM models and Universal Transformers [DGV+18] for comparison.\n2.1\nParameter and Compute Scaling of Transformers\nWe parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-\nmension of the residual stream), dﬀ(dimension of the intermediate feed-forward layer), dattn (dimension of\nthe attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input\ncontext, with nctx = 1024 except where otherwise noted.\nWe use N to denote the model size, which we deﬁne as the number of non-embedding parameters\nN ≈2dmodelnlayer (2dattn + dﬀ)\n= 12nlayerd2\nmodel\nwith the standard\ndattn = dﬀ/4 = dmodel\n(2.1)\nwhere we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters\nin an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include\nthese when discussing the ‘model size’ N; we will see that this produces signiﬁcantly cleaner scaling laws.\nEvaluating a forward pass of the Transformer involves roughly\nCforward ≈2N + 2nlayernctxdmodel\n(2.2)\nadd-multiply operations, where the factor of two comes from the multiply-accumulate operation used in\nmatrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.\n6\nOperation\nParameters\nFLOPs per Token\nEmbed\n(nvocab + nctx) dmodel\n4dmodel\nAttention: QKV\nnlayerdmodel3dattn\n2nlayerdmodel3dattn\nAttention: Mask\n—\n2nlayernctxdattn\nAttention: Project\nnlayerdattndmodel\n2nlayerdattndembd\nFeedforward\nnlayer2dmodeldﬀ\n2nlayer2dmodeldﬀ\nDe-embed\n—\n2dmodelnvocab\nTotal (Non-Embedding)\nN = 2dmodelnlayer (2dattn + dﬀ)\nCforward = 2N + 2nlayernctxdattn\nTable 1\nParameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading\nterms such as nonlinearities, biases, and layer normalization are omitted.\nFor contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a\nrelatively small fraction of the total compute. Since we primarily study models where dmodel ≫nctx/12,\nwe do not include context-dependent terms in our training compute estimate. Accounting for the backwards\npass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding\ncompute as C ≈6N ﬂoating point operators per training token.\n2.2\nTraining Procedures\nUnless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with\na batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than\n1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and\nschedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of\nlearning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate\nschedule with a 3000 step linear warmup followed by a cosine decay to zero.\n2.3\nDatasets\nWe train our models on an extended version of the WebText dataset described in [RWC+19]. The original\nWebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at\nleast 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January\nto October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether\npeople found the link interesting or useful. The text of the new links was extracted with the Newspaper3k\npython library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010\nwords (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields\n2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly-\nprepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection\nof publicly-available Internet Books.\n3\nEmpirical Results and Basic Power Laws\nTo characterize language model scaling we train a wide variety of models, varying a number of factors\nincluding:\n• Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)\n• Dataset size (ranging from 22 million to 23 billion tokens)\n• Shape (including depth, width, attention heads, and feed-forward dimension)\n• Context length (1024 for most runs, though we also experiment with shorter contexts)\n• Batch size (219 for most runs, but we also vary it to measure the critical batch size)\n7\nFeed-Forward Ratio (dff / dmodel) \n50M Parameters\nAspect Ratio (dmodel / nlayer)\nAttention Head Dimension (dmodel / nhead) \n25M Parameters\n10%\n8%\n6%\n4%\n2%\n0%\nLoss Increase\nA wide range of architectures \nachieve similar performance\n22% additional compute \ncompensates for 1% loss increase\nFigure 5\nPerformance depends very mildly on model shape when the total number of non-embedding\nparameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences\nin parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can\nvary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a\nloss within 3% of the (48, 1600) model used in [RWC+19].\n106\n107\n108\n109\nParameters (with embedding)\n2\n3\n4\n5\n6\n7\nTest Loss\n0 Layer\n1 Layer\n2 Layers\n3 Layers\n6 Layers\n> 6 Layers\n103\n104\n105\n106\n107\n108\n109\nParameters (non-embedding)\n2\n3\n4\n5\n6\n7\nTest Loss\n1 Layer\n2 Layers\n3 Layers\n6 Layers\n> 6 Layers\nFigure 6\nLeft: When we include embedding parameters, performance appears to depend strongly on the\nnumber of layers in addition to the number of parameters. Right: When we exclude embedding parameters,\nthe performance of models with different depths converge to a single trend. Only models with fewer than 2\nlayers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.\nIn this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to\nlater sections.\n3.1\nApproximate Transformer Shape and Hyperparameter Independence\nTransformer performance depends very weakly on the shape parameters nlayer, nheads, and dﬀwhen we hold\nthe total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed\nsize while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,\nwe simultaneously varied dmodel while keeping N ≈12nlayerd2\nmodel ﬁxed. Similarly, to vary dﬀat ﬁxed\nmodel size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table\n1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower\nmodels, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.\n3.2\nPerformance with Non-Embedding Parameter Count N\nIn Figure 6 we display the performance of a wide variety of models, ranging from small models with shape\n(nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through\n(207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-\nﬁtting (except possibly for the very largest models).\nAs shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter count N, which can be ﬁt to the\nﬁrst term of Equation (1.5), so that\nL(N) ≈\n\u0012Nc\nN\n\u0013αN\n(3.1)\n8\nLSTM plateaus after <100 tokens \nTransformer improves through the whole context\n2M\n200M\n3M\n300M\n5\n4\n3\n2\n6\nToken Index in Context\n103\n102\n101\nTransformers asymptotically outperform LSTMs \ndue to improved use of long contexts\n3.6\n4.2\n3.0\n2.4\n4.8\n5.4\n105\n108\n106\n107\n109\nParameters (non-embedding)\nTransformers\nLSTMs\n1 Layer\n2 Layers\n4 Layers\nTest Loss\nPer-token \nTest Loss\nParameters:\n400K\n400K\nFigure 7\nTo observe these trends it is crucial to study performance as a function of N; if we instead use the total\nparameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This\nsuggests that the embedding matrix can be made smaller without impacting performance, as has been seen in\nrecent work [LCG+19].\nAlthough these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets\nis also a power-law in N with nearly identical power, as shown in Figure 8.\n3.2.1\nComparing to LSTMs and Universal Transformers\nIn Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter\ncount N. The LSTMs were trained with the same dataset and context length. We see from these ﬁgures\nthat the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match\nthe Transformer performance for later tokens. We present power-law relationships between performance and\ncontext position Appendix D.5, where increasingly large powers for larger models suggest improved ability\nto quickly recognize patterns.\nWe also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure\n17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the\ncost of additional compute per-parameter.\n3.2.2\nGeneralization Among Data Distributions\nWe have also tested our models on a set of additional text data distributions. The test loss on these datasets\nas a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2\ndataset. We see that the loss on these other data distributions improves smoothly with model size, in direct\nparallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the\nin-distribution validation loss, and does not depend on the duration of training or proximity to convergence.\nWe also observe no dependence on model depth (see Appendix D.8).\n3.3\nPerformance with Dataset Size and Compute\nWe display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute\nC in Figure 1.\nFor the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on ﬁxed subsets of the WebText2\ndataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\nﬁt with simple power-law\nL(D) ≈\n\u0012Dc\nD\n\u0013αD\n(3.2)\nin the dataset size. The data and ﬁt appear in Figure 1.\nThe total amount of non-embedding compute used during training can be estimated as C = 6NBS, where\nB is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and\nbackward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model\n9\n104\n105\n106\n107\n108\n109\nParameters (non-embedding)\n3\n4\n5\n6\n7\nTest Loss\nWebText2 (Test)\nInternet Books\nBooks\nWikipedia\nCommon Crawl\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nTest Loss on Training Distribution\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nLoss on Other Distribution\nBooks during training\nWikipedia during training\nBooks at convergence\nWikipedia at convergence\nFigure 8\nLeft: Generalization performance to other data distributions improves smoothly with model size,\nwith only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-\nalization performance depends only on training distribution performance, and not on the phase of training.\nWe compare generalization of converged models (points) to that of a single large model (dashed curves) as it\ntrains.\nwith the best performance on step S =\nC\n6BS . Note that in these results the batch size B remains ﬁxed for\nall models, which means that these empirical results are not truly optimal. We will account for this in later\nsections using an adjusted Cmin to produce cleaner trends.\nThe result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with\nL(C) ≈\n\u0012Cc\nC\n\u0013αC\n(3.3)\nThe ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.\nWe will study the optimal allocation of compute more closely later on. The data strongly suggests that sample\nefﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.\n4\nCharting the Inﬁnite Data Limit and Overﬁtting\nIn Section 3 we found a number of basic scaling laws for language modeling performance. Here we will\nstudy the performance of a model of size N trained on a dataset with D tokens while varying N and D\nsimultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling\nlaw of Equation (1.5). This provides guidance on how much data we would need to train models of increasing\nsize while keeping overﬁtting under control.\n4.1\nProposed L(N, D) Equation\nWe have chosen the parameterization (1.5) (repeated here for convenience):\nL(N, D) =\n\"\u0012Nc\nN\n\u0013 αN\nαD + Dc\nD\n#αD\n(4.1)\nusing three principles:\n1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The\nparameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling.\n2. Fixing D and sending N →∞, the overall loss should approach L(D). Conversely, ﬁxing N and\nsending D →∞the loss must approach L(N).\n3. L(N, D) should be analytic at D = ∞, so that it has a series expansion in 1/D with integer powers.\nTheoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.\nOur choice of L(N, D) satisﬁes the ﬁrst requirement because we can rescale Nc, Dc with changes in the\nvocabulary. This also implies that the values of Nc, Dc have no fundamental meaning.\n10\n106\n107\n108\n109\nParams (non-embed)\n2.5\n3.0\n3.5\n4.0\n4.5\nTest Loss\nData Size Bottleneck\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n22.0B\n10\n4\n10\n3\n10\n2\n10\n1\nN\nN/\nD/D\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nL/L(D =\n)\n1\nOverfitting\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n22.0B\nFigure 9\nThe early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N\naccording to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller ﬁxed\nD, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,\nsee Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N\nαN\nαD /D, as predicted in\nequation (4.3). The line is our ﬁt to that equation.\nSince we stop training early when the test loss ceases to improve and optimize all models in the same way, we\nexpect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also\ndo not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,\na model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note\nthat knowledge of L(N) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N, D).\nThe third principle is more speculative. There is a simple and general reason one might expect overﬁtting\nto scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio\nof the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,\nsince we expect to be able to expand the loss about the D →∞limit. However, this argument assumes that\n1/D corrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the\nefﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.\nOur third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar\nsymmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and\nwould require the introduction of an additional parameter.\nIn any case, we will see that our equation for L(N, D) ﬁts the data well, which is the most important justiﬁ-\ncation for our L(N, D) ansatz.\n4.2\nResults\nWe regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer\ndecreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN, αD, Nc, Dc in\nEquation (1.5):\nParameter\nαN\nαD\nNc\nDc\nValue\n0.076\n0.103\n6.4 × 1013\n1.8 × 1013\nTable 2\nFits to L(N, D)\nWe obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of\n1024, to about 2 × 107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.\nPerhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very\nearly in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in\nSection 3, as here we are ﬁtting the full L(N, D) rather than just L(N, ∞) or L(∞, D).\nTo chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but\nthe largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,\nso we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by\n4For example, one might have used L(N, D) =\n\u0002\u0000 Nc\nN\n\u0001αN +\n\u0000 Dc\nD\n\u0001αD\u0003β, but this does not have a 1/D expansion.\n11\n101\n3 × 100\n4 × 100\n6 × 100\nWebText2 Train Loss\n103\n104\n105\n106\nCritical Batch Size (Tokens)\nCritical Batch Size vs. Performance\nEmpirical Bcrit, N = 3M\nEmpirical Bcrit, N = 85M\nBcrit = 2.1 × 108 tokens L\n4.8\nNoise Scale Measurement\nFigure 10\nThe critical batch size Bcrit follows a power law in the loss as performance increase, and does\nnot depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every\n13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly\npredicted by the gradient noise scale, as in [MKAT18].\ndeﬁning\nδL(N, D) ≡L(N, D)\nL(N, ∞) −1\n(4.2)\nand studying it as a function of N, D. In fact, we see empirically that δL depends only a speciﬁc combination\nof N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies\nδL ≈\n \n1 +\n\u0012 N\nNc\n\u0013 αN\nαD Dc\nD\n!αD\n−1\n(4.3)\nNote that at large D this formula also has a series expansion in powers of 1/D.\nWe estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to\navoid overﬁtting when training to within that threshold of convergence we require\nD ≳(5 × 103) N 0.74\n(4.4)\nWith this relation, models smaller than 109 parameters can be trained with minimal overﬁtting on the 22B\ntoken WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this\nrelation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however\nthat this does not typically represent maximally compute-efﬁcient training. We should also emphasize that\nwe have not optimized regularization (eg the dropout probability) while varying dataset and model size.\n5\nScaling Laws with Model Size and Training Time\nIn this section we will demonstrate that a simple scaling law provides a good description for the loss as a\nfunction of model size N and training time. First we will explain how to use the results of [MKAT18] to\ndeﬁne a universal training step Smin, which accounts for the fact that most of our models have not been\ntrained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time\ndependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation\nof training compute between model size and training time, and then conﬁrm that prediction.\n5.1\nAdjustment for Training at Bcrit(L)\nA simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also\n[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit\nthe batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > Bcrit\nincreases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple\n12\nprediction for Bcrit, and that neither depends directly on model size except through the value of the loss that\nhas been attained. These results can be used to predict how training time and compute will vary with the\nbatch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch\nsize B ≈Bcrit. Training at B ≫Bcrit minimizes the number of training steps, while B ≪Bcrit minimizes\nthe use of compute.\nMore speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training\nsteps S and the number of data examples processed E = BS satisfy the simple relation\n\u0012 S\nSmin\n−1\n\u0013 \u0012 E\nEmin\n−1\n\u0013\n= 1\n(5.1)\nwhen training to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach\nL, while Emin is the minimum number of data examples that must be processed.\nWe demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the\ncritical batch size\nBcrit(L) ≡Emin\nSmin\n(5.2)\nwhich is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal\ntime/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.\nIn Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for\ntwo different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So\nthe predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can\nbe ﬁt with a power-law in the loss\nBcrit(L) ≈\nB∗\nL1/αB\n(5.3)\nwhere B∗≈2 × 108 and αB ≈0.21.\nWe have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,\nthe gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not\nknow Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural\nlanguage is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used\na parameterization where Bcrit diverges as L →0.\nWe will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch\nsize B = 219 tokens and the number of training steps while training at B ≫Bcrit. This is simply\nSmin(S) ≡\nS\n1 + Bcrit(L)/B\n(minimum steps, at B ≫Bcrit)\n(5.4)\nfor any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L\nwith a model of size N if we were to train at B ≪Bcrit(L). This is\nCmin(C) ≡\nC\n1 + B/Bcrit(L)\n(minimum compute, at B ≪Bcrit)\n(5.5)\nwhere C = 6NBS estimates the (non-embedding) compute used at batch size B.\n5.2\nResults for L(N, Smin) and Performance with Model Size and Compute\nNow we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the\nloss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training\nruns using Equation (1.6), repeated here for convenience:\nL(N, Smin) =\n\u0012Nc\nN\n\u0013αN\n+\n\u0012 Sc\nSmin\n\u0013αS\n(5.6)\nfor the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt\nto the data with the parameters:\n5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of\nBcrit from Figures 18 and 10 for all our later analyses.\n13\n104\n106\n108\nParameters (non-embedding)\n2\n3\n4\n5\n6\n7\n8\nTest Loss\nPerformance vs Compute Budget\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nPF-dayss\n106\n107\n108\n109\nParameters (non-embedding)\n2.4\n3.0\n3.6\n4.2\n4.8\n5.4\nTest Loss\nPerformance vs Steps\n104\n105\nSteps\nFigure 11\nWhen we hold either total compute or number of training steps ﬁxed, performance follows\nL(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that\nmaximizes performance. Mediocre ﬁts at small S are unsurprising, as the power-law equation for the learning\ncurves breaks down very early in training.\nParameter\nαN\nαS\nNc\nSc\nValue\n0.077\n0.76\n6.5 × 1013\n2.1 × 103\nTable 3\nFits to L(N, S)\nWith these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe\nthey are quite compelling given the simplicity of Equation (5.6).\nThe data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we\nstudy the test loss as a function of model size while ﬁxing either the total non-embedding compute C used\nin training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters\nabove and Equation (5.6).\nThe power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss\nlandscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-\nlaw should provide information about the spectrum of the Hessian of the loss. Its universality suggests that\nthe Hessian eigenvalue density is roughly independent of model size.\n5.3\nLower Bound on Early Stopping Step\nThe results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which\nearly stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D\nlearning curves for a given model will be very similar until we reach Smin ≈Sstop. Thus overﬁtting should\nbe proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because\nin reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more\ntraining steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality\nSstop(N, D) ≳\nSc\n[L(N, D) −L(N, ∞)]1/αS\n(5.7)\nwhere L(N, ∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-\nparison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop\nand L(N, D) are empirical (though Sstop is adjusted to mimic training at B ≫Bcrit), while L(N, ∞) is\ncomputed from the ﬁt to L(N, D) evaluated at D = ∞.\n6\nOptimal Allocation of the Compute Budget\nWe displayed the empirical trend of performance as a function of the computation used during training in\nthe top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know\n14\nModels between 0.6x and 2.2x the \noptimal size can be trained with a \n20% larger compute budget\nSmaller models require \nmore steps to train, while \nlarger models require fewer\nOur framework does not \ncapture early training dynamics\nFigure 12\nLeft: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger\nor smaller models can be trained with minimal additional compute. Right: Models larger than the compute-\nefﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-\nlelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the\npower-law region of the learning curve, after initial transient effects.\n10\n8\n10\n6\n10\n4\n10\n2\n100\nCompute (PF-days), non-embedding\n2\n3\n4\n5\n6\n7\nTest Loss\nL = (Cmin/2.3 108)\n0.050\nL = (C/2.0 107)\n0.057\nFigure 13\nWhen adjusting performance to simulate training far below the critical batch size, we ﬁnd a\nsomewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous\nlump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks\nin the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger\ncompute.\nthat in fact we could train more efﬁciently6 by training at the batch size Bcrit discussed in Section 5.1.\nLarge and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,\nand correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more\npredictable trends.\nIn this section we will adjust for this oversight. More importantly, we will use the results of Section 5\nto determine the optimal allocation of compute between model size N and the quantity of data processed\nduring training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by\nusing the equation for L(N, Smin), and we will demonstrate that these methods agree.\n6.1\nOptimal Performance and Allocations\nLet us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is\nplotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the\nnew ﬁt with Cmin is somewhat improved.\nGiven L(Cmin), it is natural to ask for the optimal model size N(Cmin) that provides the minimal loss with a\ngiven quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin)\n6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the\nmodel but also on the target value of the loss we wish to achieve, and so is a moving target.\n15\n10\n7\n10\n5\n10\n3\n10\n1\nCompute (PF-days), non-embedding\n103\n105\n107\nParameters (non-embedding)\nN = (1.3 109) C0.73\nmin\nN = (1.6 109) C0.88\n10\n7\n10\n5\n10\n3\n10\n1\nCompute (PF-days), excluding embeddings\n0\n5000\n10000\n15000\nSteps\nSmin (adjusted)\nSmin = (5.4 103) C0.03\nmin\nS (fixed-batch)\nFigure 14\nLeft: Each value of the compute budget Cmin has an associated optimal model size N. Optimal\nmodel size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number\nof data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.\nRight: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most\nof the growth in data examples processed can be used for increased batch sizes.\ncan be ﬁt very well with a power-law\nN(Cmin) ∝(Cmin)0.73.\n(6.1)\nIn Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).\nBy deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since\nprior ﬁts show B ∝L−4.8 and L ∝C−0.05\nmin\n, we can conclude that Bcrit ∝C0.24\nmin . This leads us to conclude\nthat the optimal number of steps will only grow very slowly with compute, as\nSmin ∝(Cmin)0.03,\n(6.2)\nmatching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results\nmay even be consistent with an exponent of zero.\nThus we conclude that as we scale up language modeling with an optimal allocation of computation, we\nshould predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝\nBcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively\nfew optimization steps, additional work on speeding up early training dynamics may be warranted.\n6.2\nPredictions from L(N, Smin)\nThe results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in\nSection 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin\n6NB and then ﬁnd the minimum\nof the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in\nAppendix B, where we also provide some additional predictions.\nFor the loss as a function of training compute, we predict that\nL(Cmin) =\n\u0012Cmin\nc\nCmin\n\u0013αmin\nC\n(6.3)\nwhere\nαmin\nC\n≡\n1\n1/αS + 1/αB + 1/αN\n≈0.054\n(6.4)\nin excellent agreement with the exponent of Figure 13. We also predict that\nN(Cmin) ∝(Cmin)αmin\nC\n/αN ≈(Cmin)0.71\n(6.5)\nwhich also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive\nframework for the performance of language modeling.\n16\nThe intersection point is sensitive to \nthe precise power-law parameters\nFigure 15\nFar beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations\nfor L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection\nmarks the point before which we expect our predictions to break down. The location of this point is highly\nsensitive to the precise exponents from our power-law ﬁts.\n6.3\nContradictions and a Conjecture\nWe observe no signs of deviation from straight power-law trends at large values of compute, data, or model\nsize. Our trends must eventually level off, though, since natural language has non-zero entropy.\nIndeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-\ndiction. At scales several orders of magnitude above those documented here, the performance predicted by\nthe L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with\ncompute. This implies that our scaling laws must break down before this point, but we conjecture that the\nintersection point has a deeper meaning: it provides an estimate of the point at which Transformer language\nmodels reach maximal performance.\nSince the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the\nperformance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).\nLet us work this out in more detail.\nTo keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as\nD ∝N 0.74 ∝C0.54\nmin\n(6.6)\nwhere we have used the compute-efﬁcient N(Cmin) from Figure 14.\nLet us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch\nsize (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as\nD(Cmin) =\n2Cmin\n6N(Cmin) ≈\n\u00004 × 1010 tokens\n\u0001\n(Cmin/PF-Day)0.26\n(6.7)\nThis is the maximum rate at which the dataset size can productively grow with compute, since it means that\nwe are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).\nIt appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if\nthe training process never re-uses any data!\nAccording to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the\nloss should scale as L(D) ∝D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝\nC−0.03\nmin\nonce we are data-limited. Once again, we have a contradiction, as this will eventually intersect with\nour prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050\nmin\n.\nThe intersection point of L(D(Cmin)) and L(Cmin) occurs at\nC∗∼104 PF-Days\nN ∗∼1012 parameters,\nD∗∼1012 tokens,\nL∗∼1.7 nats/token\n(6.8)\nthough the numerical values are highly uncertain, varying by an order or magnitude in either direction de-\npending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is\nthat our scaling laws break down at or before we reach this point, which is still many orders of magnitude\naway in both compute and model size.\n17\nOne might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\nsize beyond N ∗without qualitatively different data requirements, perhaps this means that once we reach\nC∗\nmin and N ∗, we have extracted all of the reliable information available in natural language data. In this\ninterpretation, L∗would provide a rough estimate for the entropy-per-token7 of natural language. In this\nscenario, we would expect the loss trend to level off at or before L∗.\nWe can guess at the functional form of L(Cmin) as it levels off by considering a version of our training\ndataset with added noise. For example, we could append a random string of tokens to each context shown\nto the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise\nﬂoor L −Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\npotentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and\nmay be meaningful even if it occurs after the leveling off.\n7\nRelated Work\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\nnumber of relevant features in the data.\nSome early [BB01, Goo01] work found power-law scalings between performance and dataset size. More\nrecent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is\nperhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of\ndataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our\nﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets\n[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent\nwork [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an\nansatz similar to ours.\nEfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal\nperformance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that\nfor language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).\nBut more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the\noverall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles\nof shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width\nand depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies\nﬁx computation per data example, which tends to scale in proportion to the number of model parameters,\nwhereas we investigate scaling with both model size and the quantity of training computation.\nVarious works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-\ning a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training\nmany orders of magnitude beyond typical practice, and in particular does not use early stopping). We do\nnot observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\nHessian spectrum [Pap18, GKX19, GARD18].\n8\nDiscussion\nWe have observed consistent scalings of language model log-likelihood loss with non-embedding parameter\ncount N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and\n(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.\nSince scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale.\n7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.\n8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of\nloss on both model and dataset size.\n18\nWe were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when\nthese parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\nof overﬁtting, early stopping step, and data requirements when training large language models. So our scaling\nrelations go beyond mere observation to provide a predictive framework. One might interpret these relations\nas analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,\nindependent of most of the details of its microscopic consituents.\nIt is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a\nmaximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to\ntest these relations on other domains, such as images, audio, and video models, and perhaps also for random\nnetwork distillation. At this point we do not know which of our results depend on the structure of natural\nlanguage data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from\nwhich the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we\nhave observed. Such a theory might make it possible to derive other more precise predictions, and provide a\nsystematic understanding of the limitations of the scaling laws.\nIn the domain of natural language, it will be important to investigate whether continued improvement on the\nloss translates into improvement on relevant language tasks. Smooth quantitative change can mask major\nqualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy\nprovides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth\nimprovements in language model loss may hide seemingly qualitative changes in capability.\nOur results strongly suggest that larger models will continue to perform better, and will also be much more\nsample efﬁcient than has been previously appreciated. Big models may be more important than big data.\nIn this context, further investigation into model parallelism is warranted. Deep models can be trained using\npipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased\nbatch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization\n[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity\n[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through\nincreased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,\nit might be possible to remain on the compute-efﬁcient frontier for an entire training run.\nAcknowledgements\nWe would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\nback on drafts of this work.\n19\nAppendices\nA\nSummary of Power Laws\nFor easier reference, we provide a summary below of the key trends described throughout the paper.\nParameters\nData\nCompute\nBatch Size\nEquation\nN\n∞\n∞\nFixed\nL (N) = (Nc/N)αN\n∞\nD\nEarly Stop\nFixed\nL (D) = (Dc/D)αD\nOptimal\n∞\nC\nFixed\nL (C) = (Cc/C)αC (naive)\nNopt\nDopt\nCmin\nB ≪Bcrit\nL (Cmin) =\n\u0000Cmin\nc\n/Cmin\n\u0001αmin\nC\nN\nD\nEarly Stop\nFixed\nL (N, D) =\n\u0014\u0000 Nc\nN\n\u0001 αN\nαD + Dc\nD\n\u0015αD\nN\n∞\nS steps\nB\nL (N, S) =\n\u0000 Nc\nN\n\u0001αN +\n\u0010\nSc\nSmin(S,B)\n\u0011αS\nTable 4\nThe empirical ﬁtted values for these trends are:\nPower Law\nScale (tokenization-dependent)\nαN = 0.076\nNc = 8.8 × 1013 params (non-embed)\nαD = 0.095\nDc = 5.4 × 1013 tokens\nαC = 0.057\nCc = 1.6 × 107 PF-days\nαmin\nC\n= 0.050\nCmin\nc\n= 3.1 × 108 PF-days\nαB = 0.21\nB∗= 2.1 × 108 tokens\nαS = 0.76\nSc = 2.1 × 103 steps\nTable 5\nThe optimal parameters for compute efﬁcient training are given by:\nCompute-Efﬁcient Value\nPower Law\nScale\nNopt = Ne · CpN\nmin\npN = 0.73\nNe = 1.3 · 109 params\nB ≪Bcrit =\nB∗\nL1/αB = BeCpB\nmin\npB = 0.24\nBe = 2.0 · 106 tokens\nSmin = Se · CpS\nmin (lower bound)\npS = 0.03\nSe = 5.4 · 103 steps\nDopt = De · CpD\nmin (1 epoch)\npD = 0.27\nDe = 2 · 1010 tokens\nTable 6\nB\nEmpirical Model of Compute-Efﬁcient Frontier\nThroughout this appendix all values of C, S, and αC are adjusted for training at the critical batch size Bcrit.\nWe have left off the ‘adj’ label to avoid cluttering the notation.\nB.1\nDeﬁning Equations\nThe power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this\nappendix, we will derive the optimal performance, model size, and number of training steps as a function of\n20\nthe compute budget. We start with the Equation (1.6), repeated here for convenience:\nL (N, S) =\n\u0012Nc\nN\n\u0013αN\n+\n\u0012Sc\nS\n\u0013αS\n.\n(B.1)\nHere, S represents the number of parameter updates when training at the critical batch size [MKAT18],\nwhich was deﬁned in Equation (5.2)9:\nB (L) =\nB∗\nL1/αB .\n(B.2)\nWe would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =\nC/ (6NB (L)), where C is the number of FLOPs used in the training run:\nL (N, C) =\n\u0012Nc\nN\n\u0013αN\n+\n\u0012\n6B∗Sc\nN\nL1/αBC\n\u0013αS\n.\n(B.3)\nNow, we set ∂NL\n\f\f\nC = 0 to ﬁnd the condition for optimality:\n0 = ∂L\n∂N\n\f\f\nC\n= −αN\nN\n\u0012Nc\nN\n\u0013αN\n+ αS\nN\n\u0012\n6B∗Sc\nN\nL1/αBC\n\u0013αS \u0012\n1 −5N\nL\u001a\u001a\u001a\n∂L\n∂N\n\f\f\nC\n\u0013\n=⇒αN\nαS\n\u0012Nc\nN\n\u0013αN\n=\n\u0012\n6B∗Sc\nN\nL1/αBC\n\u0013αS\n(B.4)\nEquation (B.3) and (B.4) together determine the compute-efﬁcient frontier.\nB.2\nEfﬁcient Training\nNow we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields\nL (Neﬀ(C) , C) =\n\u0012\n1 + αN\nαS\n\u0013\nL (Neﬀ, ∞) ,\n(B.5)\nwhich implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN\nαS ≈10% above\nthe converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating\nN yields a power-law dependence of performance on compute:\nL (C) =\n\u0012Cc\nC\n\u0013αC\n(B.6)\nwhere we deﬁned\nαC = 1/ (1/αS + 1/αB + 1/αN) ≈0.052\n(B.7)\nCc = 6NcB∗Sc\n\u0012\n1 + αN\nαS\n\u00131/αS+1/αN \u0012 αS\nαN\n\u00131/αS\n.\n(B.8)\nSimilarly, we can eliminate L to ﬁnd N (C):\nN (C)\nNc\n=\n\u0012 C\nCc\n\u0013αC/αN \u0012\n1 + αN\nαS\n\u00131/αN\n(B.9)\nand\nS (C) =\nCc\n6NcB∗\n\u0012\n1 + αN\nαS\n\u0013−1/αN \u0012 C\nCc\n\u0013αC/αS\n(B.10)\n9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could\ninstead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is\nthe averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see\n[MKAT18]).\n21\nB.3\nComparison to Inefﬁcient\nTypically, researchers train models until they appear to be close to convergence. In this section, we compare\nthe efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor\nf as the percent deviation from the converged loss:\nL (N, C) = (1 + f) L (N, ∞) .\n(B.11)\nFor compute-efﬁcient training we have f = αN/αS ≈10% from the previous section, but researchers\ntypically use a much smaller value. Here, we choose f ′ = 2% as an estimate. For a ﬁxed value of the loss,\nwe predict:\nNf\nNf ′ =\n\u0012 1 + f\n1 + f ′\n\u00131/αN\n≈2.7\n(B.12)\nSf\nSf ′ =\n \n1 + 1\nf\n1 + 1\nf ′\n!1/αS\n≈0.13\n(B.13)\nCf\nCf ′ = Nf\nNf ′\nSf\nSf ′ ≈0.35\n(B.14)\nSo that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less\ncompute to reach the same loss.\nB.4\nSuboptimal Model Sizes\nWe can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss\nL with a model of size N:\nC (N, L) =\n\u0012\n6B∗Sc\nN\nL1/αB\n\u0013 \u0012\nL −\n\u0012Nc\nN\n\u0013αN \u0013−1/αS\n.\n(B.15)\nUsing A.6 and A.9, we can eliminate L in favor of Neﬀ(L), the model size which reaches L most efﬁciently.\nFrom there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal\nmodel size:\nC (N, Neﬀ)\nC (Neﬀ, Neﬀ) =\nN\nNeﬀ\n\u0014\n1 + αS\nαN\n\u0012\n1 −\n\u0012Neﬀ\nN\n\u0013αN \u0013\u0015−1/αS\n.\n(B.16)\nThe result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a\n20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A\nlarger model can be trained the the same level of performance in fewer steps, allowing for more parallelism\nand faster training if sufﬁcient harware is available (see Figure Y):\nS (N, Neﬀ)\nS (Neﬀ, Neﬀ) =\n\u0014\n1 + αS\nαN\n\u0012\n1 −\n\u0012Neﬀ\nN\n\u0013αN \u0013\u0015−1/αS\n.\n(B.17)\nA 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation\nshould not be trusted for very large models, as it is only valid in the power-law region of the learning curve\nafter initial transient effects.\nC\nCaveats\nIn this section we list some potential caveats to our analysis.\n• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.\nThe scaling relations with model size and compute are especially mysterious. It may be possible to\nunderstand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning\ncurves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very\nlarge model size still remains mysterious. Without a theory or a systematic understanding of the\ncorrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.\n22\n103\n104\n105\nSc × [L(N, D)\nL(N,\n)]\n1/\nS\n103\n104\n105\nSstop\nEarly Stopping Step\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n103\n104\n105\nStep\n2\n3\n4\n5\n6\nLoss\nTest Loss\nTrain Loss\n108\n109\n1010\nDataset Size (Tokens)\nFigure 16\nLeft: We characterize the step on which early stopping occurs, as a function of the extent of\noverﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:\nWe display train and test loss for a series of 300M parameter models trained on different sized dataset sub-\nsamples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the\ndegree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain\n(denoted by a black bar for each run).\n• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the\nrange we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between\ndata parallelism and the number of serial training steps required, which would have a major impact\non training time.\n• We did not thoroughly investigate the small data regime, and our ﬁts for L(N, D) were poor for\nthe smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did\nnot experiment with regularization and data augmentation. Improvements in these could alter our\nresults, quantitatively or qualitatively.\n• We used the estimated training compute C ≈6NBS, which did not include contributions propor-\ntional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the\nregime of very large nctx, speciﬁcally where nctx ≳12dmodel.\n• We tuned learning rates, and we experimented with learning rate schedules. But we may have\nneglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important\neffect on scaling.\n• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,\nit may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short\ntraining run (eg due to compute limitations), it may be possible to use a larger learning rate. We did\nnot experiment with higher learning rates for training runs that did not proceed to convergence.\nD\nSupplemental Figures\nD.1\nEarly Stopping and Test vs Train\nIn section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on\nthe early stopping step. We also show the train and test loss for a given model size when training on different\nsized datasets.\nD.2\nUniversal Transformers\nWe compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.\nThese models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a\nfunction of compute C. We include several different different possibilities for parameter re-use.\nD.3\nBatch Size\nWe measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate\nBcrit(L) in ﬁgure 10.\n23\n105\n106\n107\n108\n109\nParameters, including reuse (non-embedding)\n2.5\n3.0\n3.5\n4.0\n4.5\nTest Loss\n2x Reuse\n4x Reuse\n8x Reuse\nNon-recurrent Models\n105\n106\n107\n108\n109\nParameters (non-embedding)\n2.5\n3.0\n3.5\n4.0\n4.5\nTest Loss\n2x Reuse\n4x Reuse\n8x Reuse\nNon-recurrent Models\nFigure 17\nWe compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-\nformers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,\nbut slightly worse when accounting for reuse and comparing per FLOP.\n102\n103\n104\n105\nStep\n106\n107\n108\n109\n1010\n1011\nTokens Processed\nBatch Size Scan - 3M Params\n4\n6\n8\n10\nTest Loss\n101\n102\n103\n104\n105\nStep\n106\n108\n1010\nTokens Processed\nBatch Size Scan - 85M Params\n4\n6\n8\n10\nTest Loss\nFigure 18\nThese ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and\nfor two different Transformer model sizes. These ﬁts were used to measure Bcrit(L) for Figure 10.\nD.4\nSample Efﬁciency vs Model Size\nIt is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We\nprovide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach\nvarious ﬁxed values of the loss.\n106\n107\n108\nParameters (non-embedding)\n103\n104\n105\nMinimum Steps (Smin)\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nLoss\n106\n107\n108\nParameters (non-embedding)\n108\n109\n1010\n1011\nMinimum Examples (Emin)\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nLoss\nFigure 19\nThe number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases\nprecipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)\nimproves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model\nto a very large one.\n24\n100\n101\n102\n103\nToken Index\n3\n4\n5\n6\n7\n8\nPer-Token Test Loss\n4.0 + 3.2 T\n0.47\n3.4 + 4.0 T\n0.56\n2.9 + 4.5 T\n0.56\n2.7 + 4.9 T\n0.60\n2.4 + 5.1 T\n0.61\n2.3 + 5.4 T\n0.62\n106\n107\n108\nModel Parameters\n101\n103\n105\nStep\n2\n4\n6\n8\n10\nTest Loss\nPer-token Loss (774M Params)\n100\n101\n102\n103\nToken Index\nFigure 20\nThis ﬁgure provides information about the performance per token as a function of model size\nand training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales\npredictably as a power-law in T. Right: Test loss per token as a function of training step.\n104\n105\n106\n107\n108\n109\nParameters (excl. embedding)\n3.0\n4.5\n6.0\n7.5\nTest Loss\nToken 1/1024\nToken 2/1024\nToken 4/1024\nToken 8/1024\nToken 16/1024\nToken 64/1024\nToken 256/1024\nToken 1024/1024\nToken 1/8\nToken 2/8\nToken 4/8\nToken 8/8\nFigure 21\nIn addition to the averaged loss, individual tokens within the 1024-token context also improve\nsmoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better\non early tokens, since they can allocate all of their capacity to them.\nD.5\nContext Dependence\nThe trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.\nWe see that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst\ntoken.\nFixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see\nFigure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,\nLT16], or a more general feature of the model architecture and optimization. It provides some suggestion for\nthe potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge\nto better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger\nmodels are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we\nshow how per-token performance varies for a ﬁxed model as a function of the training step. The model begins\nby learning short-range information, and only learns longer-range correlations later in training.\nWe have also included models trained with a tiny context nctx = 8 in order to compare with our longer\ncontext models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024\nmodels on very early tokens. This also suggests that further improvements should be possible with much\nlarger models trained on large contexts.\nD.6\nLearning Rate Schedules and Error Analysis\nWe experimented with a variety of learning rates and schedules. A host of schedules and resulting test\nperformances for a small language model are plotted in Figure 22. We conclude that the choice of learning\nrate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the\nschedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among\n25\n0\n50000\n100000\n150000\n200000\n250000\nStep\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nLearning Rate\n50\n100\n150\n200\n250\nLR Summed Over Steps\n3.65\n3.70\n3.75\n3.80\n3.85\n3.90\nLoss\nFigure 22\nWe test a variety of learning rate schedules including cosine decay, linear decay, as well as other\nfaster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we\ndo not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.\nWe ﬁnd that, as long as the learning rate is not too small and does not decay too quickly, performance does\nnot depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging\nmultiple runs is necessary to validate performance changes smaller than this level.\n104\n105\n106\n107\n108\n109\nParameters (non-embedding)\n2\n3\n4\n5\n6\nTest Loss (at convergence)\nL = (N/8.8 1013)\n0.076\nL =\n0.25log(N/7.1 1012)\nFigure 23\nThe trend for performance as a function of parameter count, L(N), is ﬁt better by a power law\nthan by other functions such as a logarithm at a qualitative level.\nschedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different\ntraining runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different\nrandom seeds is roughly constant in magnitude for different model sizes.\nWe found that larger models require a smaller learning rate to prevent divergence, while smaller models can\ntolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:\nLR(N) ≈0.003239 + −0.0001395 log(N)\n(D.1)\nWe expect that this formula could be improved. There may be a dependence on network width, likely set by\nthe initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that\nit works sufﬁciently well for the models we considered.\nD.7\nFit Details and Power Law Quality\nWe experimented with a number of functional forms for the ﬁts to L(N), L(C), and L(D); the power-law\nﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).\nFor L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers\ncauses a noticable lump in the data. For L(N) we also do not include very small models with only 1 layer in\nthe ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change\nmarginally if we do include them, and the trend extrapolates well in both directions regardless.\nD.8\nGeneralization and Architecture\nIn ﬁgure 24 we show that generalization to other data distributions does not depend on network depth when we\nhold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.\n26\n101\n102\nDepth\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nTest Loss\nWikipedia\nBooks\nInternet Books\nCommon Crawl\nWebText2 (Train)\nWebText2 (Test)\nFigure 24\nWe show evaluations on a series of datasets for models with approximately 1.5 Billion param-\neters. We observe no effect of depth on generalization; generalization performance depends primarily on\ntraining distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the\nearly-stopped performance; we have not seen this surprising result in other experiments.\nList of Figures\n1\nSummary of simple power laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nIllustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .\n4\n3\nHow to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . .\n4\n4\nPerformance when varying model and data size, or model and training steps, simultaneously\n5\n5\nWeak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . .\n8\n6\nComparison of performance trend when including or excluding embeddings . . . . . . . . .\n8\n7\nLSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . .\n9\n8\nGeneralization to other test datasets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n9\nUniversality of overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n10\nCritical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n11\nPerformance versus compute budget or number of parameter updates . . . . . . . . . . . . .\n14\n12\nTraining on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n13\nComparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . .\n15\n14\nOptimal model size and serial number of steps versus compute budget . . . . . . . . . . . .\n16\n15\nContradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .\n17\n16\nEarly stopping lower bound and training curves for overﬁt models\n. . . . . . . . . . . . . .\n23\n17\nUniversal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n18\nBatch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n19\nAnother look at sample efﬁciency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n20\nPower-law dependence of performance on position in context . . . . . . . . . . . . . . . . .\n25\n21\nPerformance at different context positions versus model size\n. . . . . . . . . . . . . . . . .\n25\n22\nLearning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n23\nComparison of Power-Law and Logarithmic Fits\n. . . . . . . . . . . . . . . . . . . . . . .\n26\n24\nGeneralization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n27\nList of Tables\n1\nParameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2\nFits to L(N, D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nFits to L(N, S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4\nKey trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n5\nKey parameters to trend ﬁts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6\nTrends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nReferences\n[ACDE12]\nEduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-\nrange correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–\n11587, 2012. 25\n[AS17]\nMadhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in\nneural networks. arXiv, 2017, 1710.03667. 11, 18, 22\n[BB01]\nMichele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-\nbiguation. In Proceedings of the 39th annual meeting on association for computational linguis-\ntics, pages 26–33. Association for Computational Linguistics, 2001. 18\n[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine\nlearning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18\n[Bia12]\nGÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,\n13(Apr):1063–1095, 2012. 18\n[CGRS19]\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/\nabs/1904.10509. 19\n[DCLT18]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-\nversal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/\nabs/1807.03819. 6, 9, 23, 24\n[EP94]\nWerner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.\nEPL (Europhysics Letters), 26(4):241, 1994. 25\n[Fou]\nThe Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7\n[GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.\n2018, arXiv:1812.04754. 18\n[GJS+19]\nMario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,\nGiulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with\nnumber of parameters in deep learning. arXiv, 2019, 1901.01608. 18\n[GKX19]\nBehrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-\ntimization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL\nhttp://arxiv.org/abs/1901.10159. 18\n[Goo01]\nJoshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL\nhttp://arxiv.org/abs/cs.CL/0108005. 18\n[GRK17]\nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-\nnai.com, 2017. 19\n[HAD19]\nJoel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-\ntational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and\nPractice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.\ndoi:10.1145/3293883.3295710. 18\n28\n[HCC+18]\nYanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\nand Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\nCoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\nninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\ndictable, empirically, 2017, 1712.00409. 18\n[JGH18]\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in neural information processing systems, pages\n8571–8580, 2018. 18\n[KB14]\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization, 2014,\n1412.6980. 7\n[Kom19]\nAran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18\n[KSH12]\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\nInformation Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19\n[LCG+19]\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut.\nAlbert: A lite bert for self-supervised learning of language representations, 2019,\n1909.11942. 9\n[LOG+19]\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-\ning approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/\n1907.11692. 2\n[LSP+18]\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6\n[LT16]\nHenry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv\npreprint arXiv:1606.06737, 2016. 25\n[LXS+19]\nJaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent, 2019, arXiv:1902.06720. 18\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\n[Pap18]\nVardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\nCoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18\n[RNSS18]\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6\n[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.\nA constructive\nprediction of the generalization error across scales, 2019, 1909.12673. 18\n[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.\nA constructive\nprediction of the generalization error across scales, 2019, arXiv:1909.12673. 18\n[RSR+19]\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer, 2019, arXiv:1910.10683. 2\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8\n[SCP+18]\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\nBlake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19\n[SHB15]\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. CoRR, 2015, 1508.07909. 6\n29\n[SLA+18]\nChristopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,\narXiv:1811.03600. 12\n[SS18]\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.\n7\n[THK18]\nStefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.\nOxford University Press, 2018. 18\n[TL19]\nMingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.\n11946. 18\n[VSP+17]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6\n[VWB16]\nAndreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles\nof relatively shallow networks, 2016, arXiv:1605.06431. 8, 18\n[Was06]\nLarry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.\n18\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems, 2019, 1905.00537. 2\n[WRH17]\nYu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-\ncreasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19\n[WYL19]\nWei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional\nnetworks, 2019, 1906.02909. 19\n[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe.\nXlnet:\nGeneralized autoregressive pretraining for language understanding, 2019,\narXiv:1906.08237. 2\n[ZK16]\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British\nMachine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18\n[ZKZ+15]\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. 2015 IEEE International Conference on Computer Vision\n(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7\n[ZLN+19]\nGuodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,\nChristopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch\nsizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL\nhttp://arxiv.org/abs/1907.04164. 12, 18\n30"
    },
    {
        "file_name": "2106.00001.pdf",
        "title": "2106.00001",
        "year": "2021",
        "full_text": "Privately Learning Subspaces\nVikrant Singhal *\nThomas Steinke†\nAbstract\nPrivate data analysis suffers a costly curse of dimensionality. However, the data often has\nan underlying low-dimensional structure. For example, when optimizing via gradient de-\nscent, the gradients often lie in or near a low-dimensional subspace. If that low-dimensional\nstructure can be identiﬁed, then we can avoid paying (in terms of privacy or accuracy) for\nthe high ambient dimension.\nWe present differentially private algorithms that take input data sampled from a low-\ndimensional linear subspace (possibly with a small amount of error) and output that subspace\n(or an approximation to it). These algorithms can serve as a pre-processing step for other\nprocedures.\n1\nIntroduction\nDifferentially private algorithms generally have a poor dependence on the dimensionality of\ntheir input. That is, their error or sample complexity grows polynomially with the dimension.\nFor example, for the simple task of estimating the mean of a distribution supported on [0,1]d,\nwe have per-coordinate error Θ(\n√\nd/n) to attain differential privacy, where n is the number of\nsamples. In contrast, the non-private error is Θ(\np\nlog(d)/n).\nThis cost of dimensionality is inherent [BUV14; SU17; DSSUV15]. Any method with lower\nerror is susceptible to tracing attacks (a.k.a. membership inference attacks). However, these\nlower bounds only apply when the data distribution is “high-entropy.” This leaves open the\nposssibility that we can circumvent the curse of dimensionality when the data has an underlying\nlow-dimensional structure.\nData often does possess an underlying low-dimensional structure. For example, the gradients\nthat arise in deep learning tend to be close to a low-dimensional subspace [ACGMMTZ16;\nLXTSG17; GARD18; LFLY18; LGZCB20; ZWB20; FT20]. Low dimensionality can arise from\nmeaningful relationships that are at least locally linear, such as income versus tax paid. It can\nalso arise because we are looking at a function of data with relatively few attributes.\nA long line of work [BLR08; HT10; HR10; Ull15; BBNS19; BCMNUW20; ZWB20; KRRT20,\netc.] has shown how to exploit structure in the data to attain better privacy and accuracy.\n*Northeastern University.\nPart of this work was done during an internship at IBM Research – Al-\nmaden. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . singhal.vi@northeastern.edu\n†Google\nResearch,\nBrain\nTeam.\nPart\nof\nthis\nwork\nwas\ndone\nat\nIBM\nResearch\n–\nAl-\nmaden. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . subspace@thomas-steinke.net\n1\narXiv:2106.00001v3  [cs.CR]  10 Aug 2021\nHowever, these approaches assume that this structure is known a priori or that it can be learned\nfrom non-private sources. This raises the question:\nCan we learn low-dimensional structure from the data subject to differential privacy?\nWe consider the simple setting where the data lies in Rd but is in, or very close to a linear sub-\nspace, of dimension k. We focus on the setting where k ≪d and we develop algorithms whose\nsample complexity does not depend on the ambient dimension d; a polynomial dependence on\nthe true dimension k is unavoidable.\nOur algorithms identify the subspace in question or, if the data is perturbed slightly, an\napproximation to it. Identifying the subspace structure is interesting in its own right, but it also\ncan be used as a pre-processing step for further analysis – by projecting to the low-dimensional\nsubspace, we ensure subsequent data analysis steps do not need to deal with high-dimensional\ndata.\n1.1\nOur Contributions: Privately Learning Subspaces – Exact Case\nWe ﬁrst consider the exact case, where the data X1,··· ,Xn ∈Rd are assumed to lie in a\nk-dimensional subspace (rather than merely being near to it) – i.e., rank(A) = k, where A =\nPn\ni XiXT\ni ∈Rd×d. In this case, we can also recover the subspace exactly.\nHowever, we must also make some non-degeneracy assumptions. We want to avoid a\npathological input dataset such as the following. Suppose X1,··· ,Xk are linearly independent,\nbut Xk = Xk+1 = Xk+2 = ··· = Xn. While we can easily reveal the repeated data point, we cannot\nreveal anything about the other points due to the privacy constraint.\nA natural non-degeneracy assumption would be to assume that the data points are in\n“general position” – that is, that there are no non-trivial linear dependencies among the data\npoints. This means that every set of k data points spans the subspace or, equivalently, no subspace\nof dimension k −1 contains more than k −1 data points. This is a very natural assumption – if\nthe data consists of n samples from a continuous distribution on the subspace, then this holds\nwith probability 1. We relax this assumption slightly and assume that no subspace of dimension\nk −1 contains more than ℓdata points. We also assume that all points are non-zero. Note that\nwe deﬁne subspaces to pass through the origin; our results can easily be extended to afﬁne\nsubspaces.\nTheorem 1.1 (Main Result – Exact Case). For all n,d,k,ℓ∈N and ε,δ > 0 satisfying n ≥O\n\u0010\nℓ+ log(1/δ)\nε\n\u0011\n,\nthere exists a randomized algorithm M : Rd×n →Sk\nd satisfying the following. Here Sk\nd denotes the set of\nall k-dimensional subspaces of Rd.\n• M is (ε,δ)-differentially private with respect to changing one column of its input.\n• Let X = (X1,··· ,Xn) ∈Rd×n. Suppose there exists a k-dimensional subspace S∗∈Sk\nd that contains\nall but ℓof the points – i.e., |{i ∈[n] : Xi ∈S∗}| ≥n−ℓ. Further suppose that any (k−1)-dimensional\nsubspace contains at most ℓpoints – i.e., for all S ∈Sk−1\nd\n, we have |{i ∈[n] : Xi ∈S}| ≤ℓ. Then\nP[M(X) = S∗] = 1.\n2\nThe parameter ℓin Theorem 1.1 can be thought of as a robustness parameter. Ideally the\ndata points are in general position, in which case ℓ= k −1. If a few points are corrupted, then we\nincrease ℓaccordingly; our algorithm can tolerate the corruption of a small constant fraction of\nthe data points. Theorem 1.1 is optimal in the sense that n ≥Ω\n\u0010\nℓ+ log(1/δ)\nε\n\u0011\nsamples are required.\n1.2\nOur Contributions: Privately Learning Subspaces – Approximate Case\nNext we turn to the substantially more challenging approximate case, where the data\nX1,··· ,Xn ∈Rd are assumed to be close to a k-dimensional subspace, but are not assumed to be\ncontained within that subspace. Our algorithm for the exact case is robust to changing a few\npoints, but very brittle if we change all the points by a little bit. Tiny perturbations of the data\npoints (due to numerical errors or measurement imprecision) could push the point outside the\nsubspace, which would cause the algorithm to fail. Thus it is important to for us to cover the\napproximate case and our algorithm for the approximate is entirely different from our algorithm\nfor the exact case.\nThe approximate case requires us to precisely quantify how close the input data and our\noutput are to the subspace and we also need to make quantitative non-degeneracy assumptions.\nIt is easiest to formulate this via a distributional assumption. We will assume that the data\ncomes from a Gaussian distribution where the covariance matrix has a certain eigenvalue gap.\nThis is a strong assumption and we emphasize that this is only for ease of presentation; our\nalgorithm works under weaker assumptions. Furthermore, we stress that the differential privacy\nguarantee is worst-case and does not depend on any distributional assumptions.\nWe assume that the data is drawn from a multivariate Gaussian N (0,Σ). Let λ1(Σ) ≥\nλ2(Σ) ≥··· ≥λd(Σ) be the eigenvalues of Σ ∈Rd×d. We assume that there are k large eigen-\nvalues λ1(Σ),··· ,λk(Σ) – these represent the “signal” we want – and d −k small eigenvalues\nλk+1(Σ),··· ,λd(Σ) – these are the “noise”. Our goal is to recover the subspace spanned by the\neigenvectors corresponding to the k largest eigenvalues λ1(Σ),··· ,λk(Σ). Our assumption is that\nthere is a large multiplicative gap between the large and small eigenvalues. Namely, we assume\nλk+1(Σ)\nλk(Σ) ≤\n1\npoly(d).\nTheorem 1.2 (Main Result – Approximate Case). For all n,d,k ∈N and α,γ,ε,δ > 0 satisfying\nn≥Θ\n k log(1/δ)\nε\n+ ln(1/δ)ln(ln(1/δ)/ε)\nε\n!\nand γ2 ≤Θ\n \nεα2n\nd2k log(1/δ) ·min\n(1\nk ,\n1\nlog(k log(1/δ)/ε)\n)!\n,\nthere exists an algorithm M : Rd×n →Sk\nd satisfying the following. Here Sk\nd is the set of all k-dimensional\nsubspaces of Rd represented as projection matricies – i.e., Sk\nd = {Π ∈Rd×d : Π2 = Π = ΠT ,rank(Π) = k}.\n• M is (ε,δ)-differentially private with respect to changing one column of its input.\n• Let X1,··· ,Xn be independent samples from N (0,Σ). Let λ1(Σ) ≥λ2(Σ) ≥··· ≥λd(Σ) be the\neigenvalues of Σ ∈Rd×d. Suppose λk+1(Σ) ≤γ2 · λk(Σ). Let Π ∈Sk\nd be the projection matrix onto\nthe subspace spanned by the eigenvectors corresponding to the k largest eigenvalues of Σ. Then\nP[∥M(X) −Π∥≤α] ≥0.7.\n3\nThe sample complexity of our algorithm n = O(k log(1/δ)/ε) is independent of the ambient\ndimension d; this is ideal. However, there is a polynomial dependence on d in γ, which controls\nthe multiplicative eigenvalue gap. This multiplicative eigenvalue gap is a strong assumption,\nbut it is also a necessary assumption if we want the sample complexity n to be independent\nof the dimension d. In fact, it is necessary even without the differential privacy constraint [CZ16].\nThat is, if we did not assume an eigenvalue gap that depends polynomially on the ambient\ndimension d, then it would be impossible to estimate the subspace with sample complexity n\nthat is independent of the ambient dimension d even in the non-private setting.\nOur algorithm is based on the subsample and aggregate framework [NRS07] and a differ-\nentially private histogram algorithm. These methods are generally quite robust and thus our\nalgorithm is, too. For example, our algorithm can tolerate o(n/k) input points being corrupted\narbitrarily.\nWe also believe that our algorithm’s utility guarantee is robust to relaxing the\nGaussianity assumption. All that we require in the analysis is that the empirical covariance\nmatrix of a few samples from the distribution is sufﬁciently close to its expectation Σ with high\nprobability.\n1.3\nRelated Work\nTo the best of our knowledge, the problem of privately learning subspaces, as we formulate\nit, has not been studied before. However, a closely-related line of work is on Private Principal\nComponent Analysis (PCA) and low-rank approximations. We brieﬂy discuss this extensive\nline of work below, but ﬁrst we note that, in our setting, all of these techniques have a sample\ncomplexity n that grows polynomially with the ambient dimension d. Thus, they do not evade\nprivacy’s curse of dimensionality. However, we make a stronger assumption than these prior\nworks – namely, we assume a large multiplicative eigenvalue gap. (Many of the prior works\nconsider an additive eigenvalue gap, which is a weaker assumption.)\nThere has been a lot of interest in Private PCA, matrix completion, and low-rank approx-\nimation. One motivation for this is the infamous Netﬂix prize, which can be interpreted as a\nmatrix completion problem. The competition was cancelled after researchers showed that the\npublic training data revealed the private movie viewing histories of many of Netﬂix’s customers\n[NS06]. Thus privacy is a real concern for matrix analysis tasks.\nMany variants of these problems have been considered: Some provide approximations to the\ndata matrix X = (X1,··· ,Xn) ∈Rd×n; others approximate the covariance matrix A = Pn\ni XiXT\ni ∈\nRd×d (as we do). There are also different forms of approximation – we can either produce a\nsubspace or an approximation to the entire matrix, and the approximation can be measured by\ndifferent norms (we consider the operator norm between projection matrices). Importantly, we\ndeﬁne differential privacy to allow one data point Xi to be changed arbitrarily, whereas most\nof the prior work assumes a bound on the norm of the change or even assumes that only one\ncoordinate of one vector can be changed. In the discussion below we focus on the techniques\nthat have been considered for these problems, rather than the speciﬁc results and settings.\nDwork, Talwar, Thakurta, and Zhang [DTTZ14] consider the simple algorithm which adds\nindependent Gaussian noise to each of entries of the covariance matrix A, and then perform\nanalysis on the noisy matrix. (In fact, this algorithm predates the development of differential\n4\nprivacy [BDMN05] and was also analyzed under differential privacy by McSherry and Mironov\n[MM09] and Chaudhuri, Sarwate, and Sinha [CSS12].) This simple algorithm is versatile and\nseveral bounds are provided for the accuracy of the noisy PCA. The downside of this is that a\npolynomial dependence on the ambient dimension d is inherent – indeed, they prove a sample\ncomplexity lower bound of n = ˜Ω(\n√\nd) for any algorithm that identiﬁes a useful approximation\nto the top eigenvector of A. This lower bound does not contradict our results because the\nrelevant inputs do not satisfy our near low-rank assumption.\nHardt and Roth [HR12] and Arora, Braverman, and Upadhyay [ABU18] apply techniques\nfrom dimensionality reduction to privately compute a low-rank approximation to the input\nmatrix X. Hardt and Roth [HR13] and Hardt and Price [HP13] use the power iteration method\nwith noise injected at each step to compute low-rank approximations to the input matrix X. In\nall of these, the underlying privacy mechanism is still noise addition and the results still require\nthe sample complexity to grow polynomially with the ambient dimension to obtain interesting\nguarantees. (However, the results can be dimension-independent if we deﬁne differential\nprivacy so that only one entry – as opposed to one column – of the matrix X can be changed by\n1. This is a signiﬁcantly weaker privacy guarantee.)\nBlocki, Blum, Datta, and Sheffet [BBDS12] and Sheffet [She19] also use tools from dimen-\nsionality reduction; they approximate the covariance matrix A. However, they show that the\ndimensionality reduction step itself provides a privacy guarantee (whereas the aforementioned\nresults did not exploit this and relied on noise added at a later stage). Sheffet [She19] analyzes\ntwo additional techniques – the addition of Wishart noise (i.e., Y Y T where the columns of Y are\nindependent multivariate Gaussians) and sampling from an inverse Wishart distribution (which\nhas a Bayesian interpretation).\nChaudhuri, Sarwate, and Sinha [CSS12], Kapralov and Talwar [KT13], Wei, Sarwate, Coran-\nder, Hero, and Tarokh [WSCHT16], and Amin, Dick, Kulesza, Medina, and Vassilvitskii [AD-\nKMV18] apply variants of the exponential mechanism [MT07] to privately select a low-rank\napproximation to the covariance matrix A. This method is nontrivial to implement and analyse,\nbut it ultimately requires the sample complexity to grow polynomially in the ambient dimension.\nGonem and Gilad-Bachrach [GGB18] exploit smooth sensitivity [NRS07] to release a low-\nrank approximation to the matrix A. This allows adding less noise than worst case sensitivity,\nunder an eigenvalue gap assumption. However, the sample complexity n is polynomial in the\ndimension d.\nLimitations of Prior Work\nGiven the great variety of techniques and analyses that have been\napplied to differentially private matrix analysis problems, what is missing? We see that almost\nall of these techniques are ultimately based on some form of noise addition or the exponential\nmechanism. With the singular exception of the techniques of Sheffet [She19], all of these prior\ntechniques satisfy pure1 or concentrated differential privacy [BS16]. This is enough to conclude\nthat these techniques cannot yield the dimension-independent guarantees that we seek. No\namount of postprocessing or careful analysis can avoid this limitation. This is because pure and\nconcentrated differential privacy have strong group privacy properties, which means “packing”\nlower bounds [HT10] apply.\n1Pure differential privacy (a.k.a. pointwise differential privacy) is (ε,δ)-differential privacy with δ = 0.\n5\nWe brieﬂy sketch why concentrated differential privacy is incompatible with dimension-\nindependent guarantees. Let the input be X1 = X2 = ··· = Xn = ξ/\n√\nd for a uniformly random\nξ ∈{−1,+1}d. That is, the input is one random point repeated n times. If M satisﬁes O(1)-\nconcentrated differential privacy, then it satisﬁes the mutual information bound I(M(X);X) ≤\nO(n2) [BS16]. But, if M provides a meaningful approximation to X or A = XXT , then we must be\nable to recover an approximation to ξ from its output, whence I(M(X);X) ≥Ω(d), as the entropy\nof X is d bits. This gives a lower bound of n ≥Ω(\n√\nd), even though X and A have rank k = 1.\nThe above example shows that, even under the strongest assumptions (i.e., the data lies\nexactly in a rank-1 subspace), any good approximation to the subspace, to the data matrix X, or\nto the covariance matrix A = XXT must require the sample complexity n to grow polynomially\nin the ambient dimension d if we restrict to techniques that satisfy concentrated differential\nprivacy. Almost all of the prior work in this general area is subject to this restriction.\nTo avoid a sample complexity n that grows polynomially with the ambient dimension d, we\nneed fundamentally new techniques.\n1.4\nOur Techniques\nFor the exact case, we construct a score function for subspaces that has low sensitivity, assigns\nhigh score to the correct subspace, and assigns a low score to all other subspaces. Then we can\nsimply apply a GAP-MAX algorithm to privately select the correct subspace [BDRS18].\nThe GAP-MAX algorithm satisﬁes (ε,δ)-differential privacy and outputs the correct subspace\nas long as the gap between its score and that of any other subspace is larger than O(log(1/δ)/ε).\nThis works even though there are inﬁnitely many subspaces to consider, which would not be\npossible under concentrated differential privacy.\nThe simplest score function would simply be the number of input points that the subspace\ncontains. This assigns high score to the correct subspace, but it also assigns high score to any\nlarger subspace that contains the correct subspace. To remedy this, we subtract from the score\nthe number of points contained in a strictly smaller subspace. That is, the score of subspace S\nis the number of points in S minus the maximum over all subspaces S′ ⊊S of the number of\npoints contained in S′.\nThis GAP-MAX approach easily solves the exact case, but it does not readily extend to the\napproximate case. If we count points near to the subspace, rather than in it, then (inﬁnitely)\nmany subspaces will have high score, which violates the assumptions needed for GAP-MAX to\nwork. Thus we use a completely different approach for the approximate case.\nWe apply the “subsample and aggregate” paradigm of [NRS07]. That is, we split the dataset\nX1,··· ,Xn into n/O(k) sub-datasets each of size O(k). We use each sub-dataset to compute an\napproximation to the subspace by doing a (non-private) PCA on the sub-dataset. Let Π be\nthe projection matrix onto the correct subspace and Π1,··· ,Πn/O(k) the projection matrices onto\nthe approximations derived from the sub-datasets. With high probability ∥Πj −Π∥is small for\nmost j. (Exactly how small depends on the eigengap.) Now we must privately aggregate the\nprojection matrices Π1,··· ,Πn/O(k) into a single projection matrix.\nRather than directly trying to aggregate the projection matrices, we pick a set of reference\npoints, project them onto the subspaces, and then aggregate the projected points. We draw\n6\np1,··· ,pO(k) independently from a standard spherical Gaussian. Then ∥Πjpi −Πpi∥≤∥Πj −Π∥·\nO(\n√\nk) is also small for all i and most j. We wish to privately approximate Πpi and to do this\nwe have n/O(k) points Πjpi most of which are close to Πpi. This is now a location or mean\nestimation problem, which we can solve privately. Thus we obtain points ˆpi such that ∥ˆpi −Πpi∥\nis small for all i. From a PCA of these points we can obtain a projection ˆΠ with ∥ˆΠ −Π∥being\nsmall, as required.\nFinally, we discuss how to privately obtain ( ˆp1, ˆp2,··· , ˆpO(k)) from (Π1p1,··· ,Π1pO(k)),··· ,\n(Πn/O(k)p1,··· ,Πn/O(k)pO(k)). It is better here to treat ( ˆp1, ˆp2,··· , ˆpO(k)) as a single vector in RO(kd),\nrather than as O(k) vectors in Rd. We split RO(kd) into cells and then run a differentially\nprivate histogram algorithm. If we construct the cells carefully, for most j we have that\n(Πjp1,··· ,ΠjpO(k)) is in the same histogram cell as the desired point (Πp1,··· ,ΠpO(k)). The\nhistogram algorithm will thus identify this cell, and we take an arbitrary point from this cell as\nour estimate ( ˆp1, ˆp2,··· , ˆpO(k)). The differentially private histogram algorithm is run over expo-\nnentially many cells, which is possible under (ε,δ)-differential privacy if n/O(k) ≥O(log(1/δ)/ε).\n(Note that under concentrated differential privacy the histogram algorithm’s sample complexity\nn would need to depend on the number of cells and, hence, the ambient dimension d.)\nThe main technical ingredients in the analysis of our algorithm for the approximate case\nare matrix perturbation and concentration analysis and the location estimation procedure\nusing differentially private histograms. Our matrix perturbation analysis uses a variant of\nthe Davis-Kahan theorem to show that if the empirical covariance matrix is close to the true\ncovariance matrix, then the subspaces corresponding to the top k eigenvalues of each are also\nclose; this is applied to both the subsamples and the projection of the reference points. The\nmatrix concentration results that we use show that the empirical covariance matrices in all the\nsubsamples are close to the true covariance matrix. This is the only place where the multivariate\nGaussian assumption arises. Any distribution that concentrates well will work.\n2\nNotations, Deﬁnitions, and Background Results\n2.1\nLinear Algebra and Probability Preliminaries\nHere, we mention a few key technical results that we will be using to prove the main theorem\nfor the approximate case. Throughout this document, we assume that the dimension d is\nlarger than some absolute constant, and adopt the following notation: for a matrix A of rank\nr, we use s1(A) ≥··· ≥sr(A) to denote the singular values of A in decreasing order, and use\nλ1(A) ≥··· ≥λr(A) to denote the eigenvalues of A in decreasing order; let smin(A) denote the\nleast, non-zero singular value of A. We omit the parentheses when the context is clear. We begin\nby stating two results about matrix perturbation theory. The ﬁrst result says that if two matrices\nare close to one another in operator norm, then their corresponding singular values are also\nclose to one another.\nDeﬁne\n∥M∥:= sup{∥Mx∥2 : x ∈Rd, ∥x∥2 ≤1}\nto be the operator norm with respect to the Euclidean vector norm.\n7\nLemma 2.1 (Singular Value Inequality). Let A,B ∈Rd×n and let r = min{d,n}. Then for 1 ≤i,j ≤r,\nsi+j−1(A + B) ≤si(A) + sj(B).\nThe following result gives a lower bound on the least singular value of sum of two matrices.\nLemma 2.2 (Least Singular Value of Matrix Sum). Let A,B ∈Rd×n. Then\nsmin(A + B) ≥smin(A) −∥B∥.\nThe next result bounds the angle between the subspaces spanned by two matrices that are\nclose to one another. Let X ∈Rd×n have the following SVD.\nX =\nh\nU\nU⊥\ni\n·\n\"\nΣ1\n0\n0\nΣ2\n#\n·\n\"\nV T\nV T\n⊥\n#\nIn the above, U,U⊥are orthonormal matrices such that U ∈Rd×r and U⊥∈Rd×(d−r), Σ1,Σ2 are\ndiagonal matrices, such that Σ1 ∈Rr×r and Σ2 ∈R(d−r)×(n−r), and V ,V⊥are orthonormal matrices,\nsuch that V ∈Rn×r and V⊥∈Rn×(n−r). Let Z ∈Rd×n be a perturbation matrix, and ˆX = X + Z,\nsuch that ˆX has the following SVD.\nˆX =\nh ˆU\nˆU⊥\ni\n·\n\" ˆΣ1\n0\n0\nˆΣ2\n#\n·\n\" ˆV T\nˆV T\n⊥\n#\nIn the above, ˆU, ˆU⊥, ˆΣ1, ˆΣ2, ˆV , ˆV⊥have the same structures as U,U⊥,Σ1,Σ2,V ,V⊥respectively.\nLet Z21 = U⊥UT\n⊥ZV V T and Z12 = UUT ZV⊥V T\n⊥. Suppose σ1 ≥··· ≥σr ≥0 are the singular\nvalues of UT ˆU. Let Θ(U, ˆU) ∈Rr×r be a diagonal matrix, such that Θii(U, ˆU) = cos−1(σi).\nLemma 2.3 (Sin(Θ) Theorem [CZ16]). Let X, ˆX,Z,Z12,Z21 be deﬁned as above. Denote α = smin(UT ˆXV )\nand β = ∥UT\n⊥ˆXV⊥∥. If α2 > β2 + min{∥Z12∥2,∥Z21∥2}, then we have the following.\n∥Sin(Θ)(U, ˆU)∥≤\nα∥Z21∥+ β∥Z12∥\nα2 −β2 −min{∥Z12∥2,∥Z21∥2}\nThe next result bounds ∥Sin(Θ)(U, ˆU)∥in terms of the distance between UUT and ˆU ˆUT .\nLemma 2.4 (Property of ∥Sin(Θ)∥[CZ16]). Let U, ˆU ∈Rd×r be orthonormal matrices, and let Θ(U, ˆU)\nbe deﬁned as above in terms of ˆU,U. Then we have the following.\n∥Sin(Θ)(U, ˆU)∥≤∥ˆU ˆUT −UUT ∥≤2∥Sin(Θ)(U, ˆU)∥\nThe next result bounds the singular values of a matrix, whose columns are independent\nvectors from a mean zero, isotropic distribution in Rd. We ﬁrst deﬁne the sub-Gaussian norm of\na random variable.\nDeﬁnition 2.5. Let X be a sub-Gaussian random variable. The sub-Gaussian norm of X, denoted\nby ∥X∥ψ2, is deﬁned as,\n∥X∥ψ2 = inf{t > 0 : E\nh\nexp(X2/t2)\ni\n≤2}.\n8\nLemma 2.6 (Theorem 4.6.1 [Ver18]). Let A be an n × m matrix, whose columns Ai are independent,\nmean zero, sub-Gaussian isotropic random vectors in Rn. Then for any t ≥0, we have\n√\nm −CK2(\n√\nn + t) ≤sn(A) ≤s1(A) ≤\n√\nm + CK2(\n√\nn + t)\nwith probability at least 1 −2exp(−t2). Here, K = maxi ∥A∥ψ2 (sub-Gaussian norm of A).\nIn the above, ∥A∥ψ2 ∈O(1) if the distribution in question is N (⃗0,I). The following corollary\ngeneralises the above result for arbitrary Gaussians.\nCorollary 2.7. Let A be an n × m matrix, whose columns Ai are independent, random vectors in Rn\nfrom N (⃗0,Σ). Then for any t ≥0, we have\n(\n√\nm −CK2(\n√\nn + t))\np\nsn(Σ) ≤sn(A) ≤(\n√\nm + CK2(\n√\nn + t))\np\nsn(Σ)\nand\ns1(A) ≤(\n√\nm + CK2(\n√\nn + t))\np\ns1(Σ)\nwith probability at least 1 −2exp(−t2). Here, K = maxi ∥A∥ψ2 (sub-Gaussian norm of A).\nProof. First, we prove the lower bound on sn(A). Note that sn(A) = min\n∥x∥>0\n∥Ax∥\n∥x∥, and that the\ncolumns of Σ−1\n2 A are distributed as N (⃗0,I). Therefore, we have the following.\nmin\n∥x∥>0\n∥Ax∥\n∥x∥= min\n∥x∥>0\n∥Σ\n1\n2 Σ−1\n2 Ax∥\n∥x∥\n= min\n∥x∥>0\n∥Σ\n1\n2 Σ−1\n2 Ax∥\n∥Σ−1\n2 Ax∥\n∥Σ−1\n2 Ax∥\n∥x∥\n≥min\n∥x∥>0\n∥Σ\n1\n2 Σ−1\n2 Ax∥\n∥Σ−1\n2 Ax∥\nmin\n∥x∥>0\n∥Σ−1\n2 Ax∥\n∥x∥\n≥min\n∥y∥>0\n∥Σ\n1\n2 y∥\n∥y∥\nmin\n∥x∥>0\n∥Σ−1\n2 Ax∥\n∥x∥\n≥(\n√\nm −CK2(\n√\nn + t))\np\nsn(Σ)\n(Lemma 2.6)\nNext, we prove the upper bound on sn(A). For this, we ﬁrst show that for X ∈Rm×d and Y ∈Rd×n,\nsmin(XY ) ≤smin(X) · ∥Y ∥.\nsmin(XY ) = min\n∥z∥=1∥XY z∥\n≤min\n∥z∥=1∥X∥∥Y z∥\n= ∥X∥· min\n∥z∥=1∥Y z∥\n= ∥X∥· smin(Y )\nNow, smin(XY ) = smin(Y T XT ) ≤∥Y ∥·smin(X) by the above reasoning. Using this results, we have\nthe following.\nsn(A) = sn(Σ1/2 · Σ−1/2A)\n9\n≤sn(Σ1/2)∥Σ−1/2A∥\n≤(\n√\nm + CK2(\n√\nn + t))\np\nsn(Σ)\n(Lemma 2.6)\nNow, we show the upper bound on s1(A). Note that s1(A) = ∥A∥.\n∥A∥= ∥Σ\n1\n2 Σ−1\n2 A∥\n≤∥Σ\n1\n2 ∥· ∥Σ−1\n2 A∥\n≤(\n√\nm + CK2(\n√\nn + t))\np\ns1(Σ)\n(Lemma 2.6)\nThis completes the proof.\nNow, we state a concentration inequality for χ2 random variables.\nLemma 2.8. Let X be a χ2 random variable with k degrees of freedom. Then,\nP\nh\nX > k + 2\n√\nkt + 2t\ni\n≤e−t.\nNext, we state the well-known Bernstein’s inequality for sums of independent Bernoulli\nrandom variables.\nLemma 2.9 (Bernstein’s Inequality). Let X1,...,Xm be independent Bernoulli random variables taking\nvalues in {0,1}. Let p = E[Xi]. Then for m ≥5p\n2ε2 ln(2/β) and ε ≤p/4,\nP\n\u0014\f\f\f\f\f\n1\nm\nX\nXi −p\n\f\f\f\f\f ≥ε\n\u0015\n≤2e−ε2m/2(p+ε) ≤β.\nWe ﬁnally state a result about the norm of a vector sampled from N (⃗0,I).\nLemma 2.10. Let X1,...,Xq ∼N (⃗0,Σ) be vectors in Rd, where Σ is the projection of Id×d on to a\nsubspace of Rd of rank k. Then\nP\nh\n∀i,∥Xi∥2 ≤k + 2\n√\nkt + 2t\ni\n≥1 −qe−t.\nProof. Since Σ is of rank k, we can directly use Lemma 2.8 for a ﬁxed i ∈[q], and the union\nbound over all i ∈[q] to get the required result. This is because for any i, ∥Xi∥2 is a χ2 random\nvariable with k degrees of freedom.\n2.2\nPrivacy Preliminaries\nDeﬁnition 2.11 (Differential Privacy (DP) [DMNS06]). A randomized algorithm M : X n →Y\nsatisﬁes (ε,δ)-differential privacy ((ε,δ)-DP) if for every pair of neighboring datasets X,X′ ∈X n\n(i.e., datasets that differ in exactly one entry),\n∀Y ⊆Y\nP[M(X) ∈Y ] ≤eε · P\u0002M(X′) ∈Y \u0003 + δ.\nWhen δ = 0, we say that M satisﬁes ε-differential privacy or pure differential privacy.\n10\nNeighbouring datasets are those that differ by the replacement of one individual’s data.\nIn our setting, each individual’s data is assumed to correspond to one point in X = Rd, so\nneighbouring means one point is changed arbitrarily.\nThroughout the document, we will assume that ε is smaller than some absolute constant less\nthan 1 for notational convenience, but note that our results still hold for general ε. Now, this\nprivacy deﬁnition is closed under post-processing.\nLemma 2.12 (Post Processing [DMNS06]). If M : X n →Y is (ε,δ)-DP, and P : Y →Z is any\nrandomized function, then the algorithm P ◦M is (ε,δ)-DP.\n2.3\nBasic Differentially Private Mechanisms.\nWe ﬁrst state standard results on achieving privacy via noise addition proportional to\nsensitivity [DMNS06].\nDeﬁnition 2.13 (Sensitivity). Let f : X n →Rd be a function, its ℓ1-sensitivity and ℓ2-sensitivity\nare\n∆f ,1 =\nmax\nX∼X′∈X n ∥f (X) −f (X′)∥1\nand\n∆f ,2 =\nmax\nX∼X′∈X n ∥f (X) −f (X′)∥2,\nrespectively. Here, X ∼X′ denotes that X and X′ are neighboring datasets (i.e., those that differ\nin exactly one entry).\nOne way of introducing (ε,δ)-differential privacy is via adding noise sampled from the\ntruncated Laplace distribution, proportional to the ℓ1 sensitivity.\nLemma 2.14 (Truncated Laplace Mechanism [GDGK20]). Deﬁne the probability density function\n(p) of the truncated Laplace distribution as follows.\np(x) =\n\nBe−|x|\nλ\nif x ∈[−A,A]\n0\notherwise\nIn the above,\nλ = ∆\nε ,\nA = ∆\nε log\n \n1 + eε −1\n2δ\n!\n,\nB =\n1\n2λ(1 −e−A\nλ )\n.\nLet TLap(∆,ε,δ) denote a draw from the above distribution.\nLet f : X n →Rd be a function with sensitivity ∆. Then the truncated Laplace mechanism\nM(X) = f (X) + TLap(∆,ε,δ)\nsatisﬁes (ε,δ)-DP.\nIn the above A ≤\n∆f ,1\nε log(1/δ) since ε is smaller than some absolute constant less than 1. Now,\nwe introduce differentially private histograms.\nLemma 2.15 (Private Histograms). Let n ∈N, ε,δ,β > 0, and X a set. There exists M : X n →RX\nwhich is (ε,δ)-differentially private and, for all x ∈X n, we have\nP\nM\n\nsup\ny∈X\n\f\f\f\f\fM(x)y −1\nn|{i ∈[n] : xi = y}|\n\f\f\f\f\f ≤O\n log(1/δβ)\nεn\n!\n≥1 −β.\n11\nThe above holds due to [BNS16; Vad17]. Finally, we introduce the GAP-MAX algorithm from\n[BDRS18] that outputs the element from the output space that has the highest score function,\ngiven that there is a signiﬁcant gap between the scores of the highest and the second to the\nhighest elements.\nLemma 2.16 (GAP-MAX Algorithm [BDRS18]). Let SCORE : X n × Y →R be a score function with\nsensitivity 1 in its ﬁrst argument, and let ε,δ > 0. Then there exists a (ε,δ)-differentially private\nalgorithm M : X n →Y and α = Θ(log(1/δ)/εn) with the following property. Fix an input X ∈X n. Let\ny∗= argmax\ny∈Y\n{SCORE(X,y)}.\nSuppose\n∀y ∈Y,y , y∗=⇒SCORE(X,y) < SCORE(X,y∗) −αn.\nThen M outputs y∗with probability 1.\n3\nExact case\nHere, we discuss the case, where all n points lie exactly in a subspace s∗of dimension k of\nRd. Our goal is to privately output that subspace. We do it under the assumption that all strict\nsubspaces of s∗contain at most ℓpoints. If the points are in general position, then ℓ= k−1, as any\nstrictly smaller subspace has dimension < k and cannot contain more points than its dimension.\nLet Sk\nd be the set of all k-dimensional subspaces of Rd. Let Sd be the set of all subspaces of Rd.\nWe formally deﬁne that problem as follows.\nProblem 3.1. Assume (i) all but at most ℓ, input points are in some s∗∈Sk\nd, and (ii) every\nsubspace of dimension < k contains at most ℓpoints. (If the points are in general position – aside\nfrom being contained in s∗– then ℓ= k −1.) The goal is to output a representation of s∗.\nWe call these ≤ℓpoints that do not lie in s∗, “adversarial points”. With the problem deﬁned\nin Problem 3.1, we will state the main theorem of this section.\nTheorem 3.2. For any ε,δ > 0, ℓ≥k −1 ≥0, and\nn ≥O\n \nℓ+ log(1/δ)\nε\n!\n,\nthere exists an (ε,δ)-DP algorithm M : Rd×n →Sk\nd, such that if X is a dataset of n points satisfying the\nconditions in Problem 3.1, then M(X) outputs a representation of s∗with probability 1.\nWe prove Theorem 3.2 by proving the privacy and the accuracy guarantees of Algorithm 1.\nThe algorithm performs a GAP-MAX (cf. Lemma 2.16). It assigns a score to all the relevant\nsubspaces, that is, the subspaces spanned by the points of the dataset X. We show that the only\nsubspace that has a high score is the true subspace s∗, and the rest of the subspaces have low\nscores. Then GAP-MAX outputs the true subspace successfully because of the gap between the\nscores of the best subspace and the second to the best one. For GAP-MAX to work all the time,\n12\nwe deﬁne a default option in the output space that has a high score, which we call NULL. Thus,\nthe output space is now Y = Sd ∪{NULL}. Also, for GAP-MAX to run in ﬁnite time, we ﬁlter Sd\nto select ﬁnite number of subspaces that have at least 0 scores on the basis of X. Note that this is\na preprocessing step, and does not violate privacy as, we will show, all other subspaces already\nhave 0 probability of getting output. We deﬁne the score function u : X n × Y →N as follows.\nu(x,s) :=\n\n|x ∩s| −sup{|x ∩t| : t ∈Sd,t ⊊s}\nif s ∈Sd\nℓ+ 4log(1/δ)\nε\n+ 1\nif s = NULL\nNote that this score function can be computed in ﬁnite time because for any m points and i > 0,\nif the points are contained in an i-dimensional subspace, then the subspace that contains all m\npoints must lie within the set of subspaces spanned by \u0000 m\ni+1\n\u0001 subsets of points.\nAlgorithm 1: DP Exact Subspace Estimator DPESEε,δ,k,ℓ(X)\nInput: Samples X ∈Rd×n. Parameters ε,δ,k,ℓ> 0.\nOutput: ˆs ∈Sk\nd.\nSet Y ←{NULL} and sample noise ξ(NULL) from TLap(2,ε,δ).\nSet score u(X,NULL) = ℓ+ 4log(1/δ)\nε\n+ 1.\n// Identify candidate outputs.\nFor each subset S of X of size k\nLet s be the subspace spanned by S.\nY ←Y ∪{s}.\nSample noise ξ(s) from TLap(2,ε,δ).\nSet score u(X,s) = |x ∩s| −sup{|x ∩t| : t ∈Sd,t ⊊s}.\n// Apply GAP-MAX.\nLet s1 = argmaxs∈Y u(X,s) be the candidate with the largest score.\nLet s2 = argmaxs∈Y\\{s1} u(X,s) be the candidate with the second-largest score.\nLet ˆs = argmaxs∈Y max{0,u(X,s) −u(X,s2) −1} + ξ(s).\n// Truncated Laplace noise ξ ∼TLap(2,ε,δ); see Lemma 2.14\nReturn ˆs.\nWe split the proof of Theorem 1.1 into sections for privacy (Lemma 3.3) and accuracy\n(Lemma 3.5).\n3.1\nPrivacy\nLemma 3.3. Algorithm 1 is (ε,δ)-differentially private.\nThe proof of Lemma 3.3 closely follows the privacy analysis of GAP-MAX by [BDRS18]. The\nonly novelty is that Algorithm 1 may output NULL in the case that the input is malformed (i.e.,\ndoesn’t satisfy the assumptions of Problem 3.1).\nThe key is that the score u(X,s) is low sensitivity. Thus max{0,u(X,s) −u(X,s2) −1} also\nhas low sensitivity. What we gain from subtracting the second-largest score and taking this\n13\nmaximum is that these values are also sparse – only one (s = s1) is nonzero. This means we can\nadd noise to all the values without paying for composition. We now prove Lemma 3.3.\nProof. First, we argue that the sensitivity of u is 1. The quantity |X ∩s| has sensitivity 1 and so\ndoes sup{|X ∩t| : t ∈Sd,t ⊊s}. This implies sensitivity 2 by the triangle inequality. However, we\nsee that it is not possible to change one point that simultaneously increases |X ∩s| and decreases\nsup{|X ∩t| : t ∈Sd,t ⊊s} or vice versa. Thus the sensitivity is actually 1.\nWe also argue that u(X,s2) has sensitivity 1, where s2 is the candidate with the second-largest\nscore. Observe that the second-largest score is a monotone function of the collection of all scores\n– i.e., increasing scores cannot decrease the second-largest score and vice versa. Changing one\ninput point can at most increase all the scores by 1, which would only increase the second-largest\nscore by 1.\nThis implies that max{0,u(X,s) −u(X,s2) −1} has sensitivity 2 by the triangle inequality and\nthe fact that the maximum does not increase the sensitivity.\nNow we observe that for any input X there is at most one s such that max{0,u(X,s)−u(X,s2)−\n1} , 0, namely s = s1. We can say something even stronger: Let X and X′ be neighbouring\ndatasets with s1 and s2 the largest and second-largest scores on X and s′\n1 and s′\n2 the largest and\nsecond-largest scores on X′. Then there is at most one s such that max{0,u(X,s)−u(X,s2)−1} , 0\nor max{0,u(X′,s) −u(X′,s′\n2) −1} , 0. In other words, we cannot have both u(X,s1) −u(X,s2) > 1\nand u(X′,s′\n1) −u(X′,s′\n2) > 1 unless s1 = s′\n1. This holds because u(X,s) −u(X,s2) has sensitivity 2.\nWith these observations in hand, we can delve into the privacy analysis. Let X and X′ be\nneighbouring datasets with s1 and s2 the largest and second-largest scores on X and s′\n1 and s′\n2\nthe largest and second-largest scores on X′. Let Y be the set of candidates from X and let Y′ be\nthe set of candidates from X′. Let ˇY = Y ∪Y′ and ˆY = Y ∩Y′.\nWe note that, for s ∈ˇY, if u(X,s) ≤ℓ, then there is no way that ˆs = s. This is because\n|ξ(s)| ≤2log(1/δ)\nε\nfor all s and hence, there is no way we could have argmaxs∈Y max{0,u(X,s) −\nu(X,s2) −1} + ξ(s) ≥argmaxs∈Y max{0,u(X,NULL) −u(X,s2) −1} + ξ(NULL).\nIf s ∈ˇY \\ ˆY, then u(X,s) ≤|X ∩s| ≤k + 1 ≤ℓand u(X′,s) ≤ℓ. This is because s < ˆY implies\n|X ∩s| < k or |X′ ∩s| < k, but |X ∩s| ≤|X′ ∩s| + 1. Thus, there is no way these points are output\nand, hence, we can ignore these points in the privacy analysis. (This is the reason for adding the\nNULL candidate.)\nNow we argue that the entire collection of noisy values max{0,u(X,s) −u(X,s2) −1} + ξ(s)\nfor s ∈ˆY is differentially private. This is because we are adding noise to a vector where (i) on\nthe neighbouring datasets only 1 coordinate is potentially different and (ii) this coordinate has\nsensitivity 2.\n3.2\nAccuracy\nWe start by showing that the true subspace s∗has a high score, while the rest of the subspaces\nhave low scores.\nLemma 3.4. Under the assumptions of Problem 3.1, u(x,s∗) ≥n −2ℓand u(x,s′) ≤2ℓfor s′ , s∗.\nProof. We have u(x,s∗) = |x ∩s∗| −|x ∩s′| for some s′ ∈Sd with s′ ⊊s∗. The dimension of s′ is at\nmost k −1 and, by the assumption (ii), |x ∩s′| ≤ℓ.\n14\nLet s′ ∈Sd \\ {s∗}. There are three cases to analyse:\n1. Let s′ ⊋s∗. Then u(x,s′) ≤|x ∩s′| −|x ∩s∗| ≤ℓbecause the ≤ℓadverserial points and the\n≥n −ℓnon-adversarial points may not together lie in a subspace of dimension k.\n2. Let s′ ⊊s∗. Let k′ be the dimension of s′. Clearly k′ < k. By our assumption (ii), |s′ ∩x| ≤ℓ.\nThen u(x,s′) = |x∩s′|−|x∩t| ≤ℓfor some t because the ≤ℓadversarial points already don’t\nlie in s∗, so they will not lie in any subspace of s∗.\n3. Let s′ be incomparable to s∗. Let s′′ = s′ ∩s∗. Then u(x,s′) ≤|x ∩s′| −|x ∩s′′| ≤ℓbecause the\nadversarial points may not lie in s∗, but could be in s′ \\ s′′.\nThis completes the proof.\nNow, we show that the algorithm is accurate.\nLemma 3.5. If n ≥3ℓ+ 8log(1/δ)\nε\n+ 2, then Algorithm 1 outputs s∗for Problem 3.1.\nProof. From Lemma 3.4, we know that s∗has a score of at least n−2ℓ, and the next best subspace\ncan have a score of at most ℓ. Also, the score of NULL is deﬁned to be ℓ+ 4log(1/δ)\nε\n+ 1. This\nmeans that the gap satisﬁes max{0,u(X,s∗)−u(X,s2)−1} ≥n−3ℓ−4log(1/δ)\nε\n−1. Since the noise is\nbounded by 2log(1/δ)\nε\n, our bound on n implies that ˆs = s∗\n3.3\nLower Bound\nHere, we show that our upper bound is optimal up to constants for the exact case.\nTheorem 3.6. Any (ε,δ)-DP algorithm that takes a dataset of n points satisfying the conditions in\nProblem 3.1 and outputs s∗with probability > 0.5 requires n ≥Ω\n\u0010\nℓ+ log(1/δ)\nε\n\u0011\n.\nProof. First, n ≥ℓ+k. This is because we need at least k points to span the subspace, and ℓpoints\ncould be corrupted. Second, n ≥Ω(log(1/δ)/ε) by group privacy. Otherwise, the algorithm\nis (10,0.1)-differentially private with respect to changing the entire dataset and it is clearly\nimpossible to output the subspace under this condition.\n4\nApproximate Case\nIn this section, we discuss the case, where the data “approximately” lies in a k-dimensional\nsubspace of Rd. We make a Gaussian distributional assumption, where the covariance is\napproximately k-dimensional, though the results could be extended to distributions with heavier\ntails using the right inequalities. We formally deﬁne the problem:\nProblem 4.1. Let Σ ∈Rd×d be a symmetric, PSD matrix of rank ≥k ∈{1,...,d}, and let 0 < γ ≪1,\nsuch that λk+1\nλk ≤γ2. Suppose Π is the projection matrix corresponding to the subspace spanned\nby the eigenvectors of Σ corresponding to the eigenvalues λ1,...,λk. Given sample access to\nN (⃗0,Σ), and 0 < α < 1, output a projection matrix bΠ, such that ∥Π −bΠ∥≤α.\n15\nWe solve Problem 4.1 under the constraint of (ε,δ)-differential privacy. Throughout this\nsection, we would refer to the subspace spanned by the top k eigenvectors of Σ as the “true” or\n“actual” subspace.\nAlgorithm 2 solves Problem 4.1 and proves Theorem 1.2. Here ∥· ∥is the operator norm.\nRemark 4.2. We scale the eigenvalues of Σ so that λk = 1 and λk+1 ≤γ2. Also, for the purpose\nof the analysis, we will be splitting Σ = Σk + Σd−k, where Σk is the covariance matrix formed by\nthe top k eigenvalues and the corresponding eigenvectors of Σ and Σd−k is remainder.\nAlso, we assume the knowledge of γ (or an upper bound on γ). Our solution is presented in\nAlgorithm 2. The following theorem is the main result of the section.\nTheorem 4.3. Let Σ ∈Rd×d be an arbitrary, symmetric, PSD matrix of rank ≥k ∈{1,...,d}, and let\n0 < γ < 1. Suppose Π is the projection matrix corresponding to the subspace spanned by the vectors of\nΣk. Then given\nγ2 ∈O\n \nεα2n\nd2k ln(1/δ) · min\n(1\nk ,\n1\nln(k ln(1/δ)/ε)\n)!\n,\nsuch that λk+1(Σ) ≤γ2λk(Σ), for every ε,δ > 0, and 0 < α < 1, there exists and (ε,δ)-DP algorithm that\ntakes\nn ≥O\n k log(1/δ)\nε\n+ log(1/δ)log(log(1/δ)/ε)\nε\n!\nsamples from N (⃗0,Σ), and outputs a projection matrix bΠ, such that ∥Π −bΠ∥≤α with probability at\nleast 0.7.\nAlgorithm 2 is a type of “Subsample-and-Aggregate” algorithm [NRS07]. Here, we consider\nmultiple subspaces formed by the points from the same Gaussian, and privately ﬁnd a subspace\nthat is close to all those subspaces. Since the subspaces formed by the points would be close to\nthe true subspace, the privately found subspace would be close to the true subspace.\nA little more formally, we ﬁrst sample q public data points (called “reference points”) from\nN (⃗0,I). Next, we divide the original dataset X into disjoint datasets of m samples each, and\nproject all reference points on the subspaces spanned by every subset. Now, for every reference\npoint, we do the following. We have t = n\nm projections of the reference point. Using DP histogram\nover Rd, we aggregate those projections in the histogram cells; with high probability all those\nprojections will be close to one another, so they would lie within one histogram cell. We output\na random point from the histogram cell corresponding to the reference point. With a total of q\npoints output in this way, we ﬁnally output the projection matrix spanned by these points. In\nthe algorithm C0, C1, and C2 are universal constants.\nWe divide the proof of Theorem 4.3 into two parts: privacy (Lemma 4.4) and accuracy\n(Lemma 4.9).\n4.1\nPrivacy\nWe analyse the privacy by understanding the sensitivities at the only sequence of steps invok-\ning a differentially private mechanism, that is, the sequence of steps involving DP-histograms.\n16\nAlgorithm 2: DP Approximate Subspace Estimator DPASEε,δ,α,γ,k(X)\nInput: Samples X1,...,Xn ∈Rd. Parameters ε,δ,α,γ,k > 0.\nOutput: Projection matrix bΠ ∈Rd×d of rank k.\nSet parameters: t ←C0 ln(1/δ)\nε\nm ←⌊n/t⌋\nq ←C1k\nℓ←\nC2γ\n√\ndk(\n√\nk+√\nln(kt))\n√m\nSample reference points p1,...,pq from N (⃗0,I) independently.\n// Subsample from X, and form projection matrices.\nFor j ∈1,...,t\nLet Xj = (X(j−1)m+1,...,Xjm) ∈Rd×m.\nLet Πj ∈Rd×d be the projection matrix onto the subspace spanned by the\neigenvectors of Xj(Xj)T ∈Rd×d corresponding to the largest k eigenvalues.\nFor i ∈1,...,q\npj\ni ←Πjpi\n// Create histogram cells with random offset.\nLet λ be a random number in [0,1).\nDivide Rqd into Ω= {...,[λℓ+ iℓ,λℓ+ (i + 1)ℓ),...}qd, for all i ∈Z.\nLet each disjoint cell of length ℓbe a histogram bucket.\n// Perform private aggregation of subspaces.\nFor each i ∈[q], let Qi ∈Rd×t be the dataset, where column j is pj\ni.\nLet Q ∈Rqd×t be the vertical concatenation of all Qi’s in order.\nRun (ε,δ)-DP histogram over Ωusing Q to get ω ∈Ωthat contains at least t\n2 points.\nIf no such ω exists\nReturn ⊥\n// Return the subspace.\nLet bp = (bp1,...,bpd,...,bp(q−1)d+1,...,bpqd) be a random point in ω.\nFor each i ∈[q]\nLet bpi = (bp(i−1)d+1,...,bpid).\nLet bΠ be the projection matrix of the top-k subspace of (bp1,...,bpq).\nReturn bΠ.\nLemma 4.4. Algorithm 2 is (ε,δ)-differentially private.\nProof. Changing one point in X can change only one of the Xj’s. This can only change one\npoint in Q, which in turn can only change the counts in two histogram cells by 1. Therefore,\nthe sensitivity is 2. Because the sensitivity of the histogram step is bounded by 2 (Lemma 4.4),\nan application of DP-histogram, by Lemma 2.15, is (ε,δ)-DP. Outputting a random point in the\nprivately found histogram cell preserves privacy by post-processing (Lemma 2.12). Hence, the\nclaim.\n17\n4.2\nAccuracy\nNow we delve into the utility analysis of the algorithm. For 1 ≤j ≤t, let Xj be the subsets of\nX as deﬁned in Algorithm 2, and Πj be the projection matrices of their respective subspaces. We\nnow show that Πj and the projection matrix of the subspace spanned by Σk are close in operator\nnorm.\nLemma 4.5. Let Π be the projection matrix of the subspace spanned by the vectors of Σk, and for each\n1 ≤j ≤t, let Πj be the projection matrix as deﬁned in Algorithm 2. If m ≥O(k + ln(qt)), then\nP\n\n∀j,∥Π −Πj∥≤O\n\n\nγ\n√\nd\n√m\n\n\n\n≥0.95\nProof. We show that the subspaces spanned by Xj and the true subspace spanned by Σ are\nclose. Formally, we invoke Lemmata 2.3 and 2.4. This closeness follows from standard matrix\nconcentration inequalities.\nFix a j ∈[t]. Note that Xj can be written as Y j + H, where Y j is the matrix of vectors\ndistributed as N (⃗0,Σk), and H is a matrix of vectors distributed as N (⃗0,Σd−k), where Σk and\nΣd−k are deﬁned as in Remark 4.2. By Corollary 2.7, with probability at least 1 −0.02\nt , sk(Y j) ∈\nΘ((√m +\n√\nk)(\np\nsk(Σk))) = Θ(√m +\n√\nk) > 0. Therefore, the subspace spanned by Y j is the same as\nthe subspace spanned by Σk. So, it sufﬁces to look at the subspace spanned by Y j.\nNow, by Corollary 2.7, we know that with probability at least 1 −0.02\nt , ∥Xj −Y j∥= ∥H∥≤\nO((√m +\n√\nd)\np\ns1(Σd−k)) ≤O(γ(√m +\n√\nd)\np\nsk(Σk)) ≤O(γ(√m +\n√\nd)).\nWe wish to invoke Lemma 2.3. Let UDV T be the SVD of Y j, and let ˆU ˆD ˆV T be the SVD of\nXj. Now, for a matrix M, let ΠM denote the projection matrix of the subspace spanned by the\ncolumns of M. Deﬁne quantities a,b,z12,z21 as follows.\na = smin(UT XjV )\n= smin(UT Y jV + UT HV )\n= smin(UT Y jV )\n(Columns of U are orthogonal to columns of H)\n= sk(Y j)\n∈Θ(\n√\nm +\n√\nk)\n∈Θ(\n√\nm)\nb = ∥UT\n⊥XjV⊥∥\n= ∥UT\n⊥Y jV⊥+ UT\n⊥HV⊥∥\n= ∥UT\n⊥HV⊥∥\n(Columns of U⊥are orthogonal to columns of Y j)\n≤∥H∥\n≤O(γ(\n√\nm +\n√\nd))\nz12 = ∥ΠUHΠV⊥∥\n= 0\nz21 = ∥ΠU⊥HΠV ∥\n= ∥ΠU⊥Σ1/2\nd−k(Σ−1/2\nd−k H)ΠV ∥\n18\nNow, in the above, Σ−1/2\nd−k H ∈Rd×m, such that each of its entry is an independent sample\nfrom N (0,1). Right-multiplying it by ΠV makes it a matrix in a k-dimensional subspace\nof Rm, such that each row is an independent vector from a spherical Gaussian. Using Corol-\nlary 2.7, ∥Σ−1/2\nd−k H∥≤O(\n√\nd +\n√\nk) ≤O(\n√\nd) with probability at least 1 −0.01\nt . Also, ∥ΠU⊥Σ1/2\nd−k∥≤\nO(γ\np\nsk(Σk)) ≤O(γ). This gives us:\nz21 ≤O(γ\n√\nd).\nSince a2 > 2b2, we get the following by Lemma 2.3.\n∥Sin(Θ)(U, ˆU)∥≤\naz21 + bz12\na2 −b2 −min{z2\n12,z2\n21}\n≤O\n\n\nγ\n√\nd\n√m\n\n\nTherefore, using Lemma 2.4, and applying the union bound over all j, we get the required\nresult.\nLet ξ = O\n\u0012\nγ\n√\nd\n√m\n\u0013\n. We show that the projections of any reference point are close.\nCorollary 4.6. Let p1,...,pq be the reference points as deﬁned in Algorithm 2, and let Π and Πj (for\n1 ≤j ≤t) be projections matrices as deﬁned in Lemma 4.5. Then\nP\nh\n∀i,j,∥(Π −Πj)pi∥≤O(ξ(\n√\nk +\np\nln(qt)))\ni\n≥0.9.\nProof. We know from Lemma 4.5 that ∥Π −Πj∥≤ξ for all j with probability at least 0.95. For\nj ∈[t], let bΠj be the projection matrix for the union of the jth subspace and the subspace spanned\nby Σk. Lemma 2.10 implies that with probability at least 0.95, for all i,j, ∥bΠjpi∥≤O(\n√\nk+\np\nln(qt)).\nTherefore,\n∥(Π −Πj)pi∥= ∥(Π −Πj)bΠjpi∥≤∥Π −Πj∥· ∥bΠjpi∥≤O(ξ(\n√\nk +\np\nln(qt))).\nHence, the claim.\nThe above corollary shows that the projections of each reference point lie in a ball of radius\nO(ξ\n√\nk). Next, we show that for each reference point, all the projections of the point lie inside\na histogram cell with high probability. For notational convenience, since each point in Q is a\nconcatenation of the projection of all reference points on a given subspace, for all i,j, we refer\nto (0,...,0,Qj\n(i−1)d+1,...,Qj\nid,0,...,0) ∈Rqd (where there are (i −1)d zeroes behind Qj\n(i−1)d+1, and\n(q −i)d zeroes after Qj\nid) as pj\ni.\nLemma 4.7. Let ℓand λ be the length of a histogram cell and the random offset respectively, as deﬁned\nin Algorithm 2. Then\nP[|ω ∩Q| = t] ≥0.8.\nThus there exists ω ∈Ωthat, such that all points in Q lie within ω.\n19\nProof. Let r = O(ξ(\n√\nk +\np\nln(qt))). This implies that ℓ= 20r√q. The random offset could also be\nviewed as moving along a diagonal of a cell by λℓ\np\ndq. We know that with probability at least\n0.8, for each i, all projections of reference point pi lie in a ball of radius r. This means that all the\npoints in Q lie in a ball of radius r√q. Then\nP[|ω ∩Q| = t] ≤P\n\u0014 1\n20 ≥λ ∨λ ≥19\n20\n\u0015\n= 1\n10.\nTaking the union bound over all q and the failure of the event in Corollary 4.6, we get the\nclaim.\nNow, we analyse the sample complexity due to the private algorithm, that is, DP-histograms.\nLemma 4.8. Let ω be the histogram cell as deﬁned in Algorithm 2. Suppose Count(ω) is the noisy\ncount of ω as a result of applying the private histogram. If t ≥O\n\u0010log(1/δ)\nε\n\u0011\n, then\nP\n\u0014\n|Count(ω)| ≥t\n2\n\u0015\n≥0.75.\nProof. Lemma 4.7 implies that with probability at least 0.8, for each i, all projections of pi lie in a\nhistogram cell, that is, all points of Q lie in a histogram cell in Ω. Because of the error bound in\nLemma 2.15 and our bound on t, we see at least t\n2 points in that cell with probability at least\n1 −0.05. Therefore, by taking the union bound, the proof is complete.\nWe ﬁnally show that the error of the projection matrix that is output by Algorithm 2 is small.\nLemma 4.9. Let bΠ be the projection matrix as deﬁned in Algorithm 2, and n be the total number of\nsamples. If\nγ2 ∈O\n \nεα2n\nd2k ln(1/δ) · min\n(1\nk ,\n1\nln(k ln(1/δ)/ε)\n)!\n,\nn ≥O(k log(1/δ)\nε\n+ ln(1/δ)ln(ln(1/δ)/ε)\nε\n), and q ≥O(k) the with probability at least 0.7, ∥bΠ −Π∥≤α.\nProof. For each i ∈[q], let p∗\ni be the projection of pi on to the subspace spanned by Σk, bpi be as\ndeﬁned in the algorithm, and pj\ni be the projection of pi on to the subspace spanned by the jth\nsubset of X. From Lemma 4.8, we know that all pj\ni ’s are contained in a histogram cell of length ℓ.\nThis implies that p∗\ni is also contained within the same histogram cell.\nNow, let P = (p∗\n1,...,p∗\nq) and bP = (bp1,...,bpq). Then by above, bP = P + E, where ∥E∥F ≤2ℓ\np\ndq.\nTherefore, ∥E∥≤2ℓ\np\ndq. Let E = EP + EP , where EP is the component of E in the subspace\nspanned by P , and EP be the orthogonal component. Let P ′ = P + EP . We will be analysing bP\nwith respect to P ′.\nNow, with probability at least 0.95, sk(P ) ∈Θ(\n√\nk) due to our choice of q and using Corol-\nlary 2.7, and sk+1(P ) = 0. So, sk+1(P ′) = 0 because EP is in the same subspace as P . Now, using\nLemma 2.2, we know that sk(P ′) ≥sk(P ) −∥EP ∥≥Ω(\n√\nk) > 0. This means that P ′ has rank k, so\nthe subspaces spanned by Σk and P ′ are the same.\nAs before, we will try to bound the distance between the subspaces spanned by P ′ and bP .\nNote that using Lemma 2.1, we know that sk(P ′) ≤sk(P ) + ∥EP ∥≤O(\n√\nk).\n20\nWe wish to invoke Lemma 2.3 again. Let UDV T be the SVD of P ′, and let ˆU ˆD ˆV T be the SVD\nof bP . Now, for a matrix M, let ΠM denote the projection matrix of the subspace spanned by the\ncolumns of M. Deﬁne quantities a,b,z12,z21 as follows.\na = smin(UT bP V )\n= smin(UT P ′V + UT EP V )\n= smin(UT P ′V )\n(Columns of U are orthogonal to columns of EP )\n= sk(P ′)\n∈Θ(\n√\nk)\nb = ∥UT\n⊥bP V⊥∥\n= ∥UT\n⊥P ′V⊥+ UT\n⊥EP V⊥∥\n= ∥UT\n⊥EP V⊥∥\n(Columns of U⊥are orthogonal to columns of P ′)\n≤∥EP ∥\n≤O(ℓ\np\ndq)\nz12 = ∥ΠUEP ΠV⊥∥\n= 0\nz21 = ∥ΠU⊥EP ΠV ∥\n≤∥EP ∥\n≤O(ℓ\np\ndq)\nUsing Lemma 2.3, we get the following.\n∥Sin(Θ)(U, ˆU)∥≤\naz21 + bz12\na2 −b2 −min{z2\n12,z2\n21}\n≤O\n\u0010\nℓ\n√\ndk\n\u0011\n≤α\nThis completes our proof.\n4.3\nBoosting\nIn this subsection, we discuss boosting of error guarantees of Algorithm 2. The approach we\nuse is very similar to the well-known Median-of-Means method: we run the algorithm multiple\ntimes, and choose an output that is close to all other “good” outputs. We formalise this in\nAlgorithm 3.\nNow, we present the main result of this subsection.\nTheorem 4.10. Let Σ ∈Rd×d be an arbitrary, symmetric, PSD matrix of rank ≥k ∈{1,...,d}, and let\n0 < γ < 1. Suppose Π is the projection matrix corresponding to the subspace spanned by the vectors of\nΣk. Then given\nγ2 ∈O\n \nεα2n\nd2k ln(1/δ) · min\n(1\nk ,\n1\nln(k ln(1/δ)/ε)\n)!\n,\n21\nAlgorithm 3: DP Approximate Subspace Estimator Boosted DPASEBε,δ,α,β,γ,k(X)\nInput: Samples X1,...,Xn ∈Rd. Parameters ε,δ,α,β,γ,k > 0.\nOutput: Projection matrix bΠ ∈Rd×d of rank k.\nSet parameters: t ←C3 log(1/β)\nm ←⌊n/t⌋\nSplit X into t datasets of size m: X1,...,Xt.\n// Run DPASE t times to get multiple projection matrices.\nFor i ←1,...,t\nbΠi ←DPASEε,δ,α,γ,k(Xi)\n// Select a good subspace.\nFor i ←1,...,t\nci ←0\nFor j ∈[t] \\ {i}\nIf ∥bΠi −bΠj∥≤2α\nci ←ci + 1\nIf ci ≥0.6t −1\nReturn bΠi.\n// If there were not enough good subspaces, return ⊥.\nReturn ⊥.\nsuch that λk+1(Σ) ≤γ2λk(Σ), for every ε,δ > 0, and 0 < α,β < 1, there exists and (ε,δ)-DP algorithm\nthat takes\nn ≥O\n k log(1/δ)log(1/β)\nε\n+ log(1/δ)log(log(1/δ)/ε)log(1/β)\nε\n!\nsamples from N (⃗0,Σ), and outputs a projection matrix bΠ, such that ∥Π −bΠ∥≤α with probability at\nleast 1 −β.\nProof. Privacy holds trivially by Theorem 4.3.\nWe know by Theorem 4.3 that for each i, with probability at least 0.7, ∥bΠi −Π∥≤α. This\nmeans that by Lemma 2.9, with probability at least 1 −β, at least 0.6t of all the computed\nprojection matrices are accurate.\nThis means that there has to be at least one projection matrix that is close to 0.6t −1 > 0.5t of\nthese accurate projection matrices. So, the algorithm cannot return ⊥.\nNow, we want to argue that the returned projection matrix is accurate, too. Any projection\nmatrix that is close to at least 0.6t −1 projection matrices must be close to at least one accurate\nprojection matrix (by pigeonhole principle). Therefore, by triangle inequality, it will be close to\nthe true subspace. Therefore, the returned projection matrix is also accurate.\n22\nReferences\n[ABU18]\nR. Arora, V. Braverman, and J. Upadhyay. “Differentially private robust low-\nrank approximation”. In: Advances in neural information processing systems\n(2018).\n[ACGMMTZ16]\nM. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,\nand L. Zhang. “Deep learning with differential privacy”. In: Proceedings of the\n2016 ACM SIGSAC conference on computer and communications security. 2016,\npp. 308–318.\n[ADKMV18]\nK. Amin, T. Dick, A. Kulesza, A. M. Medina, and S. Vassilvitskii. “Private\ncovariance estimation via iterative eigenvector sampling”. In: 2018 NIPS\nworkshop in Privacy-Preserving Machine Learning. Vol. 250. 2018.\n[BBDS12]\nJ. Blocki, A. Blum, A. Datta, and O. Sheffet. “The Johnson-Lindenstrauss Trans-\nform Itself Preserves Differential Privacy”. In: Proceedings of the 53rd Annual\nIEEE Symposium on Foundations of Computer Science. FOCS ’12. Washington,\nDC, USA: IEEE Computer Society, 2012, pp. 410–419.\n[BBNS19]\nJ. Błasiok, M. Bun, A. Nikolov, and T. Steinke. “Towards instance-optimal\nprivate query release”. In: Proceedings of the Thirtieth Annual ACM-SIAM\nSymposium on Discrete Algorithms. SIAM. 2019, pp. 2480–2497.\n[BCMNUW20]\nR. Bassily, A. Cheu, S. Moran, A. Nikolov, J. Ullman, and S. Wu. “Private\nquery release assisted by public data”. In: International Conference on Machine\nLearning. PMLR. 2020, pp. 695–703.\n[BDMN05]\nA. Blum, C. Dwork, F. McSherry, and K. Nissim. “Practical Privacy: The\nSuLQ Framework”. In: Proceedings of the 24th ACM SIGMOD-SIGACT-SIGART\nSymposium on Principles of Database Systems. PODS ’05. New York, NY, USA:\nACM, 2005, pp. 128–138.\n[BDRS18]\nM. Bun, C. Dwork, G. N. Rothblum, and T. Steinke. “Composable and Versatile\nPrivacy via Truncated CDP”. In: Proceedings of the 50th Annual ACM Symposium\non the Theory of Computing. STOC ’18. New York, NY, USA: ACM, 2018, pp. 74–\n86.\n[BLR08]\nA. Blum, K. Ligett, and A. Roth. “A Learning Theory Approach to Non-\nInteractive Database Privacy”. In: STOC. 2008.\n[BNS16]\nM. Bun, K. Nissim, and U. Stemmer. “Simultaneous Private Learning of Multi-\nple Concepts”. In: Proceedings of the 7th Conference on Innovations in Theoretical\nComputer Science. ITCS ’16. New York, NY, USA: ACM, 2016, pp. 369–380.\n[BS16]\nM. Bun and T. Steinke. “Concentrated Differential Privacy: Simpliﬁcations,\nExtensions, and Lower Bounds”. In: Proceedings of the 14th Conference on Theory\nof Cryptography. TCC ’16-B. Berlin, Heidelberg: Springer, 2016, pp. 635–658.\n23\n[BUV14]\nM. Bun, J. Ullman, and S. Vadhan. “Fingerprinting Codes and the Price of\nApproximate Differential Privacy”. In: Proceedings of the 46th Annual ACM\nSymposium on the Theory of Computing. STOC ’14. New York, NY, USA: ACM,\n2014, pp. 1–10.\n[CSS12]\nK. Chaudhuri, A. Sarwate, and K. Sinha. “Near-optimal differentially private\nprincipal components”. In: Advances in Neural Information Processing Systems\n25 (2012), pp. 989–997.\n[CZ16]\nT. Cai and A. Zhang. “Rate-Optimal Perturbation Bounds for Singular Sub-\nspaces with Applications to High-Dimensional Statistics”. In: The Annals of\nStatistics 46 (May 2016).\n[DMNS06]\nC. Dwork, F. McSherry, K. Nissim, and A. Smith. “Calibrating Noise to Sensi-\ntivity in Private Data Analysis”. In: Proceedings of the 3rd Conference on Theory\nof Cryptography. TCC ’06. Berlin, Heidelberg: Springer, 2006, pp. 265–284.\n[DSSUV15]\nC. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. “Robust Traceability\nfrom Trace Amounts”. In: Proceedings of the 56th Annual IEEE Symposium\non Foundations of Computer Science. FOCS ’15. Washington, DC, USA: IEEE\nComputer Society, 2015, pp. 650–669.\n[DTTZ14]\nC. Dwork, K. Talwar, A. Thakurta, and L. Zhang. “Analyze Gauss: Optimal\nBounds for Privacy-Preserving Principal Component Analysis”. In: Proceed-\nings of the 46th Annual ACM Symposium on the Theory of Computing. STOC ’14.\nNew York, NY, USA: ACM, 2014, pp. 11–20.\n[FT20]\nY. Feng and Y. Tu. “How neural networks ﬁnd generalizable solutions: Self-\ntuned annealing in deep learning”. In: arXiv preprint arXiv:2001.01678 (2020).\n[GARD18]\nG. Gur-Ari, D. A. Roberts, and E. Dyer. “Gradient descent happens in a tiny\nsubspace”. In: arXiv preprint arXiv:1812.04754 (2018).\n[GDGK20]\nQ. Geng, W. Ding, R. Guo, and S. Kumar. “Tight Analysis of Privacy and\nUtility Tradeoff in Approximate Differential Privacy”. In: Proceedings of the\nTwenty Third International Conference on Artiﬁcial Intelligence and Statistics. Ed.\nby S. Chiappa and R. Calandra. Vol. 108. Proceedings of Machine Learning\nResearch. PMLR, 2020, pp. 89–99.\n[GGB18]\nA. Gonem and R. Gilad-Bachrach. “Smooth Sensitivity Based Approach for\nDifferentially Private PCA”. In: Algorithmic Learning Theory. ALT ’18. JMLR,\nInc., 2018, pp. 438–450.\n[HP13]\nM. Hardt and E. Price. “The noisy power method: A meta algorithm with\napplications”. In: arXiv preprint arXiv:1311.2495 (2013).\n[HR10]\nM. Hardt and G. N. Rothblum. “A multiplicative weights mechanism for\nprivacy-preserving data analysis”. In: 2010 IEEE 51st Annual Symposium on\nFoundations of Computer Science. IEEE. 2010, pp. 61–70.\n24\n[HR12]\nM. Hardt and A. Roth. “Beating randomized response on incoherent matrices”.\nIn: Proceedings of the forty-fourth annual ACM symposium on Theory of computing.\n2012, pp. 1255–1268.\n[HR13]\nM. Hardt and A. Roth. “Beyond worst-case analysis in private singular vector\ncomputation”. In: Proceedings of the forty-ﬁfth annual ACM symposium on Theory\nof computing. 2013, pp. 331–340.\n[HT10]\nM. Hardt and K. Talwar. “On the Geometry of Differential Privacy”. In:\nProceedings of the 42nd Annual ACM Symposium on the Theory of Computing.\nSTOC ’10. New York, NY, USA: ACM, 2010, pp. 705–714.\n[KRRT20]\nP. Kairouz, M. Ribero, K. Rush, and A. Thakurta. Fast Dimension Independent\nPrivate AdaGrad on Publicly Estimated Subspaces. 2020.\n[KT13]\nM. Kapralov and K. Talwar. “On Differentially Private Low Rank Approxi-\nmation”. In: Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete\nAlgorithms. SODA ’13. Philadelphia, PA, USA: SIAM, 2013, pp. 1395–1414.\n[LFLY18]\nC. Li, H. Farkhoor, R. Liu, and J. Yosinski. “Measuring the intrinsic dimension\nof objective landscapes”. In: arXiv preprint arXiv:1804.08838 (2018).\n[LGZCB20]\nX. Li, Q. Gu, Y. Zhou, T. Chen, and A. Banerjee. “Hessian based analysis of\nsgd for deep nets: Dynamics and generalization”. In: Proceedings of the 2020\nSIAM International Conference on Data Mining. SIAM. 2020, pp. 190–198.\n[LXTSG17]\nH. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. “Visualizing the loss\nlandscape of neural nets”. In: arXiv preprint arXiv:1712.09913 (2017).\n[MM09]\nF. McSherry and I. Mironov. “Differentially private recommender systems:\nBuilding privacy into the netﬂix prize contenders”. In: Proceedings of the 15th\nACM SIGKDD international conference on Knowledge discovery and data mining.\n2009, pp. 627–636.\n[MT07]\nF. McSherry and K. Talwar. “Mechanism Design via Differential Privacy”. In:\nProceedings of the 48th Annual IEEE Symposium on Foundations of Computer Sci-\nence. FOCS ’07. Washington, DC, USA: IEEE Computer Society, 2007, pp. 94–\n103.\n[NRS07]\nK. Nissim, S. Raskhodnikova, and A. Smith. “Smooth Sensitivity and Sam-\npling in Private Data Analysis”. In: Proceedings of the 39th Annual ACM Sympo-\nsium on the Theory of Computing. STOC ’07. New York, NY, USA: ACM, 2007,\npp. 75–84.\n[NS06]\nA. Narayanan and V. Shmatikov. “How to break anonymity of the netﬂix\nprize dataset”. In: arXiv preprint cs/0610105 (2006).\n[She19]\nO. Sheffet. “Old techniques in differentially private linear regression”. In:\nAlgorithmic Learning Theory. PMLR. 2019, pp. 789–827.\n[SU17]\nT. Steinke and J. Ullman. “Between Pure and Approximate Differential Pri-\nvacy”. In: The Journal of Privacy and Conﬁdentiality 7.2 (2017), pp. 3–22.\n25\n[Ull15]\nJ. Ullman. “Private multiplicative weights beyond linear queries”. In: Pro-\nceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of\nDatabase Systems. 2015, pp. 303–312.\n[Vad17]\nS. Vadhan. “The Complexity of Differential Privacy”. In: Tutorials on the Foun-\ndations of Cryptography: Dedicated to Oded Goldreich. Ed. by Y. Lindell. Cham,\nSwitzerland: Springer International Publishing AG, 2017. Chap. 7, pp. 347–\n450.\n[Ver18]\nR. Vershynin. High-Dimensional Probability: An Introduction with Applications in\nData Science. Cambridge Series in Statistical and Probabilistic Mathematics.\nCambridge University Press, 2018.\n[WSCHT16]\nL. Wei, A. D. Sarwate, J. Corander, A. Hero, and V. Tarokh. “Analysis of a\nprivacy-preserving PCA algorithm using random matrix theory”. In: 2016\nIEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE.\n2016, pp. 1335–1339.\n[ZWB20]\nY. Zhou, Z. S. Wu, and A. Banerjee. “Bypassing the ambient dimension: Private\nsgd with gradient subspace identiﬁcation”. In: arXiv preprint arXiv:2007.03813\n(2020).\n26"
    },
    {
        "file_name": "2106.01361.pdf",
        "title": "2106.01361",
        "year": "2021",
        "full_text": "Mixed-parity octupolar pairing and corner Majorana modes in three dimensions\nBitan Roy1, ∗and Vladimir Juriˇci´c2, 3\n1Department of Physics, Lehigh University, Bethlehem, Pennsylvania, 18015, USA\n2Nordita, KTH Royal Institute of Technology and Stockholm University, Roslagstullsbacken 23, 10691 Stockholm, Sweden\n3Departamento de F´ısica, Universidad T´ecnica Federico Santa Mar´ıa, Casilla 110, Valpara´ıso, Chile\n(Dated: November 11, 2021)\nWe identify time-reversal symmetry breaking mixed-parity superconducting states that feature\neight Majorana corner modes in properly cleaved three-dimensional cubic crystals. Namely, when\nan odd-parity isotropic p-wave pairing coexists with cubic symmetry preserving even-parity octupo-\nlar dx2−y2 + id3z2−r2 pairing, the gapless surface Majorana modes of the former get localized at\nthe eight corners, thus yielding an intrinsic third-order topological superconductor (TOTSC). A\ncousin dxy +id3z2−r2 pairing also accommodating eight corner Majorana modes, by virtue of break-\ning the cubic symmetry, in contrast, yields an extrinsic TOTSC. We identify a doped octupolar\n(topological or trivial) Dirac insulator as a suitable platform to sustain such unconventional super-\nconductors, realized from an intraunit cell pairing. Finally, we argue that the proposed TOTSC can\nbe experimentally realizable in NaCl and other structurally similar compounds under high pressure.\nIntroduction. Localized Majorana zero modes are of\nparamount importance for braiding and non-Abelian\nstatistics, and their applications in topological quantum\ncomputation [1–3]. For these purposes, one-dimensional\nquantum nanowires oﬀer a great potential as they can\nhost topologically robust endpoint Majorana zero modes\nat low temperatures: a hallmark of the traditional bulk-\nboundary correspondence. Nonetheless, its recently dis-\ncovered higher-order generalization manifesting through\nrobust gapless modes localized on even lower-dimensional\nboundaries, such as corners and hinges [4–16], when ex-\ntended to the territory of neutral Bogoliubov-de Gennes\n(BdG) quasiparticles, boosts in this regard the promi-\nnence of higher-dimensional higher-order topological su-\nperconductors (HOTSCs) [17–40]. For example, in con-\ntrast to conventional (or ﬁrst-order) topological p + ip\nand d + id pairings, supporting one-dimensional Majo-\nrana edge modes, a two-dimensional p+id HOTSC hosts\nfour pointlike corner localized Mojorana modes [17, 33].\nHowever, thus far the proposed three-dimensional (3D)\nHOTSCs only encompass Majorana hinge modes, while\nthe mechanism and the platforms for the realizations of\n3D corner Majorana modes remained elusive. In this Let-\nter, we therefore venture the following set of questions,\nand provide deﬁnite answers to them. (1) What is the\nunderlying pairing symmetry of 3D HOTSCs that sup-\nports corner Majorana modes? (2) What are the suitable\nmaterial platforms where such pairings can be realized?\nKey results. Here, we identify two candidate mixed-\nparity time-reversal symmetry breaking octupolar pair-\nings, each of which supports eight zero-energy Majo-\nrana corner modes in suitably cleaved cubic crystals\n[Figs. 1 and 2].\nSpeciﬁcally, we show that when an\nodd-parity spin-triplet isotropic p-wave pairing (analog\nof the B-phase of 3He) coexists with an even-parity sin-\nglet dx2−y2 +id3z2−r2 pairing, the resulting mixed parity\nsuperconducting state supports eight Majorana corner\nmodes.\nThis pairing is a prototypical example of oc-\ntupolar pairing in a cubic system, transforming under\nthe irreducible Eg representation.\nIt breaks the time-\nreversal symmetry, but preserves the cubic symmetry.\nThus p ⊕(dx2−y2 + id3z2−r2) pairing stands as an intrin-\nsic HOTSC [41]. A cousin p ⊕(dxy + id3z2−r2) pairing,\ntransforming under the mixed T2g and Eg representa-\ntions, although supporting eight corner Majorana modes,\nbreaks the cubic symmetry. It thus stands as an extrin-\nsic HOTSC. Since pointlike corner Majorana modes with\ndimensionality dB = 0 in three dimensions (d = 3) are\ncharacterized by the codimension dc = d −dB = 3, these\ntwo paired states represent third-order topological super-\nconductors (TOTSCs). They can be realized around an\nunderlying Fermi surface with an additional two-fold sub-\nlattice degeneracy besides the conventional Kramers de-\ngeneracy. The corner Majorana modes are stable even in\nthe presence of a weak s-wave pairing that gets induced\nnaturally in the presence of dominant d-wave pairings.\nWe identify a doped octupolar Dirac insulator (deﬁned\nlater) as a suitable platform where such unconventional\npairings stem from a unique fully gapped local pair-\ning.\nWhile an intrinsic TOTSC possesses a quantized\noctupolar moment Qxyz = 0.5, for an extrinsic TOTSC\nQxyz = 0. See the phase diagrams in Fig. 3. Finally,\nthe proposed TOTSC may be experimentally realizable\nin NaCl and structurally similar compounds InTe, SnAs\nand SnSb under high pressure [42–45].\nHOTSCs around Fermi surface. The eﬀective single\nparticle BdG Hamiltonian in the presence of p ⊕(dα +\nid3z2−r2) pairings, with α = x2 −y2 and xy, around\nthe Fermi surface (FS), possessing Kramers and two-fold\nsublattice degeneracy reads\nHFS\noctu =\n\u0012 k2\n2m∗\n−µ\n\u0013\nΓ300 + ∆p\n3\nX\nj=1\nkj\nkF\nΓ13j\n+ ∆1 d1(k) Γ110 + ∆2 d2(k) Γ200 + ∆sΓ100,(1)\nwhere Γµνρ = ηµτνσρ. Three sets of Pauli matrices {η},\narXiv:2106.01361v2  [cond-mat.supr-con]  10 Nov 2021\n2\n{τ}, and {σ} respectively act on the Nambu or particle-\nhole, sublattice, and spin or Kramers indices, m∗is the\neﬀective mass, µ is the chemical doping, and kF is the\nFermi momentum. Throughout we consider m∗, µ > 0,\nsuch that the pairing of sharp normal state quasiparticles\ntakes place around a Fermi surface. We are then in the\nweak-coupling regime. The triplet p-wave pairing with\namplitude ∆p is odd under parity k →−k, while it pre-\nserves the time-reversal symmetry. The two components\nof the cubic d-wave pairings (with explicit forms deﬁned\nbelow) are even under parity, i.e., d1,2(−k) = d1,2(k).\nBut the component d2(k) is odd under the reversal of\ntime, generated by T = Γ002K, where K is the complex\nconjugation and T 2 = −1. In addition, we also include\nan s-wave pairing with amplitude ∆s, which preserves\nthe time reversal symmetry and gets naturally induced\nin the presence of a d-wave pairing, as both pairing chan-\nnels are even under parity.\nThe above eﬀective single\nparticle Hamiltonian enjoys the particle-hole symmetry,\ngenerated by the antiunitary operator Θ = Γ202K with\nΘ2 = +1 and ΘHFS\noctuΘ−1 = −HFS\noctu.\nIn the absence of d- and s-wave pairings, HFS\noctu de-\nscribes a fully gapped isotropic odd-parity p-wave pair-\ning (class DIII). As such, it supports two copies of gap-\nless Majorana states on all six surfaces of a cubic crystal,\nirrespective of its speciﬁc cut [46].\nWhen only the d-\nwave pairings are included, all the matrices appearing in\nHFS\noctu mutually anticommute. Since then HFS\noctu involves\nsix mutually anticommuting matrices, their minimal di-\nmensionality has to be eight, which in turn demands an\nadditional two-fold sublattice degeneracy of the Fermi\nsurface. We now address the role of the d-wave pairings\nfor the realization of Majorana corner modes.\nIntrinsic TOTSC. From ﬁve possible cubic d-wave\npairings, one can construct only one combination with\nd1(k) =\n√\n3\n2k2\nF\n(k2\nx −k2\ny), d2(k) =\n1\n2k2\nF\n(2k2\nz −k2\nx −k2\ny) (2)\nthat preserves the cubic symmetry, but breaks the time-\nreversal symmetry. The resulting dx2−y2 + id3z2−r2 state\nis an octupolar pairing and supports eight Majorana-\nWeyl nodes at ±kx = ±ky = ±kz = kF /\n√\n3 (in the\nabsence of other superconducting orders). Even though\nboth d-wave components transform under the irreducible\ndoublet Eg representation of the cubic or Oh point group,\ntheir amplitudes in Eq. (1) are set to be diﬀerent, since\nthese two pairings cannot be transformed into each other\nby an arbitrary SO(3) rotation. Nonetheless, their tran-\nsition temperatures are the same, as expected [47, 48].\nIn the presence of such octupolar pairing, the gapless\nsurface states of the isotropic p-wave pairing get partially\ngapped, since all the involved matrices in Eq. (1) then\nmutually anticommute.\nIn other words, the dx2−y2 +\nid3z2−r2 pairing acts as a mass for gapless surface Ma-\njorana fermions of the p-wave superconductor. However,\n0\n4000\n8000\n-10\n0\n10\nn\nEn\n4000\n-1\n0\n1\n(a)\n(b)\n0.00\n0.04\n0.08\nFIG. 1: (a) Eigenvalue spectra (En) for an intrinsic TOTSC,\nrealized around a Fermi surface, on a cubic lattice.\nIn-\nset: Eight near (due to ﬁnite system size) zero-energy cor-\nner modes (red dots), well separated from nearby bulk states\n(black dots). (b) Local density of states for the zero energy\nstates in (a), displaying sharp localization around the corners\nin the ⟨111⟩directions. These results remain qualitatively un-\nchanged in the presence of a small s-wave component, and for\nthe local pairing shown in Eq. (7) in an octupolar (topologi-\ncal or trivial) Dirac insulator (doped or undoped) [46]. The\nlinear dimension of the system is L = 10 in each direction,\nand t1 = t0 = ∆1 = ∆2 = m0/2 = 1 and ∆s = 0 in Eq. (5).\nsuch a BdG Wilson-Dirac mass vanishes along the high-\nsymmetry eight body-diagonal ⟨111⟩directions. As a re-\nsult, the surface states of isotropic p-wave pairing are left\ngapless only at eight corners of a cubic crystal cleaved\nso that they are placed at (±1, ±1, ±1)L/2, where L is\nthe linear dimension of the system in each direction, see\nFig. 1. The resulting p⊕(dx2−y2+id3z2−r2) pairing there-\nfore stands as an intrinsic TOTSC that supports eight\nzero-energy Majorana corner modes. On the other hand,\nwhen ∆2 = 0, the xy surfaces and four hinges along the\nz direction host gapless Majorana modes, and we realize\na second-order topological superconductor [46].\nExtrinsic TOTSC. Another octupolar pairing with\nd1(k) =\n√\n3\nk2\nF\n(kxky),\nd2(k) =\n1\n2k2\nF\n(2k2\nz −k2\nx −k2\ny)\n(3)\nthat also supports eight Majorana Weyl nodes at\n(±\n√\n2, 0, ±1)kF /\n√\n3 and (0, ±\n√\n2, ±1)kF /\n√\n3 (in the ab-\nsence of any other pairings), partially gaps out the sur-\nface Majorana modes of the isotropic p-wave supercon-\nductor. Such an octupolar pairing leaves eight corners\ngapless, which, as dictated by the dxy pairing compo-\nnent in Eq. (3), are pinned at the four side centers on\neach of the two xy planes in real space, see Fig. 2. The\nabove two components of the d-wave pairings respectively\ntransform under the T2g and Eg representations, thereby\nbreaking the cubic symmetry and the corresponding two\namplitudes in Eq. (1) are generically diﬀerent. The re-\nsulting mixed-parity p ⊕(dxy + id3z2−r2) pairing then\nstands as an extrinsic TOTSC. Once again if we switch\n3\n0\n3400\n6800\n-10\n0\n10\nn\nEn\n3400\n-1\n0\n1\n(a)\n(b)\n0.00\n0.05\n0.10\nFIG. 2: (a) Eigenvalue spectra (En) for an extrinsic TOTSC,\nrealized around a Fermi surface, on a cubic lattice (dashed\ncube), cleaved in such a way (solid cube) that eight corners\nare now placed at the four side centers on each of the two\nxy planes. Inset: Eight near zero-energy corner modes (red\ndots), well separated from nearby bulk states (black dots). (b)\nLocal density of states for the zero-energy states, displaying\nsharp corner localization. These results remain qualitatively\nunchanged in the presence of a small s-wave component, and\nfor the local pairing shown in Eq. (7) in an octupolar Dirac\nmaterial [46]. The linear dimensions of the system are Lx =\n13, Ly = 13, and Lz = 10 in the x, y and z directions,\nrespectively. The parameter values are the same as in Fig. 1.\noﬀthe d3z2−r2-wave pairing, a second-order topological\nsuperconductor is realized [46].\nInduced s-wave pairing. Now we address the impact\nof the induced s-wave component on the fully gapped\nTOTSC. For a small amplitude of such parasitic s-wave\npairing the spectra of BdG quasiparticles remain fully\ngapped, and the system continues to support eight lo-\ncalized corner Majorana modes [46]. However, beyond a\ncritical amplitude of the s-wave pairing, which for the in-\ntrinsic (int) and extrinsic (ext) TOTSCs are respectively\n∆∗,int\ns\n= ∆p,\n∆∗,ext\ns\n=\nq\n∆2p + ∆2\n1/3 ,\n(4)\nthe fully gapped paired state becomes topologically triv-\nial and thus no longer supports corner modes.\nLattice model and numerical results.\nTo anchor the\nabove outlined key results, next we perform a numeri-\ncal analysis on a cubic lattice.\nThe lattice-regularized\nHamiltonian corresponding to Eq. (1), which, pending\nthe representation of the Γ matrices, also describes the\noctupolar Dirac insulator (deﬁned below) and the real-\nization of the TOTSC therein, reads [49]\nHlat\noctu = t1\nX\nj=1,2,3\nsin(kja)Γj + Γ4 mlat\n1 (k)\n−∆1 Γ5\n√\n3 dlat\n1 (k) −∆2 Γ6 dlat\n2 (k) + ∆s˜Γ.\n(5)\nHere a is the lattice spacing and mlat\n1 (k) = m0 −6t0 +\n2t0[cos(kxa)+cos(kya)+cos(kza)] is the ﬁrst-order Dirac\nmass.\nFor intrinsic and extrinsic TOTSCs dlat\n1 (k) =\ncos(kxa) −cos(kya) and sin(kxa) sin(kya), respectively,\nwhile dlat\n2 (k) = 2 cos(kza) −cos(kxa) −cos(kya) in both\ncases.\nHere j = 1, 2 and 3 correspond to x, y and z,\nrespectively. Comparing with Eq. (1), we ﬁnd the fol-\nlowing correspondences among the matrices Γj = Γ13j\nfor j = 1, 2, 3, Γ4 = Γ300, Γ5 = Γ110, Γ6 = Γ200 and\n˜Γ = Γ100.\nWhen expanded around the Γ = (0, 0, 0)\npoint of the cubic Brillouin zone, for example, Hlat\noctu\ntakes the form of HFS\noctu with m∗= (2t0a2)−1, µ = m0,\nkF = a−1, and ∆p = t1. We implement the above tight\nbinding model on a cubic lattice with open boundary con-\ndition and numerically diagonalize it for diﬀerent cuts of\nthe crystal. The results are displayed in Figs. 1 and 2.\nEight zero energy Majorana corner modes are found when\n0 < m0/t0 < 12. On the other hand, for m0/t0 < 0 and\nm0/t0 > 12, the paired state is topologically trivial [46].\nIn the following, we identify the octupolar Dirac insulator\nas a suitable platform for the realization of the TOTSC\nand the corresponding Majorana corner modes.\nOctupolar Dirac insulators. The lattice model for an\noctupolar Dirac insulator takes the form shown in Eq. (5)\nwhen ∆s = 0, with eight-component mutually anticom-\nmuting Hermitian Γ matrices now given by Γj = β1τ1σj\nfor j = 1, 2, 3, Γ4 = β1τ3σ0, Γ5 = β1τ2σ0, and Γ6 =\nβ2τ0σ0. Three sets of Pauli matrices {σ}, {τ} and {β}\nrespectively act on the spin (↑, ↓), parity (±) and sublat-\ntice (A,B) indices. The Hamiltonian is invariant under\na composite PT symmetry, where T = (β0τ0σ2)K, P =\nβ1τ3σ0, and under P: k →−k. Here T and P respec-\ntively play the role of time-reversal and parity operators,\nwith (T P)2 = −1. Furthermore, the Hamiltonian is in-\nvariant under an additional parity operator P′ = β2τ1σ0\nand P′ : k →−k, and enjoys a unitary particle-hole\nor chiral symmetry, generated by Γ7 = β3τ0σ0.\nEven\nthough the above model for 0 < m0/t0 < 12 supports\na topological octupolar insulator with charged corner\nmodes, here we consider the trivial regimes, m0/t0 < 0\nand m0/t0 > 12. The normal state then does not support\nany topological boundary modes. Therefore, the appear-\nance of Majorana bound states can solely be attributed\nto pairing, which we discuss next.\nTo select the pairing realizing the TOTSC in an oc-\ntupolar insulator, we ﬁrst notice that the system supports\n28 (the number of purely imaginary eight-component\nHermitian matrices) local (onsite or intra-unit) cell pair-\nings, due to the Pauli exclusion principle.\nTo capture\nall the pairings in a uniﬁed framework we Nambu-double\nthe original eight-component spinor, and absorb the uni-\ntary part of the time-reversal operator (T ) in the hole\npart of the Nambu spinor. In such a basis the octupo-\nlar Dirac insulator takes the form shown in Eq. (5), with\nsixteen-dimensional Γ matrices taking the explicit forms\nΓ1 = η3β1τ1σ1, Γ2 = η3β1τ1σ2, Γ3 = η3β1τ1σ3,\nΓ4 = η3β1τ3σ0, Γ5 = η0β1τ2σ0, Γ6 = η0β2τ0σ0. (6)\n4\nThe chemical potential term is given by −µ(η3β0τ0σ0).\nA local pairing (with a constant amplitude) supporting\nMajorana corner modes satisﬁes the following algebraic\nconstraints. It anticommutes with the Dirac kinetic en-\nergy (proportional to t1) and commutes with the ﬁrst-\norder Dirac mass [50]. The paired state then represents\na fully gapped topological pairing with two-dimensional\ndispersive massless Majorana modes occupying all six\nsurfaces of a cubic crystal, when ∆1 = ∆2 = 0.\nIn\naddition, the paired state must also simultaneously anti-\ncommute with two higher-order Wilson-Dirac insulating\nmasses (proportional to Γ5 and Γ6), such that surface\nstates get partially gapped, leaving eight corners gapless.\nOnly one pairing satisﬁes all these constraints [46], for\nwhich the eﬀective single-particle Hamiltonian is\nHoctu = ∆(η1 cos φ + η2 sin φ) β1τ1σ0,\n(7)\nwhere φ is the U(1) superconducting phase and ∆is the\npairing amplitude.\nThis pairing is a spin-singlet, but\nmixes even and odd parity bands, and two sublattices.\nWe numerically diagonalize Hlat\noctu corresponding to the\noctupolar insulator in the presence of this pairing and\nﬁnd the eight zero-energy corner Majorana modes in a\ncubic system, cleaved according to the chosen form of\ndlat\n1 (k), similar to Figs. 1 and 2, thus yielding a TOTSC.\nIf, on the other hand, we set ∆2 = 0, the same paired\nstate corresponds to a second-order topological supercon-\nductor with gapless hinge modes along the z direction\nand surface states occupying the xy surfaces [46].\nThese observations can be supported by projecting the\nabove local pairing onto the Fermi surface using the band\nbasis of the single-particle Hamiltonian in Eq. (5), and\nneglecting the interband pairing components.\nThe re-\nduced Hamiltonian (after a suitable global unitary ro-\ntation) assumes the form of HFS\noctu in Eq. (1), when ex-\npanded around the Γ or R point of the Brillouin zone.\nFurthermore, with appropriate choices of the insulating\nmass form factor dlat\n1 (k) the same local pairing from\nEq. (7) yields either intrinsic or extrinsic TOTSC [46].\nTherefore, the local pairing Hoctu imposes a nontrivial\noctupolar topology when projected onto the Fermi sur-\nface, in spite of the parent insulating phase being trivial.\nThese conclusions remain qualitatively unchanged when\nthe normal state is a topological octupolar insulator.\nTopological invariant. Intrinsic and extrinsic TOTSCs\ncan be distinguished besides by symmetry, also in terms\nof a bulk topological invariant, the octupolar moment\nQxyz [51–53]. To extract Qxyz, we treat holelike excita-\ntions as independent particlelike excitations and compute\nn = Re\n\"\n−i\n2π Tr\n \nln\n(\nU † exp\n\"\n2πi\nX\nr\nˆqxyz(r)\n#\nU\n)!#\n,\n(8)\nwhere ˆqxyz(r) = xyzˆn(r)/L3, ˆn(r) is the number operator\nat r = (x, y, z) of a periodic cubic system of linear dimen-\n-6\n0\n6\n12\n18\n0\n6\n12\nm0/t0\nΔ\nTrivial pairing\nThird-order\ntopological pairing\nTrivial pairing\n(a)\n-6\n0\n6\n12\n18\n0\n6\n12\nm0/t0\nΔ\nTrivial pairing\nThird-order top ological pairing\n(b)\nFIG. 3:\nPhase diagrams of TOTSCs (always supporting\neight corner Majorana modes) for (a) lattice regularized BdG\nHamiltonian and (b) local pairing in Eq. (7) in an octupolar\nDirac insulator for t1 = 1. For intrinsic (extrinsic) TOTSC\nQxyz = 0.5 (0.0). Trivial pairing does not support any corner\nmodes and Qxyz = 0 therein. In (a) ∆1 = ∆2 = ∆, while in\n(b) ∆1 = ∆2 = 1.0 and ∆denotes amplitude of the local pair-\ning in Eq. (7). The octupolar Dirac insulator is topological\n(trivial) for 0 < m0/t0 < 12 (m0/t0 < 0 and m0/t0 > 12).\nsion L in each direction, and U is constructed by colum-\nnwise arranging the eigenvectors for the negative energy\nstates.\nThe octupolar moment is deﬁned as Qxyz =\nn −nal (modulo 1), where nal = (1/2) P\nr xyz/L3 repre-\nsents n in the atomic limit and at half ﬁlling. We compute\nQxyz for the lattice regularized BdG Hamiltonian and the\nlocal pairing in an octupolar Dirac insulator [Eqs. (5) and\n(7)], which depending on the form factor dlat\n1 (k) yields in-\ntrinsic or extrinsic TOTSC. While the octupolar moment\nis quantized Qxyz = 0.5 in an intrinsic TOTSC, Qxyz = 0\nin an extrinsic TOTSC. In terms of the corner modes and\nQxyz, we construct cuts of the phase diagram for intrinsic\nand extrinsic TOTSCs in Fig. 3.\nSummary and discussions. We show that time-reversal\nsymmetry breaking mixed parity octupolar p ⊕(dα +\nid3z2−r2) pairing supports eight corner localized Majo-\nrana modes in properly cleaved cubic crystals [Figs. 1\nand 2]. There are two such orders, representing intrinsic\n(for α = x2 −y2) and extrinsic (for α = xy) TOTSCs.\nThe corner modes can be detected by scanning tunnel-\ning microscopy, for example. We furthermore identify a\ndoped octupolar (topological or trivial) Dirac insulator\nas the suitable material platform where such supercon-\nducting order can arise from local or on-site Cooper pairs.\nRemarkably, among all possible local pairings in this sys-\ntem, the unique pairing supporting the Majorana corner\nmodes is also energetically most favored over a wide range\nof m0/t0, covering both topological and trivial Dirac in-\nsulating phases in the normal state [46]. In addition, the\nTOTSC and its associated corner modes remain stable\nin the presence of a weak induced s-wave pairing.\nPresently, NaCl is the only known candidate material\nfor octupolar topological Dirac insulator [54] and it may\nbe a superconductor under pressure with transition tem-\n5\nperature Tc ∼2-7K [42]. Nonetheless, structurally anal-\nogous binary compounds such as InTe, SnAs and SnSb\nunder high pressure also show superconductivity with\nTc ∼1-3K [43–45].\nGiven that our analysis suggests\nthat the doped octupolar Dirac insulator does not need\nto be topological to accommodate TOTSC, which is at\nthe same time energetically most favorable topological\npairing in this system [46], we expect that te topologi-\ncal nature of superconductivity in these materials will be\nscrutinized more thoroughly in the future. Our proposal\nshould also stimulate the search for new octupolar Dirac\nmaterials.\nIndeed, a recent study [55] reported possi-\nble candidate materials for the realization of the octupo-\nlar Dirac insulator in Ti4XTe3, with X=Pb, Sn. When\ndoped, these materials will constitute an ideal platform\nto harbor TOTSCs.\nAcknowledgments. B.R. was supported by the startup\ngrant from Lehigh University and thanks Andr´as L.\nSzab´o for useful discussions. V.J. acknowledges support\nof the Swedish Research Council (VR 2019-04735).\nNote added. After completing this work we became\naware of a study where proximity-induced TOTSC in\ndoped third-order topological insulator with preexisting\ncharged corner modes has been discussed [56].\n∗Corresponding author: bitan.roy@lehigh.edu\n[1] C. Nayak, S. H. Simon, A. Stern, M. Freedman, and S.\nDas Sarma, Rev. Mod. Phys. 80, 1083 (2008).\n[2] C. W. J. Beenakker, Annu. Rev. Cond. Mat. Phys. 4, 113\n(2013).\n[3] Y. Oreg and F. von Oppen, Annu. Rev. Cond. Mat. Phys.\n11, 397 (2020).\n[4] W. A. Benalcazar, B. A. Bernevig, and T. L. Hughes,\nScience 357, 61 (2017).\n[5] W. A. Benalcazar, B. A. Bernevig, and T. L. Hughes,\nPhys. Rev. B 96, 245115 (2017).\n[6] Z. Song, Z. Fang, and C. Fang, Phys. Rev. Lett. 119,\n246402 (2017).\n[7] F. Schindler, Z. Wang, M. G. Vergniory, A. M. Cook,\nA. Murani, S. Sengupta, A. Y. Kasumov, R. Deblock, S.\nJeon, I. Drozdov, H. Bouchiat, S. Gu´eron, A. Yazdani, B.\nA. Bernevig, and T. Neupert, Nat. Phys. 14, 918 (2018).\n[8] J. Langbehn, Y. Peng, L. Trifunovic, F. von Oppen, and\nP. W. Brouwer, Phys. Rev. Lett. 119, 246401 (2017).\n[9] L. Li, M. Umer, and J. Gong, Phys. Rev. B 98, 205422\n(2017).\n[10] D. C˘alug˘aru, V. Juriˇci´c, and B. Roy, Phys. Rev. B 99,\n041301(R) (2019).\n[11] D. Varjas, A. Lau, K. P¨oyh¨onen, A. R. Akhmerov, D. I.\nPikulin, I. C. Fulga, Phys. Rev. Lett. 123, 196401 (2019).\n[12] A. L. Szab´o, R. Moessner, and B. Roy, Phys. Rev. B 101,\n121301(R) (2020).\n[13] B. Wang, X. Zhou, H. Lin, A. Bansil, Phys. Rev. B 104,\nL121108 (2021).\n[14] A. C. Tyner, S. Sur, Q. Zhou, D. Puggioni, P. Darancet,\nJ. M. Rondinelli, and P. Goswami, arXiv:2102.06207\n[15] Q. Wei, X. Zhang, W. Deng, J. Lu, X. Huang, M. Yan,\nG. Chen, Z. Liu, and S. Jia, Nat. Mater. 20, 817 (2021).\n[16] C.-A. Li, S.-B. Zhang, J. Li, and B. Trauzettel, Phys.\nRev. Lett. 127, 026803 (2021).\n[17] Y. Wang, M. Lin, and T. L. Hughes, Phys. Rev. B 98,\n165144 (2018).\n[18] Z. Wu, Z. Yan, and W. Huang, Phys. Rev. B 99,\n020508(R) (2019).\n[19] Q. Wang, C.-C. Liu, Y.-M. Lu, and F. Zhang, Phys. Rev.\nLett. 121, 186801 (2018).\n[20] T. Liu, J. J. He, and F. Nori, Phys. Rev. B 98, 245413\n(2018).\n[21] Y. Volpez, D. Loss, and J. Klinovaja, Phys. Rev. Lett.\n122, 126402 (2019).\n[22] Z. Yan, Phys. Rev. Lett. 123, 177001 (2019).\n[23] X. Zhu, Phys. Rev. Lett. 122, 236401 (2019).\n[24] X.-H. Pan, K.-J. Yang, L. Chen, G. Xu, C.-X. Liu, and\nX. Liu, Phys. Rev. Lett. 123, 156801 (2019).\n[25] S. A. A. Ghorashi, X. Hu, T. L. Hughes, and E. Rossi,\nPhys. Rev. B 100, 020509(R) (2019).\n[26] S. Franca, D. V. Efremov, and I. C. Fulga, Phys. Rev. B\n100, 075415 (2019).\n[27] B. Roy, Phys. Rev. Research 1, 032048 (2019).\n[28] S.-B. Zhang and B. Trauzettel, Phys. Rev. Research 2,\n012018(R) (2020).\n[29] J. Ahn and B.-J. Yang, Phys. Rev. Research 2, 012060\n(2020).\n[30] R.-X. Zhang, Y.-T. Hsu, and S. Das Sarma, Phys. Rev.\nB 102, 094503 (2020).\n[31] S. J. De, U. Khanna, S. Rao, Phys. Rev. B 101, 125429\n(2020).\n[32] R. W. Bomantara, Phys. Rev. Research 2, 033495 (2020).\n[33] B. Roy, Phys. Rev. B 101, 220506(R) (2020).\n[34] M. Kheirkhah, Z. Yan, Y. Nagai, and F. Marsiglio, Phys.\nRev. Lett. 125, 017001 (2020).\n[35] T. E. Pahomi, M. Sigrist, and A. A. Soluyanov, Phys.\nRev. Research 2, 032068(R) (2020).\n[36] X. Wu, W. A. Benalcazar, Y. Li, R. Thomale, C-X. Liu,\nand J. Hu, Phys. Rev. X 10, 041014 (2020).\n[37] A. Tiwari, A. Jahin, and Y. Wang, Phys. Rev. Research\n2, 043300 (2020).\n[38] A. K. Ghosh, T. Nag, and A. Saha, Phys. Rev. B 103,\n045424 (2021).\n[39] B. Fu, Z.-A. Hu, C.-A. Li, J. Li, and S.-Q. Shen, Phys.\nRev. B 103, L180504 (2021).\n[40] X-J. Luo, X-H. Pan, and X. Liu, Phys. Rev. B 104,\n104510 (2021).\n[41] The “⊕” symbol indicates that pairing matrices in the\nodd and even parity channels fully anticommnute. Two\nd-wave components also always mutually anticommute as\nthey break the time-reversal symmetry.\n[42] G. N. Stepanov, E. N. Yakovlev, and T. V. Valyanskaya,\nJETP Letters 29, 418 (1979).\n[43] S. Geller and G. W. Hull, Jr., Phys. Rev. Lett. 13, 127\n(1964).\n[44] Md. R. Kasem, K. Hoshi, R. Jha, M. Katsuno, A. Ya-\nmashita, Y. Goto, T. D. Matsuda, Y. Aoki, and Y.\nMizuguchi, Appl. Phys. Express 13, 033001 (2020).\n[45] M. Katsuno, R. Jha, K. Hoshi, R. Sogabe, Y. Goto, and\nY. Mizuguchi, Condens. Matter 5, 14 (2020).\n[46] See Supplemental Materials at XXX-XXXX for band di-\nagonalization and mean-ﬁeld analysis of competing local\npairings in an octupolar Dirac insulator, additional nu-\nmerical results, and parameter range for corner modes.\n[47] M. Sigrist and K. Ueda, Rev. Mod. Phys. 63, 239 (1991).\n6\n[48] B. Roy, S. A. A. Ghorashi, M. S. Foster and A. H. Nev-\nidomskyy, Phys. Rev. B 99, 054505 (2019).\n[49] T. Nag, Juriˇci´c and B. Roy, Phys. Rev. B 103, 115308\n(2021).\n[50] Any pairing order with a constant amplitude that an-\nticommutes with the ﬁrst-order uniform Dirac mass is\ntopologically trivial as its surface states are fully gapped.\n[51] W. A. Wheeler, L. K. Wagner, and T. L. Hughes, Phys.\nRev. B 100, 245135 (2019).\n[52] B. Kang, K. Shiozaki, and G. Y. Cho, Phys. Rev. B 100,\n245134 (2019).\n[53] A. Agarwala, V. Juriˇci´c, and B. Roy, Phys. Rev. Research\n2, 012067(R) (2020).\n[54] H. Watanabe and H.-C. Po, arXiv:2009.04845\n[55] N. Mao, H. Wang, Y. Dai, B. Huang, and C. Niu,\narXiv:2108.07946.\n[56] A. K. Ghosh, T. Nag, and A. Saha, Phys. Rev. B 104,\n134508 (2021)."
    },
    {
        "file_name": "2303.00145.pdf",
        "title": "2303.00145",
        "year": "2023",
        "full_text": "MOBILE DISKS IN HYPERBOLIC SPACE AND MINIMIZATION OF\nCONFORMAL CAPACITY\nHARRI HAKULA, MOHAMED M. S. NASSER, AND MATTI VUORINEN\nAbstract. Our focus is to study constellations of disjoint disks in the hyperbolic space, the\nunit disk equipped with the hyperbolic metric.\nEach constellation corresponds to a set E\nwhich is the union of m > 2 disks with hyperbolic radii rj > 0, j = 1, ..., m. The centers\nof the disks are not fixed and hence individual disks of the constellation are allowed to move\nunder the constraints that they do not overlap and their hyperbolic radii remain invariant.\nOur main objective is to find computational lower bounds for the conformal capacity of a\ngiven constellation. The capacity depends on the centers and radii in a very complicated way\neven in the simplest cases when m = 3 or m = 4. In the absence of analytic methods our\nwork is based on numerical simulations using two different numerical methods, the boundary\nintegral equation method and the hp-FEM method, resp. Our simulations combine capacity\ncomputation with minimization methods and produce extremal cases where the disks of the\nconstellation are grouped next to each other. This resembles the behavior of animal colonies\nminimizing heat flow in arctic areas.\n1. Introduction\nMany extremal problems of physics, exact sciences, and mathematics have solutions which\nexhibit varying degree of symmetry.\nA typical situation is to minimize or maximize a set\nfunctional of a planar set under the constraint that some other functional is constant. The\nclassical isoperimetric problem [31] is an example. Here one maximizes the area of a planar\nset given its perimeter and the extremal domain is the disk.\nG. Pólya and G. Szegö [31]\ninitiated a systematic study of a large class of isoperimetric type problems of mathematical\nphysics for domain functionals such as moment of inertia, principal frequency, torsional rigidity,\nand, in particular, capacities of condensers. Certain geometric transformations, known under\nthe general name “symmetrization” have the property that they decrease the value of domain\nfunctionals and thus can give hints about the extremal configuration of isoperimetric problems\n[3, 8].\nWe study here new types of transformations which decrease the value of conformal\ncapacity.\nIn a very interesting recent paper, A. Solynin [34] describes capacity problems, motivated\nby the behavior of herds of arctic animals which keep close together to minimize the total loss\nof heat of the herd or to defend against predators (see figures in [34]). Such a herd behavior\nseems to suggest the heuristic idea that “minimization of herd’s outer perimeter” minimizes\nthe loss of heat or danger from predators. This kind of extremal problem can be classified as\nspecial type of isoperimetric problem. As an illustration of the connection between the kind of\ntransformations we are interested in and the observed behavior in nature, see Figure 1.\nFile: mcd2arXiv2.tex, printed:\n2023-11-30, 2.34\nKey words and phrases. Multiply connected domains, condenser capacity, capacity computation.\n1\narXiv:2303.00145v2  [math.CV]  29 Nov 2023\n2\nH. HAKULA, M. NASSER, AND M. VUORINEN\n(a)\n(b)\n(c)\nFigure 1. Examples of constrained optimisation. (a) Tree swallows huddle on\na branch during a spring snowstorm [39]. (b) Minimal capacity configuration for\nfour hyperbolic disks on a diameter. (c) Minimal capacity configuration for four\nhyperbolic disks on a hyperbolic circle. In (b) and (c) the hyperbolic disks are\ninside the unit disk equipped with the hyperbolic metric.\nIn a recent paper [29], isoperimetric inequalities in hyperbolic geometry were applied to\nestimate the conformal capacity of condensers of the form (B2, E) where E is a union of finitely\nmany disjoint closed disks Ej, j = 1, ..., m, in the unit disk B2. Thus E is a constellation of\ndisks. Gehring’s lower bound [9] (see also [29]) is given by condensers of the form (B2, E∗) where\nE∗is a disk with the hyperbolic area equal to that of ∪m\nj=1Ej. Further recent investigations of\ncondenser capacity in the framework of hyperbolic geometry include [28, 26, 27], where pointers\nMOBILE DISKS IN HYPERBOLIC SPACE\n3\nto earlier work can be found. It should be noticed that due to the conformal invariance of the\nconformal capacity, the hyperbolic geometry provides the natural setup for this study.\nWe continue here this work and our goal is to analyse extremal cases of the aforementioned\ncapacity and how the capacity depends on the geometry of the disk constellation. The constraint\nthat the disks do not overlap leads to problems of combinatorial geometry. Some examples of\nsuch geometric problems, related to this work and the herd behavior mentioned above, are\nDescartes’ problem of four circles with each circle tangent to three circles, Apollonian circle\npacking, and Soddy’s “complex kiss precise” problem for configurations of mutually tangent\ncircles [21]. Combinatorial geometry extremal problems motivated by biochemistry research\nand drug development are described in [23]. A very interesting discussion of many topics of\ncombinatorial geometry including packing problems is given in the encyclopedic work of M.\nBerger [6]. The three dimensional case is much more difficult than the planar case and it is the\nsubject of the extensive review paper [20] where topics range from optimal packing of spheres to\nconstrained motion of small spheres on the surface of the unit sphere. For an extensive survey\nof potential theoretic extremal problems see [7].\nAnalysing the extremal cases of the lower bound for\ncap(B2, ∪m\nj=1Ej)\nfor a constellation of disjoint hyperbolic disks Ej seems to be very difficult even in the simplest\ncases m = 3, 4. Therefore we consider this problem in special cases such as the case when the\ncircle centers are at the same distance from origin or analyse constrained motion of one circle\nalong three other fixed circles (see Figure 1). Simulations indicate that several constellations\nyield local minima of the capacity. Throughout, the hyperbolic geometry provides the natural\ngeometric framework for our study, because of the conformal invariance of the capacity. We\nuse two numerical methods for computing the capacity, the hp-FEM and the boundary integral\nequation (BIE) method. The numerical results lead to a number of conjectures and improved\nbounds. Indeed, the existing lower bound for constellations considered here is improved of\nthe order of 10% for disks of unit hyperbolic radius. Moreover, the asymptotic nature of the\ntheoretical lower bound as the hyperbolic radii rj →∞is easily understood in the context of\nhyperbolic geometry.\nIn modern physics, in particular in condensed matter physics, there has been a lot of interest\nin geometric settings with negative curvature [19, 22], that is, exactly our natural setup. The\npurpose of this paper is also to show how computations can be formulated and performed in\nboth Euclidean and hyperbolic geometries, even with the possibility of moving from one to\nanother. This is highlighted in the last section where the optimal configurations in hyperbolic\ngeometry are found by successive transformations to a Euclidean coordinate system employed\nin the optimization routines.\nFor information about potential theory and its applications,\nsee [7, 31, 32, 36].\nThe contents are organized into sections as follows. Section 2 contains the key facts about\nhyperbolic geometry, including the transformation formulae from Euclidean disks to Poincaré\ndisks and back. Section 3 covers the preliminary notations of conformal capacity, collected from\nvarious sources, e.g. from [4, 8, 10, 11, 17, 16]. These are the cornerstones of the geometric\nsetup of the computations in the sequel. Section 3 also provides an overview of the hp-FEM\n[15, 14] adjusted to the present computational tasks, our second computational work horse,\n4\nH. HAKULA, M. NASSER, AND M. VUORINEN\nFigure 2. Visualisations on Poincaré disk. Left: Images of hyperbolic disks\nwith hyperbolic radii = 1, 2, 3, 4, 5. Right: Hyperbolic disks on three diameters\nall with equal radii. Notice the lens-shaped regions containing the disks on each\ndiameter.\nthe BIE method [25, 28], and the interior-point method used in optimization. The numerical\nexperiments are discussed in Sections 4 and 5. In Section 4 the selected configurations have\nbeen designed a priori, with the goal of forming an understanding of the identifiable geometric\nfeatures of the minimal capacity configurations. In Section 5 that understanding is challenged\nby searching for the minimal capacity configurations using numerical optimization starting with\nrandom initial configurations. Finally, the conclusions are drawn in Section 6.\n2. From Euclidean Disk to Poincare and Back\nIn this section the central transformation formulae collected from various sources are pre-\nsented. In Figure 2 different properties of geometry on the Poincaré disk have been illustrated.\nIn particular, the facts that for all ϵ > 0, M > 0 there are hyperbolic disks with radii M but\nEuclidean diameter < ϵ and hyperbolic disks with equal radii have different Euclidean radii\ndepending on their location are important for our discussion below.\nFor a point x ∈Rn and a radius r > 0, define an open Euclidean ball Bn(x, r) = {y ∈\nRn | |x −y| < r} and its boundary sphere Sn−1(x, r) = {y ∈Rn | |x −y| = r}. For the unit\nball and sphere, we use the simplified notations Bn = Bn(0, 1) and Sn−1 = Sn−1(0, 1). The\nsegment joining two points x, y ∈Rn is denoted [x, y].\nDefine the hyperbolic metric in the Poincaré unit disk B2 as in [4], [5, (2.8) p. 15]\nsh2ρB2(x, y)\n2\n=\n|x −y|2\n(1 −|x|2)(1 −|y|2),\nx, y ∈B2.\n(2.1)\nWe use the notation sh and arsh for the hyperbolic sine and its inverse, respectively, and\nsimilarly, th and arth for the hyperbolic tangent and its inverse. The hyperbolic midpoint of\nMOBILE DISKS IN HYPERBOLIC SPACE\n5\nx, y ∈B2 is given by [37]\nmH(x, y) =\ny (1 −|x|2) + x (1 −|y|2)\n1 −|x|2|y|2 + A[x, y]\np\n(1 −|x|2)(1 −|y|2)\nwhere A[x, y] =\np\n|x −y|2 + (1 −|x|2)(1 −|y|2). We use the notation\nBρ(x, M) = {z ∈B2 : ρB2(x, z) < M}\nfor the hyperbolic disk centered at x ∈B2 with radius M > 0 . It is a basic fact that they are\nEuclidean disks with the center and radius given by [16, p.56, (4.20)]\n(2.2)\n\n\n\n\n\nBρ(x, M) = B2(y, r) ,\ny = x(1 −t2)\n1 −|x|2t2 , r = (1 −|x|2)t\n1 −|x|2t2 , t = th(M/2) ,\nNote the special case x = 0,\n(2.3)\nBρ(0, M) = B2(0, th(M/2)) .\nConversely, the Euclidean disks can be considered as hyperbolic ones by [37]\n(2.4)\n(\nB2(y, r) = Bρ(x, M) ,\nx = t y/|y| , M = ρB2(x, z) , t = mH (|y| −r, |y| + r) ,\nLemma 2.5 ([4, Thm 7.2.2, p. 132]). The area of a hyperbolic disc of radius r is 4π sh2(r/2)\nand the length of a hyperbolic circle of radius r is 2π sh(r).\n3. Conformal Capacity and Numerical Methods\nA condenser is a pair (G, E), where G ⊂B2 is a domain and E is a compact non-empty\nsubset of G. The conformal capacity of this condenser is defined as [8, 10, 11, 16, 17]\ncap(G, E) = inf\nu∈A\nZ\nG\n|∇u|2dm,\n(3.1)\nwhere A is the class of C∞\n0 (G) functions u : G →[0, ∞) with u(x) ≥1 for all x ∈E and dm is\nthe 2-dimensional Lebesgue measure. In this paper we assume that G = B2 is the unit disk and\nE = ∪m\nj=1Ej where E1, . . . , Em are m closed disjoint disks in the unit disk. Hence Ω= G\\E is a\nmultiply connected circular domain of connectivity m+1. In this case, the infimum is attained\nby a function u which is harmonic in Ωand satisfies the boundary conditions u = 0 on ∂G and\nu = 1 on ∂E [8]. The capacity can be expressed in terms of this extremal function as\n(3.2)\ncap(G, E) =\nZZ\nΩ\n|∇u|2dm.\nThe conformal capacity of a condenser is one of the key notions of potential theory of elliptic\npartial differential equations [17, 11] and it has numerous applications to geometric function\ntheory, both in the plane and in higher dimensions, [8, 10, 11, 16, 17]. Numerous variants of\n6\nH. HAKULA, M. NASSER, AND M. VUORINEN\nFigure 3. Discretization and optimization. For the given set of four hyperbolic\ndisks with centers constrained on a diameter, the configuration shown here mini-\nmizes the capacity. Left: Configuration and hp-FEM mesh. Center: Potential in\n2D. Right: Potential in 3D.\nthe definition (3.1) of capacity are given in [10, 11]. First, the family A may be replaced by\nseveral other families by [10, Lemma 5.21, p. 161]. Furthermore,\ncap(G, E) = M(∆(E, ∂G; G)),\n(3.3)\nwhere ∆(E, ∂G; G) is the family of all curves joining E with the boundary ∂G in the domain\nG and M stands for the modulus of a curve family [10, Thm 5.23, p. 164]. For the basic facts\nabout capacities and moduli, the reader is referred to [10, 11, 16, 17].\n3.1. Numerical Methods. In this section the numerical methods used in the numerical ex-\nperiments are briefly described. The capacities are computed using the hp-version of the finite\nelement method (FEM) and the boundary integral equation with the generalized Neumann ker-\nnel method (BIE). The minimization problems are computed using the interior-point method\nas implemented in MATLAB and Mathematica.\nSince the Dirichlet problem (3.1) is one of the primary numerical model problems, any stan-\ndard solution technique can be viewed as having been validated. Verification of the results is\ndiscussed in connection with one of the numerical experiments below.\n3.1.1. hp-FEM. What is of particular interest in the context of this paper is that the hp-FEM\nallows for large curved elements without significant loss of accuracy.\nSince the number of\nelements can be kept relatively low given that additional refinement can always be added via\nelementwise polynomial degree, variation in the boundary can be addressed directly at the level\nof the boundary representation in some exact parametric form. This is illustrated in Figure 3.\nThe following theorem due to Babuška and Guo [2] sets the limit to the rate of convergence.\nNotice that construction of the appropriate spaces is technical. For rigorous treatment of the\ntheory involved see Schwab [33] and references therein.\nTheorem 3.4. Let Ω⊂R2 be a polygon, v the FEM-solution of (3.1), and let the weak solution\nu0 be in a suitable countably normed space where the derivatives of arbitrarily high order are\nMOBILE DISKS IN HYPERBOLIC SPACE\n7\ncontrolled. Then\ninf\nv ∥u0 −v∥H1(Ω) ≤C exp(−b\n3√\nN),\nwhere C and b are independent of N, the number of degrees of freedom. Here v is computed on\na proper geometric mesh, where the order of an individual element is set to be its element graph\ndistance to the nearest singularity. (The result also holds for meshes with constant polynomial\ndegree.)\nConsider the abstract problem setting with u defined on the standard piecewise polynomial\nfinite element space on some discretization T of the computational domain Ω. Assuming that\nthe exact solution u ∈H1\n0(D) has finite energy, we arrive at the approximation problem: Find\nˆu ∈V such that\n(3.5)\na(ˆu, v) = l(v) (= a(u, v))\n(∀v ∈V ),\nwhere a( · , · ) and l( · ), are the bilinear form and the load potential, respectively. Additional\ndegrees of freedom can be introduced by enriching the space V .\nThis is accomplished via\nintroduction of an auxiliary subspace or “error space” W ⊂H1\n0(D) such that V ∩W = {0}.\nWe can then define the error problem: Find ε ∈W such that\n(3.6)\na(ε, v) = l(v) −a(ˆu, v)(= a(u −ˆu, v))\n(∀v ∈W).\nThis can be interpreted as a projection of the residual to the auxiliary space.\nThe main result on this kind of estimators for the Dirichlet problem (3.1) is the following\ntheorem.\nTheorem 3.7 ([14]). There is a constant K depending only on the dimension d, polynomial de-\ngree p, continuity and coercivity constants C and c, and the shape-regularity of the triangulation\nT such that\nc\nC ∥ε∥1 ≤∥u −ˆu∥1 ≤K (∥ε∥1 + osc(R, r, T )) ,\nwhere the residual oscillation depends on the volumetric and face residuals R and r, and the\ntriangulation T .\n3.1.2. BIE method. We review a BIE method from [28] for computing the capacity cap(B, E).\nThe method is based on the BIE with the generalized Neumann kernel. The domains considered\nin this paper are circular domains, i.e., domains whose boundary components are circles. The\nexternal boundary is the unit circle, denoted by C0, is parametrized by η0(t) = eit for t ∈\nJ0 = [0, 2π]. The inner circles Cj are parametrized by ηj(t) = zj + rje−it, t ∈Jj = [0, 2π], for\nj = 1, 2, . . . , m, where zj is the center of the circle Cj and rj is its radius. Let J be the disjoint\nunion of the m + 1 intervals Jj = [0, 2π], j = 0, 1, . . . , m. We define a parametrization of the\nwhole boundary C = ∪m\nj=0Cj on J by (see [25] for the details)\nη(t) =\n\n\n\n\n\n\n\nη0(t),\nt ∈J0,\nη1(t),\nt ∈J1,\n...\nηm(t),\nt ∈Jm.\n8\nH. HAKULA, M. NASSER, AND M. VUORINEN\nWith the parametrization η(t) of the whole boundary C, we define a complex function A by\n(3.8)\nA(t) = η(t) −α,\nwhere α is a given point in the domain G. The generalized Neumann kernel N(s, t) is defined\nfor (s, t) ∈J × J by\n(3.9)\nN(s, t) := 1\nπIm\nÅA(s)\nA(t)\nη′(t)\nη(t) −η(s)\nã\n.\nWe define also the following kernel\n(3.10)\nM(s, t) := 1\nπRe\nÅA(s)\nA(t)\nη′(t)\nη(t) −η(s)\nã\n,\n(s, t) ∈J × J.\nThe kernel N(s, t) is continuous and the kernel M(s, t) is singular where the singular part\ninvolves the cotangent function.\nHence, the integral operator N with the kernel N(s, t) is\ncompact and the integral operator M with the kernel M(s, t) is singular. Further details can\nbe found in [38].\nFor each k = 1, 2, . . . , m, let the function γk be defined by\n(3.11)\nγk(t) = log |η(t) −zk|,\nlet µk be the unique solution of the BIE\n(3.12)\nµk −Nµk = −Mγk,\nand let the piecewise constant function hk = (h0,k, h1,k, . . . , hm,k) be given by\n(3.13)\nhk = [Mµk −(I −N)γk]/2.\nFor each k = 1, 2, . . . , m, the solution µk of the BIE (3.12) and the piecewise constant function\nhk in (3.13) will be computed using the MATLAB fbie from [25]. In the function fbie, the\nintegral equation (3.12) is solved using the Nyström method with the trapezoidal rule. Solving\nthe integral equation is then reduced to solving an (m + 1)n × (m + 1)n linear system which\nis solved by the MATLAB function gmres. The matrix-vector product in gmres is computed\nby the MATLAB function zfmm2dpart from the MATLAB toolbox FMMLIB2D [12]. To use\nthe MATLAB function fbie, we define a vector s = [s1, . . . , sn] where sk = 2(k −1)π/n, k =\n1, . . . , n, and n is a given even positive integer. Then we compute the (m+1)n×1 discretization\nvectors et and etp of the parametrization η(t) of the boundary C and its derivative η′(t) by\net = [η0(s), η1(s), . . . , ηm(s)]T,\netp = [η′\n0(s), η′\n1(s), . . . , η′\nm(s)]T.\nWe also discretize the functions A(t) and γk(t) by A = et−α and gamk = γk(et), k = 1, . . . , m.\nThen we compute (m + 1)n × 1 approximate discretizations muk and hk of the functions µk(t)\nand hk(t) by calling\n[muk, hk] = fbie(et, etp, A, gamk, n, 5, [ ], 1e −14, 100),\ni.e., the tolerance of the FMM is 0.5×10−15, the GMRES is used without restart, the tolerance\nof the GMRES method is 10−14 and the maximal number of GMRES iterations is 100.\nBy computing the (m + 1)n × 1 vector hk, we obtain approximate discretizations of the\npiecewise constant function hk = (h0,k, h1,k, . . . , hm,k) in (3.13). Note that, for k = 1, . . . , m,\nMOBILE DISKS IN HYPERBOLIC SPACE\n9\nthe constant hj,k is the value of the function hk on the boundary component Γj. We approximate\nthe values of the real constants hj,k by taking arithmetic means\nhj,k = 1\nn\n(j+1)n\nX\ni=1+jn\nhki,\nj = 0, 1, . . . , m,\nk = 1, . . . , m.\nThe values of the m real constants a1, . . . , am are then approximated by solving the (m + 1) ×\n(m + 1) linear system [28]\n(3.14)\n\n\nh0,1\nh0,2\n· · ·\nh0,m\n1\nh1,1\nh1,2\n· · ·\nh1,m\n1\n...\n...\n...\n...\n...\nhm,1\nhm,2\n· · ·\nhm,m\n1\n\n\n\n\na1\na2...\nam\nc\n\n\n=\n\n\n0\n1\n...\n1\n\n.\nSince m + 1 is the number of boundary components of the domain Ω= G \\ E, we can assume\nthat m is small and solve the linear system (3.14) using the Gauss elimination method. By\nsolving the linear system, the capacity cap(B, E) will be computed by [28, Eq. (3.9)]\n(3.15)\ncap(B, E) = 2π\nm\nX\nk=1\nak.\nIn this paper, the boundary components of the domain Ωare circles. Thus, the integrands\nin (3.12) and (3.13) will be 2π-periodic functions, and can be extended holomorphically to\nsome parallel strip |Im t| < σ in the complex plane.\nHence, the trapezoidal rule will then\nconverge exponentially with O(e−σn) [35] when it is used to discretize the integrals in (3.12)\nand (3.13). The numerical solution of the integral equation will converge with a similar rate of\nconvergence [1, p. 322] (see Figure 5 (right) below).\n3.1.3. Nonlinear Optimization: Interior-Point Method. The two methods outlined above are\ncombined with a numerical optimization routine in the last set of numerical experiments below.\nThe task is to find an optimal configuration for a set of hyperbolic disks E with fixed radii. We\nuse the interior-point method as implemented in Mathematica (FindMinimum, [40]) and Matlab\n(fmincon, [24]).\nIn the most general case the problem is defined as in (3.16), where the only constraint is a\ngeometric one, that is, the disks are not allowed to overlap. Here, the radii are fixed and the\noptimization concerns only the locations of the disks.\nmin\nE\ncap(G, E)\nsubject to:\nEi ∩Ej = ∅\n∀i, j = 1, . . . , m, i ̸= j\n(3.16)\nEj⊂G\n∀j = 1, . . . , m.\nThis nonlinear optimization problem can be solved using the interior-point method.\nThis\nsolution would be a local minimum. The standard textbook reference is Nocedal and Wright\n[30].\nNotice, that the objective function is indeed the capacity of the constellation. Often opti-\nmization problems with geometric constraints are related to packing and fitting problems. The\n10\nH. HAKULA, M. NASSER, AND M. VUORINEN\ntask here is orders of magnitude more demanding since, at every point evaluation one solution\nof the capacity problem has to be computed, and as the disks move the constraints change\nas well. The number of evaluations is greater than the number of iteration steps, since the\ngradients and Hessians must be approximated numerically. It should be noted that the success\nof the optimization depends on the high accuracy of the capacity solver, since otherwise the\napproximate derivatives are not sufficiently accurate.\nIn the context of this work, there have been no attempts to devise a special method that\nwould incorporate some of the insights gathered during this study.\nInstead, the numerical\noptimization is used to challenge those insights and therefore the optimizations have been\ncomputed with minimal input information.\n4. Minimizing Capacity: Constrained Configurations\nAs mentioned above, even with a small number of disks the combinatorial explosion of the\nnumber of configurations is evident. Therefore, we restrict ourselves to a series of experiments\neach with increasing complexity building toward an understanding of the fundamental geometric\nprinciples behind the minimal configurations. In each case we consider a set of hyperbolic disks\nEj with radii rj, where some geometric constraint is placed on all or some of the disks in the\nconstellation.\nAn initial observation is that due to conformal invariance of the capacity, its numerical value\nremains invariant under a Möbius transformation of the unit disk onto itself. Therefore we may\nassume that the disk with the largest radius r1 is centered at the origin.\nFurther, consider a disk Bρ(z2, r2) with center z2 on the segment (0, 1). The disk lies in the\nlens-shaped region\nW = B2(iτ,\n√\n1 + τ 2) ∩B2(−iτ,\n√\n1 + τ 2),\nτ > 0,\nwith ρB2(0, iv) = r2 where v =\n√\n1 + τ 2 −τ and is tangent to both boundary arcs of W and\n±1 ∈∂W, see Figure 2 (right). Every disk lies within its own associated lens-shaped domain.\n4.1. Disks with collinear centers. Consider a set of m hyperbolic disks Ej with radii rj and\ncenters on the diameter (−1, 1) with Pm\nj=1 2rj = d1 = ρB2(−0.6, 0.6). We choose the hyperbolic\ncenters of these disks so that the hyperbolic distance between them is d ≥0 where d = 0\ncorresponds to the case when they touch each other. The goal is to establish upper and lower\nbounds for cap(B, ∪Ej). Since the hyperbolic radius of a hyperbolic disk is invariant under\nMöbius transform, in view of (2.2), we have cap(B, Ej) = 2π/ log(1/ th(rj/2)) for all Ej.\nThe cases cap(B, ∪m\nj=1Ej) for m = 2, 3, 4 over the range 0.02 ≤d ≤4 are shown in Figure 4.\nThe conjectured lower bound with d = 0 is computed with hp-FEM (see the ‘red dot’ in Figure 4\n(right)), all other capacities are computed with BIE. From Figure 4 we also see that\ncap(B, ∪m\nj=1Ej) ≈\nm\nX\nj=1\ncap(B, Ej),\nas the separation d becomes large.\nMOBILE DISKS IN HYPERBOLIC SPACE\n11\nFigure 4. The hyperbolic disks when the hyperbolic distance d between them\nis d = 0.02 (left) and d = 1 (middle). On the right, cap(B, ∪k\nj=1Ej) as a function\nd. In the first row: r1 = 0.55d1/2 and r2 = 0.45d1/2 where d1 = ρB2(−0.6, 0.6).\nIn the second row: r1 = 0.35d1/2, r2 = 0.25d1/2, and r3 = 0.40d1/2. In the third\nrow: r1 = 0.35d1/2, r2 = 0.15d1/2, r3 = 0.20d1/2, and r4 = 0.30d1/2.\n4.1.1. Verification of results. Let us consider the case with four disks and set E = ∪4\nj=1Ej. The\ninitial position is when the disks are contiguous, tangent to each other, and then the hyperbolic\ndistance d between the disks increases from 0 to 0.3. The conclusion is that the value d = 0\nyields the minimal value of the capacity of the constellation.\n12\nH. HAKULA, M. NASSER, AND M. VUORINEN\nTable 1. Disks with collinear centers: m hyperbolic disks Ej with radii rj and\ncenters on the diameter (−1, 1) with Pm\nj=1 2rj = d1 = ρB2(−0.6, 0.6). Conjectured\nlower and upper bounds of the capacity cap(B, ∪Ej).\nm\nLower\nUpper\n2\n8.515312094751020\n11.463763614692954\n3\n7.450131756754710\n12.744594178229441\n4\n7.017838565418236\n14.282099489357595\nTable 2. Computed values of cap(B2, E) when m = 4 for a constellation with\ndisk radii (from left to right) r1 = 0.15d1/2, r2 = 0.35d1/2, r3 = 0.20d1/2,\nand r4 = d1/2 −(r1 + r2 + r3) where d1 = ρB2(−0.6, 0.6). The centers on the\ndiameter (−1, 1) as a function of the hyperbolic distance d between disks, i.e.,\nc1 = −th((r2 + r1 + d)/2), c2 = 0, c3 = th((r2 + r3 + d)/2), c4 = −th((r2 + 2r3 +\nr4 + 2d)/2).\nd\nFEM\nBIE\nAgreement\n0.00\n7.017838565413617\n—\n—\n0.05\n7.230698262298420\n7.230698262298405\n1.51 × 10−14\n0.10\n7.442082617728579\n7.442082617728490\n8.88 × 10−14\n0.15\n7.651760366696882\n7.651760366696745\n1.37 × 10−13\n0.20\n7.859490827905997\n7.859490827905935\n6.22 × 10−14\n0.25\n8.064996233395842\n8.064996233395734\n1.08 × 10−13\n0.30\n8.267972932727597\n8.267972932727497\n9.95 × 10−14\nThe values of the capacity cap(B2, E) in Table 2 have been computed using both methods,\nthe FEM and the BIE method. For the BIE, we use n = 27 and α = 0.8i. Table 2 shows the\nabsolute differences between the computed values which indicates a good agreement between\nthe two methods. As in [13], the values computed using the FEM will be considered as reference\nvalues and used to estimate the error in the values computed by the BIE method for several\nvalues of n. The BIE method cannot be used for d = 0. The error for d = 0.05, 0.1, . . . , 0.3\nis presented in Figure 5 (right) which illustrates the exponential convergence with order of\nconvergence O(e−σn) where σ = −log |α| ≈0.223. Numerical experiments (not presented here)\nwith other values of α indicate that the order of convergence depends on α as well as the\ncenters z1, . . . , zm and the radii r1, . . . , rm of the inner circles. A detailed analysis of the order\nof convergence for the above BIE method is a subject of future work.\n4.2. Four disks: Permutation of contiguous disks. We consider next two cases where all\nthe disks of the constellation have fixed hyperbolic radii A > B > C > D > 0 but their relative\nordering is not constrained other than that each disk is tangent to at least one other disk of\nthe constellation and their hyperbolic centers lie (a) either on the diameter (−1, 1) or (b) on\nthe circle {z : |z| = 1/2}.\nNow the question is what is the effect of the permutation of the disks on the capacity. There\nare 24 permutations with 12 different capacities due to symmetry. For every realisation, the\nMOBILE DISKS IN HYPERBOLIC SPACE\n13\n■\n■\n■\n■\n■\n■\n■\n17\n19\n21\n23\n25\n27\n29\n10-13\n10-11\n10-9\n10-7\n10-5\n17\n19\n21\n23\n25\n27\n29\n10-13\n10-11\n10-9\n10-7\n10-5\nN\n3\nError estimate\nFigure 5. The error for the constellation of four disks in Table 2. Left: The hp-\nFEM error estimate as a function of\n3√\nN, where N is the number of d.o.f. (logplot)\nfor four disk configuration with contacts (d = 0). The observed constant or the\nslope of the graph = 37.1. Right: The errors in the computed values of cap(B2, E)\nusing the BIE method as functions of n, for α = 0.8i where σ = −log |α| ≈0.223.\nTable 3. Permutations of contiguous constellations. ED with centers on the\nsegment (−1, 1) and (A, B, C, D) = (1/2, 2/5, 1/4, 1/5). EC with centers on the\ncircle {z : |z| = 1/2} and (A, B, C, D) = (1/2, 1/3, 1/4, 1/5).\nCase\nr1\nr2\nr3\nr4\ncap C(ED)\ncap C(EC)\n1\nD\nB\nA\nC\n6.781488018927628\n6.451424010111881\n2\nD\nA\nB\nC\n6.788910565780309\n6.455800945561348\n3\nD\nC\nA\nB\n6.843774515059010\n6.475070264106950\n4\nC\nD\nA\nB\n6.882473842468833\n6.485425869048534\n5\nA\nB\nC\nD\n6.890544149275032\n6.496389476635198\n6\nB\nC\nA\nD\n6.897202225461369\n6.500210100051595\n7\nC\nA\nD\nB\n6.919626376828870\n6.520197932005349\n8\nA\nB\nD\nC\n6.928074481413122\n6.523073055329720\n9\nA\nC\nB\nD\n6.932436180755356\n6.542555705939787\n10\nC\nB\nD\nA\n6.962814943144452\n6.542981227003898\n11\nA\nC\nD\nB\n7.053764008325471\n6.575258877036491\n12\nA\nD\nC\nB\n7.055565195334228\n6.576332514877286\nradii are denoted by rj from left to right and the constellations are denoted by ED and EC,\nrespectively. For ED we set (A, B, C, D) = (1/2, 2/5, 1/4, 1/5), and for EC slightly perturbed\n(A, B, C, D) = (1/2, 1/3, 1/4, 1/5). The results are collected in Table 3 and Figure 1 shows the\nobserved extremal permutations. Interestingly, the resulting capacities have exactly the same\ndependence on the relative sizes of the radii.\n4.3. Three immobile disks, one rolling disk. In the final experiment of the section we\nstudy the situation when one disk is free to roll on the remaining three contiguous immobile\ndisks, centers on the diameter (−1, 1) and tangent to each other. The route of the mobile disk\n14\nH. HAKULA, M. NASSER, AND M. VUORINEN\nTable 4. Hyperbolic radii used in Figure 6.\nCase\nr1\nr2\nr3\nr4\n1\n0.4\n0.2\n0.5\n0.25\n2\n0.2\n0.5\n0.3\n0.5\n3\n0.5\n0.5\n0.5\n0.2\n4\n0.2\n0.7\n0.4\n0.1\nFigure 6. Three immobile disks, one rolling disk. Cases 1 to 4 from left to\nright. Dependence of the capacity on the relative location of the rolling disk.\nThe hyperbolic center z4 of the moving disk is on the red curve shown in the\nfigure.\nis parametrized with a parameter τ ∈[0, 1] where the values 0 and 1 are for the case when also\nthe mobile disk has its center on the diameter (−1, 1) and the values 1/3 and 2/3 correspond\nto the intermediate points on the route when the rolling disk is tangent to two immobile disks.\nDepending on the radii, it might also happen that there is only one such point. In Figure 6\nbelow we see that for the values 1/3 and 2/3 the capacity of the constellation attains a local\nminimum. The numerical results for this example are computed using the BIE method. So,\ninstead of assuming that the disks are touching each other, we assume that the disks are close\nto each other such that the hyperbolic distance between them is d = 0.02. In all cases the\nhyperbolic centers of the three fixed disks are z1 = −th((r1 −d)/2), z2 = th((r2 + 2d)/2), and\nz3 = th((r3 + 2r2 + 3d)/2). The hyperbolic center z4 of the moving disk is on the red curve\nshown in the figure. The observed results are summarized in the second row of Figure 6.\nMOBILE DISKS IN HYPERBOLIC SPACE\n15\n5. Minimizing Capacity: Optimization under Free Mobility\nIn this section we consider a series of experiments, where some disks are given fixed positions\nbut the others are free to move within constraints. The constraints can restrict the admissible\nconfigurations to specific regions. In the most general case, the only constraint is that the disks\nshould not overlap. In all simulations it is assumed that the disks have a minimal separation\nδ > 0. In those cases where the disks touch, that is, δ = 0, only hp-FEM results are reported.\n5.1. Three fixed disks.\nOne freely moving disk. Consider three hyperbolic disks with\nequal hyperbolic radii = 0.2, and whose centers are at 0.5e2(k−1)πi/3,\nk = 1, 2, 3. We consider a\nfourth hyperbolic disk whose hyperbolic radius is r and its hyperbolic center is z = x + iy such\nthat the four disks are non-overlapping. Let a function u(x, y) be defined by u(x, y) = cap(B, E),\nwhere E is the union of the four disks. The level curves of the function u(x, y) for six cases of\nr are given in Figure 7. Notice, that the locations of the local minima depend on the chosen\nradius r of the free disk. Due to symmetry, there is a local minimum at the origin in every\ncase. The results suggest that there exists a critical radius rc such that the global minimum is\nfound at the origin for all sufficiently large r, that is, r > rc, but next to one of the fixed disks\nfor r < rc. The interior-point method is guaranteed to converge to one of the local minima,\nand therefore for all r a local minimum may be attained when the mobile disk is centered at\nthe origin.\n5.2. One fixed disk.\nTwo moving disks on a circle. Let us next consider three disks\nD1, D2, D3 with equal hyperbolic radii r = 0.3. The centers of these three disks are placed on\nthe circle |z| = 0.5. We assume that the disk D1 is fixed with center on the positive real line,\nD2 is in the upper-half plane and D3 is in the lower-half plane. Starting when the three disks\nare touching each others (see Figure 8 (left)), these disks start moving away from each other\nsuch that the hyperbolic distance d between the hyperbolic centers of D1 and D2 is the same as\nfor D1 and D3. When all these disks are touching each other, d = 2r. The maximum value dmax\nof d is obtained when the the disks D2 and D3 are touching each other (see Figure 8 (middle)).\nThe values of the capacity as a function of d are shown in Figure 8 (right) where the values of\nthe capacity for 2r < d < dmax are computed by the BIE method and; for d = 2r and d = dmax\nby the FEM. The minimal capacity is found when d = 2r and the maximal when the centers\nof the three disks form an equilateral triangle.\n5.3. One fixed disk. Three moving disks on a circle. Staying on the circle |z| = 0.5 we\nconsider four disks with centers on the circle and hyperbolic radii 3/30, 5/30, 7/30, and 9/30.\nWithout any loss of generality, we will assume that the disk with hyperbolic radius 9/30 is\nfixed with its center on the positive real line at the point 0.5. Then, we search for the positions\nof the other three disks that minimize the capacity. The initial positions of these three disks\nare assumed to be 0.5e2kπi/4 for k = 1, 2, 3. For the optimized positions, we have obtained six\npositions, with three different values of the capacity due to symmetry (see Figure 9). For the\ndisks in the first column in Figure 9, the capacity is 4.6269. The capacity is 4.6193 for the\nsecond column and 4.6621 for the third column.\n16\nH. HAKULA, M. NASSER, AND M. VUORINEN\n(a) r = 0.6\n(b) r = 0.5\n(c) r = 0.4\n(d) r = 0.3\n(e) r = 0.2\n(f) r = 0.1\nFigure 7. The level curves of the capacity u(x, y) of a constellation of four disks\nas a function of the center z = x + iy of the fourth disk. Three hyperbolic disks\nwith equal hyperbolic radii = 0.2 are at fixed locations, whereas the fourth one\nwith a given radius r is free to move (the mobile fourth disk is not shown). The\nnumber of local minima depends on the radius of the fourth disk.\n5.4. One fixed disk. Three moving disks. Finally, we consider four disks with hyperbolic\nradii 3/30, 5/30, 7/30, and 9/30. This time, we will assume that the disk with hyperbolic radius\n9/30 is fixed with its center at the origin. The task is to find the positions of the three free\ndisks that minimize the capacity where the initial positions of the three disks are assumed to\nbe 0.5e2kπi/3 for k = 0, 1, 2. For the optimized positions, we have obtained two configurations,\nas shown in Figure 10, with the capacity 4.2322 which is the global minimum.\nIf we assume that the three disks with hyperbolic radii 5/30, 7/30, and 9/30 have fixed\npositions as in Figure 10 (left), and the small disk with hyperbolic radius 3/30 is moving.\nAssume that the center of the small disk is z = x + iy such that the four disks are non-\noverlapping. Let a function u(x, y) be defined by u(x, y) = cap(B, E), where E is the union of\nthe four disks. The level curves of the function u(x, y) are given in Figure 10 (right). As we\ncan see from the figure, the capacity has three local minima and the capacity for the position\nin Figure 10 (left) is the global minimum. This experiment has been repeated multiple times\nwith different initial starting positions for the free disks and every one one of the local minima\nhas been observed.\nMOBILE DISKS IN HYPERBOLIC SPACE\n17\nFigure 8. Three disks with equal hyperbolic radius = 0.3 on the circle |z| = 0.5.\nOne disk is fixed on the positive real line and the other two move symmetrically on\nupper- and lower-half planes, respectively. The left and middle figures illustrate\nthe minimal dmin and maximal dmax values of the hyperbolic distance d between\nthe hyperbolic centers of the disk on the real line and the disk on the upper-half\nplane. The right figure shows the capacity for the range between these extreme\nvalued dmin ≤d ≤dmax.\nFigure 9. Four disks with hyperbolic radii 3/30, 5/30, 7/30, and 9/30, and with\ncenters on the circle |z| = 0.5. Representative configurations of the optimised\ncases.\n5.4.1. On Computational Costs. Naturally, the optimisation problems are the computationally\nmost expensive ones of all our numerical experiments. In Table 5 performance data on the four\ndisks free mobility problem is presented. Comparison of the two methods is only qualitative,\nsince both underlying hardware and the interior-point implementations are different. However,\nsome conclusions can be derived. In all cases the interior-point tolerance is the same, ϵ = 10−6,\nand within the hp-FEM simulations, meshing is performed with the same discretization control\nin every evaluation. For optimal performance, the individual solutions must be accurate enough\nso that the error induced by numerical approximation of the gradients and Hessians is balanced\nwith other sources of error. For the hp-FEM it appears that the same mesh with p = 4 is not\nadequate in comparison with the one at p = 8. Even though the time spent in one individual\n18\nH. HAKULA, M. NASSER, AND M. VUORINEN\nFigure 10. Four disks with hyperbolic radii 3/30, 5/30, 7/30, and 9/30. The\nthree largest disks have fixed positions, and the smallest one, centered at z =\nx + iy, is free to move. The level curves of u(x, y) = cap(B, E), where E is the\nunion of the four disks, indicate three local minima. The two configurations on\nthe left have converged to the global minimum.\nTable 5. Solution times for the minimization process when one disk is fixed and\nthree disks are mobile. Number of steps is number of iterations in the interior-\npoint algorithm. Number of evaluations is the total number of solves performed\nduring the minimization.\nMethod\nDiscretization\nTime\nNumber of steps\nNumber of evaluations\nBIE\nn = 24\n472.9\n151\n1455\nn = 27\n85.6\n24\n192\nn = 210\n150.7\n24\n192\nhp-FEM\np = 4\n39600\n202\n39568\np = 6\n11100\n37\n7494\np = 8\n9100\n20\n4150\niteration step is doubled, the overall time for p = 8 is significantly lower. At every evaluation\nthe number of degrees of freedom is roughly 13000 (initial configuration: 13542, and final:\n12589). Similarly, for BIE the performance at n = 27 is superior to that at n = 24.\nThe two implementations have very different requirements per iteration step. Observe that\nthe number of iteration steps is comparable, yet the number of evaluations is not. The average\ntime for one evaluation in BIE is four to five times faster than one evaluation in hp-FEM. Matlab\nand Mathematica results have been computed on modern Intel and Apple Silicon computers,\nrespectively.\n5.5. Hyperbolic area lower bound. Finally, we compute the capacity of a constellation of\ndisjoint hyperbolic disks and compare the computed values with the Hyperbolic area lower\nbound [9]. Let Er be the union of m disjoint hyperbolic disks with equal hyperbolic radii r\nsuch the hyperbolic distance between any two disks is 0.02 (see Figure 11 for r = 0.5 and\nm = 2, 3, 4). For m = 4, we consider two cases (as shown in Figure 11) where the centers of\nMOBILE DISKS IN HYPERBOLIC SPACE\n19\nFigure 11. The four types of condensers (B, Er) for the hyperbolic radius r =\n0.5. From left: m = 2, m = 3, m = 4 (Case I), m = 4 (Case II).\nthe disks in Case I are on the real and imaginary axes. In Case II, the centers are on the rays\neiθ for θ = 0, π/3, 2π/3, 4π/3. The hyperbolic area of these m disks is 4mπ sh2(r/2). Consider\nthe hyperbolic disk Bρ(0, M) whose hyperbolic area is the same as the hyperbolic area of the\nm disks, then\nM = 2 arsh \u0000√m sh(r/2)\u0001 .\nThen L(r) = cap(B, Bρ(0, M)) is the hyperbolic area lower bound of cap(B, Er).\nIn view\nof (2.3), we have\nL(r) = cap(B, Bρ(0, M)) = cap(B, B2(0, th(M/2))) =\n2π\nlog cth(M/2).\nThe BIE method is then used to compute cap(B, Er) for several values of r with 0.02 ≤r ≤2.\nOur computed minimum value of the capacity can be considered a lower bound of the capacity\nof the constellation of m disjoint hyperbolic disks. We compare the computed value with the\nhyperbolic area lower bound by defining\nLr = cap(B, Er) −L(r)\nL(r)\n.\nThe graph of Lr is shown in Figure 12 for 0.02 ≤r ≤2 and m = 2, 3, 4. As r →∞it appears\nthat the improvement tends to zero. This is a consequence of the nature of hyperbolic geometry.\nWith one disk fixed in the centre the other three will have ever smaller contributions to the\ncapacity since their Euclidean areas tend to zero as in Figure 2 (right). It is an indication of\nthe complexity of the problem that the graphs in Figure 12 do not reveal any simple connection\nbetween the number of the disks and the minimal capacity.\n6. Conclusions\nWe study lower bounds for the conformal capacity of a constellation of disjoint hyperbolic\ndisks Ej ⊂B2, j = 1, ..., m, using a novel idea: instead of using a symmetrization transforma-\ntion, which usually leads to fusion of the disjoint disks, we are looking for a lower bound in\nterms of another constellation which yields a minimal value. The traditional symmetrization\ntransformation [31], [3], [8], is now replaced by free mobility of individual disks with the con-\nstraint that the hyperbolic radii of the disks are invariant and the disks are non-overlapping.\n20\nH. HAKULA, M. NASSER, AND M. VUORINEN\nFigure 12. The ratio Lr for the four types of condensers (B, Er). As r →∞the\nimprovement relative to the lower bound L(r) tends to zero as expected. Left:\nFor m = 2, m = 3, the improvements are very similar. Right: In line with our\nexperiments above, the Case II is indeed optimal, and gives us an improved lower\nbound.\nIn this process, due to the conformal invariance, the conformal capacity of each disk stays in-\nvariant, whereas the capacity of the whole constellation may significantly vary. Moreover, the\nhyperbolic area of the constellation is also constant.\nThe optimization methods we used produced (locally) minimal constellations such that the\ndisks group together, as closely as possible. This coalescing is reminiscent of the behavior of\nsome animal colonies in cold weather conditions for the purpose of heat flow minimization.\nMathematical methods are not available for analytic treatment of the problems, but we are\nconvinced that there is a strong connection with combinatorial geometry, topics like packing\nand covering problems. Such problems often have many local minima [6, p. 157].\nWe carried out numerical simulations using two different methods, the BIE and hp-FEM\nmethods and the close agreement of the two computational methods confirmed the results.\nBecause of the complexity of the problem we studied various subproblems where disk centers\nsatisfied constraints such that the centers are on the interval (−1, 1) or at the same distance from\nthe origin. In both cases we observed the grouping phenomenon (cf. Figure 1) and, moreover,\nnoticed that permutation of disks has influence on the capacity if the radii are different. Because\nthe hyperbolic area of a constellation is a constant, it is now clear that the hyperbolic area\nalone does not define the constellation capacity.\nThis observation led us to compare our computed lower bound to Gehring’s sharp lower\nbound given in terms of hyperbolic area. The conclusion was that we obtained in some cases\napproximately 10% improvement when m = 4.\nThe numerical agreement of the BIE and hp-FEM methods was very good, typically ten\ndecimal places or better, and the expected exponential convergence was observed, see Figure 5.\nThe performance of the BIE method was significantly faster than the hp-FEM method when\nit comes to computational time and flexilibity to modify the code to new situations. This is\nprobably due to the heavy data structure of the hp-FEM method due to hierarchial triangulation\nrefinement process of the method.\nMOBILE DISKS IN HYPERBOLIC SPACE\n21\nA vast territory of open problems remains. First, it would be interesting to study whether\nsome kind heuristic methods would lead to \"close to extremal\" constellations, to be used as\ninitial steps of the minimization. Such a method could be based on some computationally\ncheaper object function than the capacity itself: for instance, first, the maximization of the\nnumber of the mutual contact points of the constellation. Second, the case of m > 5 disks of\nequal radii seems to be completely open. Perhaps in this case the number of locally minimal\nconstellations grows exponentially as a function of m. Third, one could study constellations of\nother types of geometric figures like hyperbolic triangles.\nReferences\n1. K. E. Atkinson, The Numerical Solution of Integral Equations of the Second Kind, Cambridge University\nPress, Cambridge, 1997.\n2. I. Babuška and B. Guo, Regularity of the solutions of elliptic problems with piecewise analytical data,\nparts I and II, SIAM J. Math. Anal., 19, (1988), 172–203 and 20, (1989), pp. 763–781.\n3. A. Baernstein, Symmetrization in Analysis. With David Drasin and Richard S. Laugesen. With a foreword\nby Walter Hayman. Cambridge University Press, Cambridge, 2019.\n4. A. F. Beardon, The Geometry of Discrete Groups, Springer-Verlag, New York, 1983.\n5. A. F. Beardon and D. Minda, The hyperbolic metric and geometric function theory. Quasiconformal\nmappings and their applications, 9–56, Narosa, New Delhi, 2007.\n6. M. Berger, Geometry revealed. A Jacob’s ladder to modern higher geometry. Translated from the French\nby Lester Senechal. Springer, Heidelberg, 2010.\n7. S.V. Borodachov, D.P. Hardin, and E. B. Saff, Discrete Energy on Rectifiable Sets, Springer, New\nYork, 2019.\n8. V.N. Dubinin,\nCondenser Capacities and Symmetrization in Geometric Function Theory, Birkhäuser,\n2014.\n9. F.W. Gehring, Inequalities for condensers, conformal capacity, and extremal lengths. Michigan Math. J.\n18 (1971), 1–20.\n10. F. W. Gehring, G. J. Martin and B. Palka, An Introduction to the Theory of Higher-Dimensional\nQuasiconformal Mappings, American Mathematical Society, Providence, RI, 2017.\n11. V.M. Goldshtein and Yu. G. Reshetnyak, Quasiconformal Mappings and Sobolev Spaces. Translated\nand revised from the 1983 Russian original. Translated by O. Korneeva. Kluwer Academic Publishers Group,\nDordrecht, 1990.\n12. L. Greengard and Z. Gimbutas, FMMLIB2D: A MATLAB toolbox for fast multipole method in two\ndimensions, version 1.2. 2019, www.cims.nyu.edu/cmcl/fmm2dlib/fmm2dlib.html. Accessed 6 Nov 2020.\n13. H. Hakula, M. M.S. Nasser, and M. Vuorinen, Conformal capacity and polycircular domains, J.\nComput. Appl. Math. 420 (2023), 114802.\n14. H. Hakula, M. Neilan, and J. Ovall, A Posteriori Estimates Using Auxiliary Subspace Techniques,\nJ. Sci. Comput. 72 no. 1 (2017), pp. 97–127.\n15. H. Hakula, A. Rasila, and M. Vuorinen, On moduli of rings and quadrilaterals: algorithms and\nexperiments. SIAM J. Sci. Comput. 33 (2011), no. 1, 279–302.\n16. P. Hariri, R. Klén, and M. Vuorinen, Conformally Invariant Metrics and Quasiconformal Mappings,\nSpringer, Berlin, 2020.\n17. J. Heinonen, T. Kilpeläinen, and O. Martio, Nonlinear Potential Theory of Degenerate Elliptic\nEquations, Dover Publications, New York, 2006.\n18. H. Kober, Dictionary of Conformal Representations, Dover Publications, New York, 1957.\n19. A.J. Kollár, M. Fitzpatrick, and A.A. Houck, Hyperbolic lattices in circuit quantum electrodynam-\nics. Nature 571, 45–50 (2019).\n22\nH. HAKULA, M. NASSER, AND M. VUORINEN\n20. R. Kusner, W. Kusner, J.C. Lagarias, and S. Shlosman, Configuration spaces of equal spheres\ntouching a given sphere: the twelve spheres problem. New trends in intuitive geometry, 219–277, Bolyai\nSoc. Math. Stud., 27, János Bolyai Math. Soc., Budapest, 2018.\n21. J. C. Lagarias, C. L. Mallows, and A. R. Wilks, Beyond the Descartes Circle Theorem. Amer.\nMath. Monthly, (2002) 109:4, 338–361.\n22. P.M. Lenggenhager, A. Stegmaier, L.K. Upreti, et al., Simulating hyperbolic space on a circuit\nboard. Nat Commun 13, 4373 (2022).\n23. R.H. Lewis and S. Bridgett, Conic tangency equations and Apollonius problems in biochemistry and\npharmacology. (English summary) Math. Comput. Simulation 61 (2003), no. 2, 101–114.\n24. MATLAB, 2022a. 9.12 (R2022a), Natick, Massachusetts: The MathWorks Inc.\n25. M. M.S. Nasser, Fast solution of boundary integral equations with the generalized Neumann kernel.-\nElectron. Trans. Numer. Anal. 44 (2015), 189–229.\n26. M. M.S. Nasser, O. Rainio, and M. Vuorinen, Condenser capacity and hyperbolic diameter. J. Math.\nAnal. Appl. 508(2022), 125870.\n27. M. M.S. Nasser, O. Rainio, and M. Vuorinen, Condenser capacity and hyperbolic perimeter. Comput.\nMath. Appl. 105(2022), 54–74.\n28. M. M.S. Nasser and M. Vuorinen, Numerical computation of the capacity of generalized condensers.\nJ. Comput. Appl. Math. 377 (2020) 112865.\n29. M. M.S. Nasser and M. Vuorinen, Isoperimetric properties of condenser capacity. J. Math. Anal. Appl.\n499 (2021), 125050.\n30. J. Nocedal and S. Wright, Numerical Optimization, Springer New York, NY, 2006.\n31. G. Pólya and G. Szegö, Isoperimetric Inequalities in Mathematical Physics. Princeton Univ. Press, 1952.\n32. Th. Ransford, Potential Theory in the Complex Plane, Cambridge University Press, Cambridge, 1995.\n33. Ch. Schwab, p- and hp-Finite Element Methods, Oxford University Press, 1998.\n34. A. Yu. Solynin, Problems on the loss of heat: herd instinct versus individual feelings, St. Petersburg\nMath. J. 33 (2022), 739–775, Algebra i Analiz, tom 33 (2021), nomer 5.\n35. L. N. Trefethen and J. A.C. Weideman, The exponentially convergent trapezoidal rule. SIAM Rev.\n56 (2014), 385–458.\n36. M. Tsuji, Potential Theory in Modern Function Theory. Chelsea Publishing Co., New York, 1975.\n37. G. Wang, M. Vuorinen, and X. Zhang, On cyclic quadrilaterals in Euclidean and hyperbolic geometries.\nPubl. Math. Debrecen 99/1-2 (2021), 123–140.\n38. R. Wegmann and M. M.S. Nasser, The Riemann-Hilbert problem and the generalized Neumann kernel\non multiply connected regions. J. Comput. Appl. Math. 214 (2008), 36–57.\n39. K. Williams, Tree swallows huddle in snow, https://www.fws.gov/media/tree-swallows-huddle-snow. May\n12, 2011.\n40. Wolfram Research, Inc., Mathematica, Version 13.2.1, Champaign, IL, 2023.\nAalto University, Department of Mathematics and Systems Analysis, P.O. Box 11100, FI-\n00076 Aalto, FINLAND\nEmail address: harri.hakula@aalto.fi\nDepartment of Mathematics, Statistics, and Physics, Wichita State University, Wichita, KS\n67260-0033, USA\nEmail address: mms.nasser@wichita.edu\nDepartment of Mathematics and Statistics, University of Turku, FI-20014 Turku, Finland\nEmail address: vuorinen@utu.fi"
    }
]